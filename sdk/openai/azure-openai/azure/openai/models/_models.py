# pylint: disable=too-many-lines
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
# pylint: disable=useless-super-delegation

import datetime
from typing import Any, Dict, List, Literal, Mapping, Optional, TYPE_CHECKING, Union, overload

from azure.core.exceptions import ODataV4Format

from .. import _model_base
from .._model_base import rest_discriminator, rest_field
from .._vendor import FileType
from ._enums import (
    AzureChatExtensionType,
    ChatRole,
    OnYourDataAuthenticationType,
    OnYourDataVectorSearchAuthenticationType,
    OnYourDataVectorizationSourceType,
)

if TYPE_CHECKING:
    from .. import models as _models


class AddUploadPartRequest(_model_base.Model):
    """The multipart/form-data request body of a data part addition request for an upload.

    All required parameters must be populated in order to send to server.

    :ivar data: The chunk of bytes for this Part. Required.
    :vartype data: ~azure.openai._vendor.FileType
    """

    data: FileType = rest_field(is_multipart_file_input=True)
    """The chunk of bytes for this Part. Required."""

    @overload
    def __init__(
        self,
        *,
        data: FileType,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AudioTranscription(_model_base.Model):
    """Result information for an operation that transcribed spoken audio into written text.


    :ivar text: The transcribed text for the provided audio data. Required.
    :vartype text: str
    :ivar task: The label that describes which operation type generated the accompanying response
     data. Known values are: "transcribe" and "translate".
    :vartype task: str or ~azure.openai.models.AudioTaskLabel
    :ivar language: The spoken language that was detected in the transcribed audio data.
     This is expressed as a two-letter ISO-639-1 language code like 'en' or 'fr'.
    :vartype language: str
    :ivar duration: The total duration of the audio processed to produce accompanying transcription
     information.
    :vartype duration: float
    :ivar segments: A collection of information about the timing, probabilities, and other detail
     of each processed audio segment.
    :vartype segments: list[~azure.openai.models.AudioTranscriptionSegment]
    :ivar words: A collection of information about the timing of each processed word.
    :vartype words: list[~azure.openai.models.AudioTranscriptionWord]
    """

    text: str = rest_field()
    """The transcribed text for the provided audio data. Required."""
    task: Optional[Union[str, "_models.AudioTaskLabel"]] = rest_field()
    """The label that describes which operation type generated the accompanying response data. Known
     values are: \"transcribe\" and \"translate\"."""
    language: Optional[str] = rest_field()
    """The spoken language that was detected in the transcribed audio data.
     This is expressed as a two-letter ISO-639-1 language code like 'en' or 'fr'."""
    duration: Optional[float] = rest_field()
    """The total duration of the audio processed to produce accompanying transcription information."""
    segments: Optional[List["_models.AudioTranscriptionSegment"]] = rest_field()
    """A collection of information about the timing, probabilities, and other detail of each processed
     audio segment."""
    words: Optional[List["_models.AudioTranscriptionWord"]] = rest_field()
    """A collection of information about the timing of each processed word."""

    @overload
    def __init__(
        self,
        *,
        text: str,
        task: Optional[Union[str, "_models.AudioTaskLabel"]] = None,
        language: Optional[str] = None,
        duration: Optional[float] = None,
        segments: Optional[List["_models.AudioTranscriptionSegment"]] = None,
        words: Optional[List["_models.AudioTranscriptionWord"]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AudioTranscriptionOptions(_model_base.Model):
    """The configuration information for an audio transcription request.

    All required parameters must be populated in order to send to server.

    :ivar file: The audio data to transcribe. This must be the binary content of a file in one of
     the supported media formats:
      flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm. Required.
    :vartype file: ~azure.openai._vendor.FileType
    :ivar filename: The optional filename or descriptive identifier to associate with with the
     audio data.
    :vartype filename: str
    :ivar response_format: The requested format of the transcription response data, which will
     influence the content and detail of the result. Known values are: "json", "verbose_json",
     "text", "srt", and "vtt".
    :vartype response_format: str or ~azure.openai.models.AudioTranscriptionFormat
    :ivar language: The primary spoken language of the audio data to be transcribed, supplied as a
     two-letter ISO-639-1 language code
     such as 'en' or 'fr'.
     Providing this known input language is optional but may improve the accuracy and/or latency of
     transcription.
    :vartype language: str
    :ivar prompt: An optional hint to guide the model's style or continue from a prior audio
     segment. The written language of the
     prompt should match the primary spoken language of the audio data.
    :vartype prompt: str
    :ivar temperature: The sampling temperature, between 0 and 1.
     Higher values like 0.8 will make the output more random, while lower values like 0.2 will make
     it more focused and deterministic.
     If set to 0, the model will use log probability to automatically increase the temperature
     until certain thresholds are hit.
    :vartype temperature: float
    :ivar timestamp_granularities: The timestamp granularities to populate for this transcription.
     ``response_format`` must be set ``verbose_json`` to use timestamp granularities.
     Either or both of these options are supported: ``word``\\ , or ``segment``.
     Note: There is no additional latency for segment timestamps, but generating word timestamps
     incurs additional latency.
    :vartype timestamp_granularities: list[str or
     ~azure.openai.models.AudioTranscriptionTimestampGranularity]
    :ivar model: The model to use for this transcription request.
    :vartype model: str
    """

    file: FileType = rest_field(is_multipart_file_input=True)
    """The audio data to transcribe. This must be the binary content of a file in one of the supported
     media formats:
      flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm. Required."""
    filename: Optional[str] = rest_field()
    """The optional filename or descriptive identifier to associate with with the audio data."""
    response_format: Optional[Union[str, "_models.AudioTranscriptionFormat"]] = rest_field()
    """The requested format of the transcription response data, which will influence the content and
     detail of the result. Known values are: \"json\", \"verbose_json\", \"text\", \"srt\", and
     \"vtt\"."""
    language: Optional[str] = rest_field()
    """The primary spoken language of the audio data to be transcribed, supplied as a two-letter
     ISO-639-1 language code
     such as 'en' or 'fr'.
     Providing this known input language is optional but may improve the accuracy and/or latency of
     transcription."""
    prompt: Optional[str] = rest_field()
    """An optional hint to guide the model's style or continue from a prior audio segment. The written
     language of the
     prompt should match the primary spoken language of the audio data."""
    temperature: Optional[float] = rest_field()
    """The sampling temperature, between 0 and 1.
     Higher values like 0.8 will make the output more random, while lower values like 0.2 will make
     it more focused and deterministic.
     If set to 0, the model will use log probability to automatically increase the temperature until
     certain thresholds are hit."""
    timestamp_granularities: Optional[List[Union[str, "_models.AudioTranscriptionTimestampGranularity"]]] = rest_field()
    """The timestamp granularities to populate for this transcription.
     ``response_format`` must be set ``verbose_json`` to use timestamp granularities.
     Either or both of these options are supported: ``word``\ , or ``segment``.
     Note: There is no additional latency for segment timestamps, but generating word timestamps
     incurs additional latency."""
    model: Optional[str] = rest_field()
    """The model to use for this transcription request."""

    @overload
    def __init__(
        self,
        *,
        file: FileType,
        filename: Optional[str] = None,
        response_format: Optional[Union[str, "_models.AudioTranscriptionFormat"]] = None,
        language: Optional[str] = None,
        prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        timestamp_granularities: Optional[List[Union[str, "_models.AudioTranscriptionTimestampGranularity"]]] = None,
        model: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AudioTranscriptionSegment(_model_base.Model):
    """Extended information about a single segment of transcribed audio data.
    Segments generally represent roughly 5-10 seconds of speech. Segment boundaries typically occur
    between words but not
    necessarily sentences.


    :ivar id: The 0-based index of this segment within a transcription. Required.
    :vartype id: int
    :ivar start: The time at which this segment started relative to the beginning of the
     transcribed audio. Required.
    :vartype start: float
    :ivar end: The time at which this segment ended relative to the beginning of the transcribed
     audio. Required.
    :vartype end: float
    :ivar text: The transcribed text that was part of this audio segment. Required.
    :vartype text: str
    :ivar temperature: The temperature score associated with this audio segment. Required.
    :vartype temperature: float
    :ivar avg_logprob: The average log probability associated with this audio segment. Required.
    :vartype avg_logprob: float
    :ivar compression_ratio: The compression ratio of this audio segment. Required.
    :vartype compression_ratio: float
    :ivar no_speech_prob: The probability of no speech detection within this audio segment.
     Required.
    :vartype no_speech_prob: float
    :ivar tokens: The token IDs matching the transcribed text in this audio segment. Required.
    :vartype tokens: list[int]
    :ivar seek: The seek position associated with the processing of this audio segment.
     Seek positions are expressed as hundredths of seconds.
     The model may process several segments from a single seek position, so while the seek position
     will never represent
     a later time than the segment's start, the segment's start may represent a significantly later
     time than the
     segment's associated seek position. Required.
    :vartype seek: int
    """

    id: int = rest_field()
    """The 0-based index of this segment within a transcription. Required."""
    start: float = rest_field()
    """The time at which this segment started relative to the beginning of the transcribed audio.
     Required."""
    end: float = rest_field()
    """The time at which this segment ended relative to the beginning of the transcribed audio.
     Required."""
    text: str = rest_field()
    """The transcribed text that was part of this audio segment. Required."""
    temperature: float = rest_field()
    """The temperature score associated with this audio segment. Required."""
    avg_logprob: float = rest_field()
    """The average log probability associated with this audio segment. Required."""
    compression_ratio: float = rest_field()
    """The compression ratio of this audio segment. Required."""
    no_speech_prob: float = rest_field()
    """The probability of no speech detection within this audio segment. Required."""
    tokens: List[int] = rest_field()
    """The token IDs matching the transcribed text in this audio segment. Required."""
    seek: int = rest_field()
    """The seek position associated with the processing of this audio segment.
     Seek positions are expressed as hundredths of seconds.
     The model may process several segments from a single seek position, so while the seek position
     will never represent
     a later time than the segment's start, the segment's start may represent a significantly later
     time than the
     segment's associated seek position. Required."""

    @overload
    def __init__(
        self,
        *,
        id: int,  # pylint: disable=redefined-builtin
        start: float,
        end: float,
        text: str,
        temperature: float,
        avg_logprob: float,
        compression_ratio: float,
        no_speech_prob: float,
        tokens: List[int],
        seek: int,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AudioTranscriptionWord(_model_base.Model):
    """Extended information about a single transcribed word, as provided on responses when the 'word'
    timestamp granularity is provided.


    :ivar word: The textual content of the word. Required.
    :vartype word: str
    :ivar start: The start time of the word relative to the beginning of the audio, expressed in
     seconds. Required.
    :vartype start: float
    :ivar end: The end time of the word relative to the beginning of the audio, expressed in
     seconds. Required.
    :vartype end: float
    """

    word: str = rest_field()
    """The textual content of the word. Required."""
    start: float = rest_field()
    """The start time of the word relative to the beginning of the audio, expressed in seconds.
     Required."""
    end: float = rest_field()
    """The end time of the word relative to the beginning of the audio, expressed in seconds.
     Required."""

    @overload
    def __init__(
        self,
        *,
        word: str,
        start: float,
        end: float,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AudioTranslation(_model_base.Model):
    """Result information for an operation that translated spoken audio into written text.


    :ivar text: The translated text for the provided audio data. Required.
    :vartype text: str
    :ivar task: The label that describes which operation type generated the accompanying response
     data. Known values are: "transcribe" and "translate".
    :vartype task: str or ~azure.openai.models.AudioTaskLabel
    :ivar language: The spoken language that was detected in the translated audio data.
     This is expressed as a two-letter ISO-639-1 language code like 'en' or 'fr'.
    :vartype language: str
    :ivar duration: The total duration of the audio processed to produce accompanying translation
     information.
    :vartype duration: float
    :ivar segments: A collection of information about the timing, probabilities, and other detail
     of each processed audio segment.
    :vartype segments: list[~azure.openai.models.AudioTranslationSegment]
    """

    text: str = rest_field()
    """The translated text for the provided audio data. Required."""
    task: Optional[Union[str, "_models.AudioTaskLabel"]] = rest_field()
    """The label that describes which operation type generated the accompanying response data. Known
     values are: \"transcribe\" and \"translate\"."""
    language: Optional[str] = rest_field()
    """The spoken language that was detected in the translated audio data.
     This is expressed as a two-letter ISO-639-1 language code like 'en' or 'fr'."""
    duration: Optional[float] = rest_field()
    """The total duration of the audio processed to produce accompanying translation information."""
    segments: Optional[List["_models.AudioTranslationSegment"]] = rest_field()
    """A collection of information about the timing, probabilities, and other detail of each processed
     audio segment."""

    @overload
    def __init__(
        self,
        *,
        text: str,
        task: Optional[Union[str, "_models.AudioTaskLabel"]] = None,
        language: Optional[str] = None,
        duration: Optional[float] = None,
        segments: Optional[List["_models.AudioTranslationSegment"]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AudioTranslationOptions(_model_base.Model):
    """The configuration information for an audio translation request.

    All required parameters must be populated in order to send to server.

    :ivar file: The audio data to translate. This must be the binary content of a file in one of
     the supported media formats:
      flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm. Required.
    :vartype file: ~azure.openai._vendor.FileType
    :ivar filename: The optional filename or descriptive identifier to associate with with the
     audio data.
    :vartype filename: str
    :ivar response_format: The requested format of the translation response data, which will
     influence the content and detail of the result. Known values are: "json", "verbose_json",
     "text", "srt", and "vtt".
    :vartype response_format: str or ~azure.openai.models.AudioTranslationFormat
    :ivar prompt: An optional hint to guide the model's style or continue from a prior audio
     segment. The written language of the
     prompt should match the primary spoken language of the audio data.
    :vartype prompt: str
    :ivar temperature: The sampling temperature, between 0 and 1.
     Higher values like 0.8 will make the output more random, while lower values like 0.2 will make
     it more focused and deterministic.
     If set to 0, the model will use log probability to automatically increase the temperature
     until certain thresholds are hit.
    :vartype temperature: float
    :ivar model: The model to use for this translation request.
    :vartype model: str
    """

    file: FileType = rest_field(is_multipart_file_input=True)
    """The audio data to translate. This must be the binary content of a file in one of the supported
     media formats:
      flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm. Required."""
    filename: Optional[str] = rest_field()
    """The optional filename or descriptive identifier to associate with with the audio data."""
    response_format: Optional[Union[str, "_models.AudioTranslationFormat"]] = rest_field()
    """The requested format of the translation response data, which will influence the content and
     detail of the result. Known values are: \"json\", \"verbose_json\", \"text\", \"srt\", and
     \"vtt\"."""
    prompt: Optional[str] = rest_field()
    """An optional hint to guide the model's style or continue from a prior audio segment. The written
     language of the
     prompt should match the primary spoken language of the audio data."""
    temperature: Optional[float] = rest_field()
    """The sampling temperature, between 0 and 1.
     Higher values like 0.8 will make the output more random, while lower values like 0.2 will make
     it more focused and deterministic.
     If set to 0, the model will use log probability to automatically increase the temperature until
     certain thresholds are hit."""
    model: Optional[str] = rest_field()
    """The model to use for this translation request."""

    @overload
    def __init__(
        self,
        *,
        file: FileType,
        filename: Optional[str] = None,
        response_format: Optional[Union[str, "_models.AudioTranslationFormat"]] = None,
        prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        model: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AudioTranslationSegment(_model_base.Model):
    """Extended information about a single segment of translated audio data.
    Segments generally represent roughly 5-10 seconds of speech. Segment boundaries typically occur
    between words but not
    necessarily sentences.


    :ivar id: The 0-based index of this segment within a translation. Required.
    :vartype id: int
    :ivar start: The time at which this segment started relative to the beginning of the translated
     audio. Required.
    :vartype start: float
    :ivar end: The time at which this segment ended relative to the beginning of the translated
     audio. Required.
    :vartype end: float
    :ivar text: The translated text that was part of this audio segment. Required.
    :vartype text: str
    :ivar temperature: The temperature score associated with this audio segment. Required.
    :vartype temperature: float
    :ivar avg_logprob: The average log probability associated with this audio segment. Required.
    :vartype avg_logprob: float
    :ivar compression_ratio: The compression ratio of this audio segment. Required.
    :vartype compression_ratio: float
    :ivar no_speech_prob: The probability of no speech detection within this audio segment.
     Required.
    :vartype no_speech_prob: float
    :ivar tokens: The token IDs matching the translated text in this audio segment. Required.
    :vartype tokens: list[int]
    :ivar seek: The seek position associated with the processing of this audio segment.
     Seek positions are expressed as hundredths of seconds.
     The model may process several segments from a single seek position, so while the seek position
     will never represent
     a later time than the segment's start, the segment's start may represent a significantly later
     time than the
     segment's associated seek position. Required.
    :vartype seek: int
    """

    id: int = rest_field()
    """The 0-based index of this segment within a translation. Required."""
    start: float = rest_field()
    """The time at which this segment started relative to the beginning of the translated audio.
     Required."""
    end: float = rest_field()
    """The time at which this segment ended relative to the beginning of the translated audio.
     Required."""
    text: str = rest_field()
    """The translated text that was part of this audio segment. Required."""
    temperature: float = rest_field()
    """The temperature score associated with this audio segment. Required."""
    avg_logprob: float = rest_field()
    """The average log probability associated with this audio segment. Required."""
    compression_ratio: float = rest_field()
    """The compression ratio of this audio segment. Required."""
    no_speech_prob: float = rest_field()
    """The probability of no speech detection within this audio segment. Required."""
    tokens: List[int] = rest_field()
    """The token IDs matching the translated text in this audio segment. Required."""
    seek: int = rest_field()
    """The seek position associated with the processing of this audio segment.
     Seek positions are expressed as hundredths of seconds.
     The model may process several segments from a single seek position, so while the seek position
     will never represent
     a later time than the segment's start, the segment's start may represent a significantly later
     time than the
     segment's associated seek position. Required."""

    @overload
    def __init__(
        self,
        *,
        id: int,  # pylint: disable=redefined-builtin
        start: float,
        end: float,
        text: str,
        temperature: float,
        avg_logprob: float,
        compression_ratio: float,
        no_speech_prob: float,
        tokens: List[int],
        seek: int,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureChatEnhancementConfiguration(_model_base.Model):
    """A representation of the available Azure OpenAI enhancement configurations.

    :ivar grounding: A representation of the available options for the Azure OpenAI grounding
     enhancement.
    :vartype grounding: ~azure.openai.models.AzureChatGroundingEnhancementConfiguration
    :ivar ocr: A representation of the available options for the Azure OpenAI optical character
     recognition (OCR) enhancement.
    :vartype ocr: ~azure.openai.models.AzureChatOCREnhancementConfiguration
    """

    grounding: Optional["_models.AzureChatGroundingEnhancementConfiguration"] = rest_field()
    """A representation of the available options for the Azure OpenAI grounding enhancement."""
    ocr: Optional["_models.AzureChatOCREnhancementConfiguration"] = rest_field()
    """A representation of the available options for the Azure OpenAI optical character recognition
     (OCR) enhancement."""

    @overload
    def __init__(
        self,
        *,
        grounding: Optional["_models.AzureChatGroundingEnhancementConfiguration"] = None,
        ocr: Optional["_models.AzureChatOCREnhancementConfiguration"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureChatEnhancements(_model_base.Model):
    """Represents the output results of Azure enhancements to chat completions, as configured via the
    matching input provided
    in the request.

    :ivar grounding: The grounding enhancement that returns the bounding box of the objects
     detected in the image.
    :vartype grounding: ~azure.openai.models.AzureGroundingEnhancement
    """

    grounding: Optional["_models.AzureGroundingEnhancement"] = rest_field()
    """The grounding enhancement that returns the bounding box of the objects detected in the image."""

    @overload
    def __init__(
        self,
        *,
        grounding: Optional["_models.AzureGroundingEnhancement"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureChatExtensionConfiguration(_model_base.Model):
    """A representation of configuration data for a single Azure OpenAI chat extension. This will be
    used by a chat
      completions request that should use Azure OpenAI chat extensions to augment the response
    behavior.
      The use of this configuration is compatible only with Azure OpenAI.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    AzureCosmosDBChatExtensionConfiguration, AzureSearchChatExtensionConfiguration,
    ElasticsearchChatExtensionConfiguration, MongoDBChatExtensionConfiguration,
    PineconeChatExtensionConfiguration

    All required parameters must be populated in order to send to server.

    :ivar type: The label for the type of an Azure chat extension. This typically corresponds to a
     matching Azure resource.
       Azure chat extensions are only compatible with Azure OpenAI. Required. Known values are:
     "azure_search", "azure_cosmos_db", "elasticsearch", "pinecone", and "mongo_db".
    :vartype type: str or ~azure.openai.models.AzureChatExtensionType
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The label for the type of an Azure chat extension. This typically corresponds to a matching
     Azure resource.
       Azure chat extensions are only compatible with Azure OpenAI. Required. Known values are:
     \"azure_search\", \"azure_cosmos_db\", \"elasticsearch\", \"pinecone\", and \"mongo_db\"."""

    @overload
    def __init__(
        self,
        *,
        type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureChatExtensionDataSourceResponseCitation(_model_base.Model):  # pylint: disable=name-too-long
    """A single instance of additional context information available when Azure OpenAI chat extensions
    are involved
    in the generation of a corresponding chat completions response. This context information is
    only populated when
    using an Azure OpenAI request configured to use a matching extension.


    :ivar content: The content of the citation. Required.
    :vartype content: str
    :ivar title: The title of the citation.
    :vartype title: str
    :ivar url: The URL of the citation.
    :vartype url: str
    :ivar filepath: The file path of the citation.
    :vartype filepath: str
    :ivar chunk_id: The chunk ID of the citation.
    :vartype chunk_id: str
    :ivar rerank_score: The rerank score of the retrieved document.
    :vartype rerank_score: float
    """

    content: str = rest_field()
    """The content of the citation. Required."""
    title: Optional[str] = rest_field()
    """The title of the citation."""
    url: Optional[str] = rest_field()
    """The URL of the citation."""
    filepath: Optional[str] = rest_field()
    """The file path of the citation."""
    chunk_id: Optional[str] = rest_field()
    """The chunk ID of the citation."""
    rerank_score: Optional[float] = rest_field()
    """The rerank score of the retrieved document."""

    @overload
    def __init__(
        self,
        *,
        content: str,
        title: Optional[str] = None,
        url: Optional[str] = None,
        filepath: Optional[str] = None,
        chunk_id: Optional[str] = None,
        rerank_score: Optional[float] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureChatExtensionRetrievedDocument(_model_base.Model):
    """The retrieved document.


    :ivar content: The content of the citation. Required.
    :vartype content: str
    :ivar title: The title of the citation.
    :vartype title: str
    :ivar url: The URL of the citation.
    :vartype url: str
    :ivar filepath: The file path of the citation.
    :vartype filepath: str
    :ivar chunk_id: The chunk ID of the citation.
    :vartype chunk_id: str
    :ivar rerank_score: The rerank score of the retrieved document.
    :vartype rerank_score: float
    :ivar search_queries: The search queries used to retrieve the document. Required.
    :vartype search_queries: list[str]
    :ivar data_source_index: The index of the data source. Required.
    :vartype data_source_index: int
    :ivar original_search_score: The original search score of the retrieved document.
    :vartype original_search_score: float
    :ivar filter_reason: Represents the rationale for filtering the document. If the document does
     not undergo filtering,
     this field will remain unset. Known values are: "score" and "rerank".
    :vartype filter_reason: str or
     ~azure.openai.models.AzureChatExtensionRetrieveDocumentFilterReason
    """

    content: str = rest_field()
    """The content of the citation. Required."""
    title: Optional[str] = rest_field()
    """The title of the citation."""
    url: Optional[str] = rest_field()
    """The URL of the citation."""
    filepath: Optional[str] = rest_field()
    """The file path of the citation."""
    chunk_id: Optional[str] = rest_field()
    """The chunk ID of the citation."""
    rerank_score: Optional[float] = rest_field()
    """The rerank score of the retrieved document."""
    search_queries: List[str] = rest_field()
    """The search queries used to retrieve the document. Required."""
    data_source_index: int = rest_field()
    """The index of the data source. Required."""
    original_search_score: Optional[float] = rest_field()
    """The original search score of the retrieved document."""
    filter_reason: Optional[Union[str, "_models.AzureChatExtensionRetrieveDocumentFilterReason"]] = rest_field()
    """Represents the rationale for filtering the document. If the document does not undergo
     filtering,
     this field will remain unset. Known values are: \"score\" and \"rerank\"."""

    @overload
    def __init__(
        self,
        *,
        content: str,
        search_queries: List[str],
        data_source_index: int,
        title: Optional[str] = None,
        url: Optional[str] = None,
        filepath: Optional[str] = None,
        chunk_id: Optional[str] = None,
        rerank_score: Optional[float] = None,
        original_search_score: Optional[float] = None,
        filter_reason: Optional[Union[str, "_models.AzureChatExtensionRetrieveDocumentFilterReason"]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureChatExtensionsMessageContext(_model_base.Model):
    """A representation of the additional context information available when Azure OpenAI chat
    extensions are involved
      in the generation of a corresponding chat completions response. This context information is
    only populated when
      using an Azure OpenAI request configured to use a matching extension.

    :ivar citations: The contextual information associated with the Azure chat extensions used for
     a chat completions request.
       These messages describe the data source retrievals, plugin invocations, and other
     intermediate steps taken in the
       course of generating a chat completions response that was augmented by capabilities from
     Azure OpenAI chat
       extensions.
    :vartype citations: list[~azure.openai.models.AzureChatExtensionDataSourceResponseCitation]
    :ivar intent: The detected intent from the chat history, used to pass to the next turn to carry
     over the context.
    :vartype intent: str
    :ivar all_retrieved_documents: All the retrieved documents.
    :vartype all_retrieved_documents:
     list[~azure.openai.models.AzureChatExtensionRetrievedDocument]
    """

    citations: Optional[List["_models.AzureChatExtensionDataSourceResponseCitation"]] = rest_field()
    """The contextual information associated with the Azure chat extensions used for a chat
     completions request.
       These messages describe the data source retrievals, plugin invocations, and other
     intermediate steps taken in the
       course of generating a chat completions response that was augmented by capabilities from
     Azure OpenAI chat
       extensions."""
    intent: Optional[str] = rest_field()
    """The detected intent from the chat history, used to pass to the next turn to carry over the
     context."""
    all_retrieved_documents: Optional[List["_models.AzureChatExtensionRetrievedDocument"]] = rest_field()
    """All the retrieved documents."""

    @overload
    def __init__(
        self,
        *,
        citations: Optional[List["_models.AzureChatExtensionDataSourceResponseCitation"]] = None,
        intent: Optional[str] = None,
        all_retrieved_documents: Optional[List["_models.AzureChatExtensionRetrievedDocument"]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureChatGroundingEnhancementConfiguration(_model_base.Model):  # pylint: disable=name-too-long
    """A representation of the available options for the Azure OpenAI grounding enhancement.

    All required parameters must be populated in order to send to server.

    :ivar enabled: Specifies whether the enhancement is enabled. Required.
    :vartype enabled: bool
    """

    enabled: bool = rest_field()
    """Specifies whether the enhancement is enabled. Required."""

    @overload
    def __init__(
        self,
        *,
        enabled: bool,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureChatOCREnhancementConfiguration(_model_base.Model):
    """A representation of the available options for the Azure OpenAI optical character recognition
    (OCR) enhancement.

    All required parameters must be populated in order to send to server.

    :ivar enabled: Specifies whether the enhancement is enabled. Required.
    :vartype enabled: bool
    """

    enabled: bool = rest_field()
    """Specifies whether the enhancement is enabled. Required."""

    @overload
    def __init__(
        self,
        *,
        enabled: bool,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureCosmosDBChatExtensionConfiguration(AzureChatExtensionConfiguration, discriminator="azure_cosmos_db"):
    """A specific representation of configurable options for Azure Cosmos DB when using it as an Azure
    OpenAI chat
    extension.

    All required parameters must be populated in order to send to server.

    :ivar type: The type label to use when configuring Azure OpenAI chat extensions. This should
     typically not be changed from its
     default value for Azure Cosmos DB. Required. Represents the use of Azure Cosmos DB as an Azure
     OpenAI chat extension.
    :vartype type: str or ~azure.openai.models.AZURE_COSMOS_DB
    :ivar parameters: The parameters to use when configuring Azure OpenAI CosmosDB chat extensions.
     Required.
    :vartype parameters: ~azure.openai.models.AzureCosmosDBChatExtensionParameters
    """

    type: Literal[AzureChatExtensionType.AZURE_COSMOS_DB] = rest_discriminator(name="type")  # type: ignore
    """The type label to use when configuring Azure OpenAI chat extensions. This should typically not
     be changed from its
     default value for Azure Cosmos DB. Required. Represents the use of Azure Cosmos DB as an Azure
     OpenAI chat extension."""
    parameters: "_models.AzureCosmosDBChatExtensionParameters" = rest_field()
    """The parameters to use when configuring Azure OpenAI CosmosDB chat extensions. Required."""

    @overload
    def __init__(
        self,
        *,
        parameters: "_models.AzureCosmosDBChatExtensionParameters",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=AzureChatExtensionType.AZURE_COSMOS_DB, **kwargs)


class AzureCosmosDBChatExtensionParameters(_model_base.Model):
    """Parameters to use when configuring Azure OpenAI On Your Data chat extensions when using Azure
    Cosmos DB for
    MongoDB vCore. The supported authentication type is ConnectionString.

    All required parameters must be populated in order to send to server.

    :ivar top_n_documents: The configured top number of documents to feature for the configured
     query.
    :vartype top_n_documents: int
    :ivar in_scope: Whether queries should be restricted to use of indexed data.
    :vartype in_scope: bool
    :ivar strictness: The configured strictness of the search relevance filtering. The higher of
     strictness, the higher of the precision but lower recall of the answer.
    :vartype strictness: int
    :ivar max_search_queries: The max number of rewritten queries should be send to search provider
     for one user message. If not specified,
     the system will decide the number of queries to send.
    :vartype max_search_queries: int
    :ivar allow_partial_result: If specified as true, the system will allow partial search results
     to be used and the request fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails.
    :vartype allow_partial_result: bool
    :ivar include_contexts: The included properties of the output context. If not specified, the
     default value is ``citations`` and ``intent``.
    :vartype include_contexts: list[str or ~azure.openai.models.OnYourDataContextProperty]
    :ivar authentication: The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication.
    :vartype authentication: ~azure.openai.models.OnYourDataAuthenticationOptions
    :ivar database_name: The MongoDB vCore database name to use with Azure Cosmos DB. Required.
    :vartype database_name: str
    :ivar container_name: The name of the Azure Cosmos DB resource container. Required.
    :vartype container_name: str
    :ivar index_name: The MongoDB vCore index name to use with Azure Cosmos DB. Required.
    :vartype index_name: str
    :ivar fields_mapping: Customized field mapping behavior to use when interacting with the search
     index. Required.
    :vartype fields_mapping: ~azure.openai.models.AzureCosmosDBFieldMappingOptions
    :ivar embedding_dependency: The embedding dependency for vector search. Required.
    :vartype embedding_dependency: ~azure.openai.models.OnYourDataVectorizationSource
    """

    top_n_documents: Optional[int] = rest_field()
    """The configured top number of documents to feature for the configured query."""
    in_scope: Optional[bool] = rest_field()
    """Whether queries should be restricted to use of indexed data."""
    strictness: Optional[int] = rest_field()
    """The configured strictness of the search relevance filtering. The higher of strictness, the
     higher of the precision but lower recall of the answer."""
    max_search_queries: Optional[int] = rest_field()
    """The max number of rewritten queries should be send to search provider for one user message. If
     not specified,
     the system will decide the number of queries to send."""
    allow_partial_result: Optional[bool] = rest_field()
    """If specified as true, the system will allow partial search results to be used and the request
     fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails."""
    include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = rest_field()
    """The included properties of the output context. If not specified, the default value is
     ``citations`` and ``intent``."""
    authentication: Optional["_models.OnYourDataAuthenticationOptions"] = rest_field()
    """The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication."""
    database_name: str = rest_field()
    """The MongoDB vCore database name to use with Azure Cosmos DB. Required."""
    container_name: str = rest_field()
    """The name of the Azure Cosmos DB resource container. Required."""
    index_name: str = rest_field()
    """The MongoDB vCore index name to use with Azure Cosmos DB. Required."""
    fields_mapping: "_models.AzureCosmosDBFieldMappingOptions" = rest_field()
    """Customized field mapping behavior to use when interacting with the search index. Required."""
    embedding_dependency: "_models.OnYourDataVectorizationSource" = rest_field()
    """The embedding dependency for vector search. Required."""

    @overload
    def __init__(
        self,
        *,
        database_name: str,
        container_name: str,
        index_name: str,
        fields_mapping: "_models.AzureCosmosDBFieldMappingOptions",
        embedding_dependency: "_models.OnYourDataVectorizationSource",
        top_n_documents: Optional[int] = None,
        in_scope: Optional[bool] = None,
        strictness: Optional[int] = None,
        max_search_queries: Optional[int] = None,
        allow_partial_result: Optional[bool] = None,
        include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = None,
        authentication: Optional["_models.OnYourDataAuthenticationOptions"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureCosmosDBFieldMappingOptions(_model_base.Model):
    """Optional settings to control how fields are processed when using a configured Azure Cosmos DB
    resource.

    All required parameters must be populated in order to send to server.

    :ivar title_field: The name of the index field to use as a title.
    :vartype title_field: str
    :ivar url_field: The name of the index field to use as a URL.
    :vartype url_field: str
    :ivar filepath_field: The name of the index field to use as a filepath.
    :vartype filepath_field: str
    :ivar content_fields: The names of index fields that should be treated as content. Required.
    :vartype content_fields: list[str]
    :ivar content_fields_separator: The separator pattern that content fields should use.
    :vartype content_fields_separator: str
    :ivar vector_fields: The names of fields that represent vector data. Required.
    :vartype vector_fields: list[str]
    """

    title_field: Optional[str] = rest_field()
    """The name of the index field to use as a title."""
    url_field: Optional[str] = rest_field()
    """The name of the index field to use as a URL."""
    filepath_field: Optional[str] = rest_field()
    """The name of the index field to use as a filepath."""
    content_fields: List[str] = rest_field()
    """The names of index fields that should be treated as content. Required."""
    content_fields_separator: Optional[str] = rest_field()
    """The separator pattern that content fields should use."""
    vector_fields: List[str] = rest_field()
    """The names of fields that represent vector data. Required."""

    @overload
    def __init__(
        self,
        *,
        content_fields: List[str],
        vector_fields: List[str],
        title_field: Optional[str] = None,
        url_field: Optional[str] = None,
        filepath_field: Optional[str] = None,
        content_fields_separator: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureGroundingEnhancement(_model_base.Model):
    """The grounding enhancement that returns the bounding box of the objects detected in the image.


    :ivar lines: The lines of text detected by the grounding enhancement. Required.
    :vartype lines: list[~azure.openai.models.AzureGroundingEnhancementLine]
    """

    lines: List["_models.AzureGroundingEnhancementLine"] = rest_field()
    """The lines of text detected by the grounding enhancement. Required."""

    @overload
    def __init__(
        self,
        *,
        lines: List["_models.AzureGroundingEnhancementLine"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureGroundingEnhancementCoordinatePoint(_model_base.Model):
    """A representation of a single polygon point as used by the Azure grounding enhancement.


    :ivar x: The x-coordinate (horizontal axis) of the point. Required.
    :vartype x: float
    :ivar y: The y-coordinate (vertical axis) of the point. Required.
    :vartype y: float
    """

    x: float = rest_field()
    """The x-coordinate (horizontal axis) of the point. Required."""
    y: float = rest_field()
    """The y-coordinate (vertical axis) of the point. Required."""

    @overload
    def __init__(
        self,
        *,
        x: float,
        y: float,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureGroundingEnhancementLine(_model_base.Model):
    """A content line object consisting of an adjacent sequence of content elements, such as words and
    selection marks.


    :ivar text: The text within the line. Required.
    :vartype text: str
    :ivar spans: An array of spans that represent detected objects and its bounding box
     information. Required.
    :vartype spans: list[~azure.openai.models.AzureGroundingEnhancementLineSpan]
    """

    text: str = rest_field()
    """The text within the line. Required."""
    spans: List["_models.AzureGroundingEnhancementLineSpan"] = rest_field()
    """An array of spans that represent detected objects and its bounding box information. Required."""

    @overload
    def __init__(
        self,
        *,
        text: str,
        spans: List["_models.AzureGroundingEnhancementLineSpan"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureGroundingEnhancementLineSpan(_model_base.Model):
    """A span object that represents a detected object and its bounding box information.


    :ivar text: The text content of the span that represents the detected object. Required.
    :vartype text: str
    :ivar offset: The character offset within the text where the span begins. This offset is
     defined as the position of the first
     character of the span, counting from the start of the text as Unicode codepoints. Required.
    :vartype offset: int
    :ivar length: The length of the span in characters, measured in Unicode codepoints. Required.
    :vartype length: int
    :ivar polygon: An array of objects representing points in the polygon that encloses the
     detected object. Required.
    :vartype polygon: list[~azure.openai.models.AzureGroundingEnhancementCoordinatePoint]
    """

    text: str = rest_field()
    """The text content of the span that represents the detected object. Required."""
    offset: int = rest_field()
    """The character offset within the text where the span begins. This offset is defined as the
     position of the first
     character of the span, counting from the start of the text as Unicode codepoints. Required."""
    length: int = rest_field()
    """The length of the span in characters, measured in Unicode codepoints. Required."""
    polygon: List["_models.AzureGroundingEnhancementCoordinatePoint"] = rest_field()
    """An array of objects representing points in the polygon that encloses the detected object.
     Required."""

    @overload
    def __init__(
        self,
        *,
        text: str,
        offset: int,
        length: int,
        polygon: List["_models.AzureGroundingEnhancementCoordinatePoint"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureSearchChatExtensionConfiguration(AzureChatExtensionConfiguration, discriminator="azure_search"):
    """A specific representation of configurable options for Azure Search when using it as an Azure
    OpenAI chat
    extension.

    All required parameters must be populated in order to send to server.

    :ivar type: The type label to use when configuring Azure OpenAI chat extensions. This should
     typically not be changed from its
     default value for Azure Cognitive Search. Required. Represents the use of Azure AI Search as
     an Azure OpenAI chat extension.
    :vartype type: str or ~azure.openai.models.AZURE_SEARCH
    :ivar parameters: The parameters to use when configuring Azure Search. Required.
    :vartype parameters: ~azure.openai.models.AzureSearchChatExtensionParameters
    """

    type: Literal[AzureChatExtensionType.AZURE_SEARCH] = rest_discriminator(name="type")  # type: ignore
    """The type label to use when configuring Azure OpenAI chat extensions. This should typically not
     be changed from its
     default value for Azure Cognitive Search. Required. Represents the use of Azure AI Search as an
     Azure OpenAI chat extension."""
    parameters: "_models.AzureSearchChatExtensionParameters" = rest_field()
    """The parameters to use when configuring Azure Search. Required."""

    @overload
    def __init__(
        self,
        *,
        parameters: "_models.AzureSearchChatExtensionParameters",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=AzureChatExtensionType.AZURE_SEARCH, **kwargs)


class AzureSearchChatExtensionParameters(_model_base.Model):
    """Parameters for Azure Cognitive Search when used as an Azure OpenAI chat extension. The
    supported authentication types are APIKey, SystemAssignedManagedIdentity and
    UserAssignedManagedIdentity.

    All required parameters must be populated in order to send to server.

    :ivar top_n_documents: The configured top number of documents to feature for the configured
     query.
    :vartype top_n_documents: int
    :ivar in_scope: Whether queries should be restricted to use of indexed data.
    :vartype in_scope: bool
    :ivar strictness: The configured strictness of the search relevance filtering. The higher of
     strictness, the higher of the precision but lower recall of the answer.
    :vartype strictness: int
    :ivar max_search_queries: The max number of rewritten queries should be send to search provider
     for one user message. If not specified,
     the system will decide the number of queries to send.
    :vartype max_search_queries: int
    :ivar allow_partial_result: If specified as true, the system will allow partial search results
     to be used and the request fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails.
    :vartype allow_partial_result: bool
    :ivar include_contexts: The included properties of the output context. If not specified, the
     default value is ``citations`` and ``intent``.
    :vartype include_contexts: list[str or ~azure.openai.models.OnYourDataContextProperty]
    :ivar authentication: The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication.
    :vartype authentication: ~azure.openai.models.OnYourDataAuthenticationOptions
    :ivar endpoint: The absolute endpoint path for the Azure Cognitive Search resource to use.
     Required.
    :vartype endpoint: str
    :ivar index_name: The name of the index to use as available in the referenced Azure Cognitive
     Search resource. Required.
    :vartype index_name: str
    :ivar fields_mapping: Customized field mapping behavior to use when interacting with the search
     index.
    :vartype fields_mapping: ~azure.openai.models.AzureSearchIndexFieldMappingOptions
    :ivar query_type: The query type to use with Azure Cognitive Search. Known values are:
     "simple", "semantic", "vector", "vector_simple_hybrid", and "vector_semantic_hybrid".
    :vartype query_type: str or ~azure.openai.models.AzureSearchQueryType
    :ivar semantic_configuration: The additional semantic configuration for the query.
    :vartype semantic_configuration: str
    :ivar filter: Search filter.
    :vartype filter: str
    :ivar embedding_dependency: The embedding dependency for vector search.
    :vartype embedding_dependency: ~azure.openai.models.OnYourDataVectorizationSource
    """

    top_n_documents: Optional[int] = rest_field()
    """The configured top number of documents to feature for the configured query."""
    in_scope: Optional[bool] = rest_field()
    """Whether queries should be restricted to use of indexed data."""
    strictness: Optional[int] = rest_field()
    """The configured strictness of the search relevance filtering. The higher of strictness, the
     higher of the precision but lower recall of the answer."""
    max_search_queries: Optional[int] = rest_field()
    """The max number of rewritten queries should be send to search provider for one user message. If
     not specified,
     the system will decide the number of queries to send."""
    allow_partial_result: Optional[bool] = rest_field()
    """If specified as true, the system will allow partial search results to be used and the request
     fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails."""
    include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = rest_field()
    """The included properties of the output context. If not specified, the default value is
     ``citations`` and ``intent``."""
    authentication: Optional["_models.OnYourDataAuthenticationOptions"] = rest_field()
    """The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication."""
    endpoint: str = rest_field()
    """The absolute endpoint path for the Azure Cognitive Search resource to use. Required."""
    index_name: str = rest_field()
    """The name of the index to use as available in the referenced Azure Cognitive Search resource.
     Required."""
    fields_mapping: Optional["_models.AzureSearchIndexFieldMappingOptions"] = rest_field()
    """Customized field mapping behavior to use when interacting with the search index."""
    query_type: Optional[Union[str, "_models.AzureSearchQueryType"]] = rest_field()
    """The query type to use with Azure Cognitive Search. Known values are: \"simple\", \"semantic\",
     \"vector\", \"vector_simple_hybrid\", and \"vector_semantic_hybrid\"."""
    semantic_configuration: Optional[str] = rest_field()
    """The additional semantic configuration for the query."""
    filter: Optional[str] = rest_field()
    """Search filter."""
    embedding_dependency: Optional["_models.OnYourDataVectorizationSource"] = rest_field()
    """The embedding dependency for vector search."""

    @overload
    def __init__(
        self,
        *,
        endpoint: str,
        index_name: str,
        top_n_documents: Optional[int] = None,
        in_scope: Optional[bool] = None,
        strictness: Optional[int] = None,
        max_search_queries: Optional[int] = None,
        allow_partial_result: Optional[bool] = None,
        include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = None,
        authentication: Optional["_models.OnYourDataAuthenticationOptions"] = None,
        fields_mapping: Optional["_models.AzureSearchIndexFieldMappingOptions"] = None,
        query_type: Optional[Union[str, "_models.AzureSearchQueryType"]] = None,
        semantic_configuration: Optional[str] = None,
        filter: Optional[str] = None,  # pylint: disable=redefined-builtin
        embedding_dependency: Optional["_models.OnYourDataVectorizationSource"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class AzureSearchIndexFieldMappingOptions(_model_base.Model):
    """Optional settings to control how fields are processed when using a configured Azure Search
    resource.

    :ivar title_field: The name of the index field to use as a title.
    :vartype title_field: str
    :ivar url_field: The name of the index field to use as a URL.
    :vartype url_field: str
    :ivar filepath_field: The name of the index field to use as a filepath.
    :vartype filepath_field: str
    :ivar content_fields: The names of index fields that should be treated as content.
    :vartype content_fields: list[str]
    :ivar content_fields_separator: The separator pattern that content fields should use.
    :vartype content_fields_separator: str
    :ivar vector_fields: The names of fields that represent vector data.
    :vartype vector_fields: list[str]
    :ivar image_vector_fields: The names of fields that represent image vector data.
    :vartype image_vector_fields: list[str]
    """

    title_field: Optional[str] = rest_field()
    """The name of the index field to use as a title."""
    url_field: Optional[str] = rest_field()
    """The name of the index field to use as a URL."""
    filepath_field: Optional[str] = rest_field()
    """The name of the index field to use as a filepath."""
    content_fields: Optional[List[str]] = rest_field()
    """The names of index fields that should be treated as content."""
    content_fields_separator: Optional[str] = rest_field()
    """The separator pattern that content fields should use."""
    vector_fields: Optional[List[str]] = rest_field()
    """The names of fields that represent vector data."""
    image_vector_fields: Optional[List[str]] = rest_field()
    """The names of fields that represent image vector data."""

    @overload
    def __init__(
        self,
        *,
        title_field: Optional[str] = None,
        url_field: Optional[str] = None,
        filepath_field: Optional[str] = None,
        content_fields: Optional[List[str]] = None,
        content_fields_separator: Optional[str] = None,
        vector_fields: Optional[List[str]] = None,
        image_vector_fields: Optional[List[str]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class Batch(_model_base.Model):
    """The Batch object.

    Readonly variables are only populated by the server, and will be ignored when sending a request.


    :ivar id: The id assigned to the Batch. Required.
    :vartype id: str
    :ivar object: The object type, which is always ``batch``. Required. Default value is "batch".
    :vartype object: str
    :ivar endpoint: The OpenAI API endpoint used by the batch.
    :vartype endpoint: str
    :ivar errors: The list of Batch errors.
    :vartype errors: ~azure.openai.models.BatchErrorList
    :ivar input_file_id: The ID of the input file for the batch. Required.
    :vartype input_file_id: str
    :ivar completion_window: The time frame within which the batch should be processed.
    :vartype completion_window: str
    :ivar status: The current status of the batch. Known values are: "validating", "failed",
     "in_progress", "finalizing", "completed", "expired", "cancelling", and "cancelled".
    :vartype status: str or ~azure.openai.models.BatchStatus
    :ivar output_file_id: The ID of the file containing the outputs of successfully executed
     requests.
    :vartype output_file_id: str
    :ivar error_file_id: The ID of the file containing the outputs of requests with errors.
    :vartype error_file_id: str
    :ivar created_at: The Unix timestamp (in seconds) for when the batch was created.
    :vartype created_at: ~datetime.datetime
    :ivar in_progress_at: The Unix timestamp (in seconds) for when the batch started processing.
    :vartype in_progress_at: ~datetime.datetime
    :ivar expires_at: The Unix timestamp (in seconds) for when the batch will expire.
    :vartype expires_at: ~datetime.datetime
    :ivar finalizing_at: The Unix timestamp (in seconds) for when the batch started finalizing.
    :vartype finalizing_at: ~datetime.datetime
    :ivar completed_at: The Unix timestamp (in seconds) for when the batch was completed.
    :vartype completed_at: ~datetime.datetime
    :ivar failed_at: The Unix timestamp (in seconds) for when the batch failed.
    :vartype failed_at: ~datetime.datetime
    :ivar expired_at: The Unix timestamp (in seconds) for when the batch expired.
    :vartype expired_at: ~datetime.datetime
    :ivar cancelling_at: The Unix timestamp (in seconds) for when the batch started cancelling.
    :vartype cancelling_at: ~datetime.datetime
    :ivar cancelled_at: The Unix timestamp (in seconds) for when the batch was cancelled.
    :vartype cancelled_at: ~datetime.datetime
    :ivar request_counts: The request counts for different statuses within the batch.
    :vartype request_counts: ~azure.openai.models.BatchRequestCounts
    :ivar metadata: A set of key-value pairs that can be attached to the batch. This can be useful
     for storing additional information about the batch in a structured format.
    :vartype metadata: dict[str, str]
    """

    id: str = rest_field()
    """The id assigned to the Batch. Required."""
    object: Literal["batch"] = rest_field()
    """The object type, which is always ``batch``. Required. Default value is \"batch\"."""
    endpoint: Optional[str] = rest_field()
    """The OpenAI API endpoint used by the batch."""
    errors: Optional["_models.BatchErrorList"] = rest_field()
    """The list of Batch errors."""
    input_file_id: str = rest_field()
    """The ID of the input file for the batch. Required."""
    completion_window: Optional[str] = rest_field()
    """The time frame within which the batch should be processed."""
    status: Optional[Union[str, "_models.BatchStatus"]] = rest_field()
    """The current status of the batch. Known values are: \"validating\", \"failed\", \"in_progress\",
     \"finalizing\", \"completed\", \"expired\", \"cancelling\", and \"cancelled\"."""
    output_file_id: Optional[str] = rest_field()
    """The ID of the file containing the outputs of successfully executed requests."""
    error_file_id: Optional[str] = rest_field()
    """The ID of the file containing the outputs of requests with errors."""
    created_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch was created."""
    in_progress_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch started processing."""
    expires_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch will expire."""
    finalizing_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch started finalizing."""
    completed_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch was completed."""
    failed_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch failed."""
    expired_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch expired."""
    cancelling_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch started cancelling."""
    cancelled_at: Optional[datetime.datetime] = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the batch was cancelled."""
    request_counts: Optional["_models.BatchRequestCounts"] = rest_field()
    """The request counts for different statuses within the batch."""
    metadata: Optional[Dict[str, str]] = rest_field()
    """A set of key-value pairs that can be attached to the batch. This can be useful for storing
     additional information about the batch in a structured format."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        input_file_id: str,
        endpoint: Optional[str] = None,
        errors: Optional["_models.BatchErrorList"] = None,
        completion_window: Optional[str] = None,
        status: Optional[Union[str, "_models.BatchStatus"]] = None,
        output_file_id: Optional[str] = None,
        error_file_id: Optional[str] = None,
        created_at: Optional[datetime.datetime] = None,
        in_progress_at: Optional[datetime.datetime] = None,
        expires_at: Optional[datetime.datetime] = None,
        finalizing_at: Optional[datetime.datetime] = None,
        completed_at: Optional[datetime.datetime] = None,
        failed_at: Optional[datetime.datetime] = None,
        expired_at: Optional[datetime.datetime] = None,
        cancelling_at: Optional[datetime.datetime] = None,
        cancelled_at: Optional[datetime.datetime] = None,
        request_counts: Optional["_models.BatchRequestCounts"] = None,
        metadata: Optional[Dict[str, str]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.object: Literal["batch"] = "batch"


class BatchCreateRequest(_model_base.Model):
    """Defines the request to create a batch.

    All required parameters must be populated in order to send to server.

    :ivar endpoint: The API endpoint used by the batch. Required.
    :vartype endpoint: str
    :ivar input_file_id: The ID of the input file for the batch. Required.
    :vartype input_file_id: str
    :ivar completion_window: The time frame within which the batch should be processed. Required.
    :vartype completion_window: str
    :ivar metadata: A set of key-value pairs that can be attached to the batch. This can be useful
     for storing additional information about the batch in a structured format.
    :vartype metadata: dict[str, str]
    """

    endpoint: str = rest_field()
    """The API endpoint used by the batch. Required."""
    input_file_id: str = rest_field()
    """The ID of the input file for the batch. Required."""
    completion_window: str = rest_field()
    """The time frame within which the batch should be processed. Required."""
    metadata: Optional[Dict[str, str]] = rest_field()
    """A set of key-value pairs that can be attached to the batch. This can be useful for storing
     additional information about the batch in a structured format."""

    @overload
    def __init__(
        self,
        *,
        endpoint: str,
        input_file_id: str,
        completion_window: str,
        metadata: Optional[Dict[str, str]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class BatchErrorDatum(_model_base.Model):
    """A Datum containing information about a Batch Error.

    :ivar code: An error code identifying the error type.
    :vartype code: str
    :ivar message: A human-readable message providing more details about the error.
    :vartype message: str
    :ivar param: The name of the parameter that caused the error, if applicable.
    :vartype param: str
    :ivar line: The line number of the input file where the error occurred, if applicable.
    :vartype line: int
    """

    code: Optional[str] = rest_field()
    """An error code identifying the error type."""
    message: Optional[str] = rest_field()
    """A human-readable message providing more details about the error."""
    param: Optional[str] = rest_field()
    """The name of the parameter that caused the error, if applicable."""
    line: Optional[int] = rest_field()
    """The line number of the input file where the error occurred, if applicable."""

    @overload
    def __init__(
        self,
        *,
        code: Optional[str] = None,
        message: Optional[str] = None,
        param: Optional[str] = None,
        line: Optional[int] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class BatchErrorList(_model_base.Model):
    """A list of Batch errors.

    Readonly variables are only populated by the server, and will be ignored when sending a request.


    :ivar object: The object type, which is always ``list``. Required. Default value is "list".
    :vartype object: str
    :ivar data: The list of Batch error data.
    :vartype data: list[~azure.openai.models.BatchErrorDatum]
    """

    object: Literal["list"] = rest_field()
    """The object type, which is always ``list``. Required. Default value is \"list\"."""
    data: Optional[List["_models.BatchErrorDatum"]] = rest_field()
    """The list of Batch error data."""

    @overload
    def __init__(
        self,
        *,
        data: Optional[List["_models.BatchErrorDatum"]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.object: Literal["list"] = "list"


class BatchRequestCounts(_model_base.Model):
    """BatchRequestCounts.

    :ivar total: Total number of requests in the batch.
    :vartype total: int
    :ivar completed: Number of requests that have been completed successfully.
    :vartype completed: int
    :ivar failed: Number of requests that have failed.
    :vartype failed: int
    """

    total: Optional[int] = rest_field()
    """Total number of requests in the batch."""
    completed: Optional[int] = rest_field()
    """Number of requests that have been completed successfully."""
    failed: Optional[int] = rest_field()
    """Number of requests that have failed."""

    @overload
    def __init__(
        self,
        *,
        total: Optional[int] = None,
        completed: Optional[int] = None,
        failed: Optional[int] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatChoice(_model_base.Model):
    """The representation of a single prompt completion as part of an overall chat completions
    request.
    Generally, ``n`` choices are generated per provided prompt with a default value of 1.
    Token limits and other settings may limit the number of choices generated.


    :ivar message: The chat message for a given chat completions prompt.
    :vartype message: ~azure.openai.models.ChatResponseMessage
    :ivar logprobs: The log probability information for this choice, as enabled via the 'logprobs'
     request option. Required.
    :vartype logprobs: ~azure.openai.models.ChatChoiceLogProbabilityInfo
    :ivar index: The ordered index associated with this chat completions choice. Required.
    :vartype index: int
    :ivar finish_reason: The reason that this chat completions choice completed its generated.
     Required. Known values are: "stop", "length", "content_filter", "function_call", and
     "tool_calls".
    :vartype finish_reason: str or ~azure.openai.models.CompletionsFinishReason
    :ivar delta: The delta message content for a streaming response.
    :vartype delta: ~azure.openai.models.ChatResponseMessage
    :ivar content_filter_results: Information about the content filtering category (hate, sexual,
     violence, self_harm), if it
     has been detected, as well as the severity level (very_low, low, medium, high-scale that
     determines the intensity and risk level of harmful content) and if it has been filtered or
     not.
    :vartype content_filter_results: ~azure.openai.models.ContentFilterResultsForChoice
    :ivar enhancements: Represents the output results of Azure OpenAI enhancements to chat
     completions, as configured via the matching input
     provided in the request. This supplementary information is only available when using Azure
     OpenAI and only when the
     request is configured to use enhancements.
    :vartype enhancements: ~azure.openai.models.AzureChatEnhancements
    """

    message: Optional["_models.ChatResponseMessage"] = rest_field()
    """The chat message for a given chat completions prompt."""
    logprobs: "_models.ChatChoiceLogProbabilityInfo" = rest_field()
    """The log probability information for this choice, as enabled via the 'logprobs' request option.
     Required."""
    index: int = rest_field()
    """The ordered index associated with this chat completions choice. Required."""
    finish_reason: Union[str, "_models.CompletionsFinishReason"] = rest_field()
    """The reason that this chat completions choice completed its generated. Required. Known values
     are: \"stop\", \"length\", \"content_filter\", \"function_call\", and \"tool_calls\"."""
    delta: Optional["_models.ChatResponseMessage"] = rest_field()
    """The delta message content for a streaming response."""
    content_filter_results: Optional["_models.ContentFilterResultsForChoice"] = rest_field()
    """Information about the content filtering category (hate, sexual, violence, self_harm), if it
     has been detected, as well as the severity level (very_low, low, medium, high-scale that
     determines the intensity and risk level of harmful content) and if it has been filtered or not."""
    enhancements: Optional["_models.AzureChatEnhancements"] = rest_field()
    """Represents the output results of Azure OpenAI enhancements to chat completions, as configured
     via the matching input
     provided in the request. This supplementary information is only available when using Azure
     OpenAI and only when the
     request is configured to use enhancements."""

    @overload
    def __init__(
        self,
        *,
        logprobs: "_models.ChatChoiceLogProbabilityInfo",
        index: int,
        finish_reason: Union[str, "_models.CompletionsFinishReason"],
        message: Optional["_models.ChatResponseMessage"] = None,
        delta: Optional["_models.ChatResponseMessage"] = None,
        content_filter_results: Optional["_models.ContentFilterResultsForChoice"] = None,
        enhancements: Optional["_models.AzureChatEnhancements"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatChoiceLogProbabilityInfo(_model_base.Model):
    """Log probability information for a choice, as requested via 'logprobs' and 'top_logprobs'.


    :ivar content: The list of log probability information entries for the choice's message content
     tokens, as requested via the 'logprobs' option. Required.
    :vartype content: list[~azure.openai.models.ChatTokenLogProbabilityResult]
    :ivar refusal: The list of log probability information entries for the choice's message refusal
     message tokens, as requested via the 'logprobs' option. Required.
    :vartype refusal: list[~azure.openai.models.ChatTokenLogProbabilityResult]
    """

    content: List["_models.ChatTokenLogProbabilityResult"] = rest_field()
    """The list of log probability information entries for the choice's message content tokens, as
     requested via the 'logprobs' option. Required."""
    refusal: List["_models.ChatTokenLogProbabilityResult"] = rest_field()
    """The list of log probability information entries for the choice's message refusal message
     tokens, as requested via the 'logprobs' option. Required."""

    @overload
    def __init__(
        self,
        *,
        content: List["_models.ChatTokenLogProbabilityResult"],
        refusal: List["_models.ChatTokenLogProbabilityResult"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletions(_model_base.Model):
    """Representation of the response data from a chat completions request.
    Completions support a wide variety of tasks and generate text that continues from or
    "completes"
    provided prompt data.


    :ivar id: A unique identifier associated with this chat completions response. Required.
    :vartype id: str
    :ivar created: The first timestamp associated with generation activity for this completions
     response,
     represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
    :vartype created: ~datetime.datetime
    :ivar choices: The collection of completions choices associated with this completions response.
     Generally, ``n`` choices are generated per provided prompt with a default value of 1.
     Token limits and other settings may limit the number of choices generated. Required.
    :vartype choices: list[~azure.openai.models.ChatChoice]
    :ivar model: The model name used for this completions request.
    :vartype model: str
    :ivar prompt_filter_results: Content filtering results for zero or more prompts in the request.
     In a streaming request,
     results for different prompts may arrive at different times or in different orders.
    :vartype prompt_filter_results: list[~azure.openai.models.ContentFilterResultsForPrompt]
    :ivar system_fingerprint: Can be used in conjunction with the ``seed`` request parameter to
     understand when backend changes have been made that
     might impact determinism.
    :vartype system_fingerprint: str
    :ivar usage: Usage information for tokens processed and generated as part of this completions
     operation. Required.
    :vartype usage: ~azure.openai.models.CompletionsUsage
    """

    id: str = rest_field()
    """A unique identifier associated with this chat completions response. Required."""
    created: datetime.datetime = rest_field(format="unix-timestamp")
    """The first timestamp associated with generation activity for this completions response,
     represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required."""
    choices: List["_models.ChatChoice"] = rest_field()
    """The collection of completions choices associated with this completions response.
     Generally, ``n`` choices are generated per provided prompt with a default value of 1.
     Token limits and other settings may limit the number of choices generated. Required."""
    model: Optional[str] = rest_field()
    """The model name used for this completions request."""
    prompt_filter_results: Optional[List["_models.ContentFilterResultsForPrompt"]] = rest_field()
    """Content filtering results for zero or more prompts in the request. In a streaming request,
     results for different prompts may arrive at different times or in different orders."""
    system_fingerprint: Optional[str] = rest_field()
    """Can be used in conjunction with the ``seed`` request parameter to understand when backend
     changes have been made that
     might impact determinism."""
    usage: "_models.CompletionsUsage" = rest_field()
    """Usage information for tokens processed and generated as part of this completions operation.
     Required."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        created: datetime.datetime,
        choices: List["_models.ChatChoice"],
        usage: "_models.CompletionsUsage",
        model: Optional[str] = None,
        prompt_filter_results: Optional[List["_models.ContentFilterResultsForPrompt"]] = None,
        system_fingerprint: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsToolCall(_model_base.Model):
    """An abstract representation of a tool call that must be resolved in a subsequent request to
    perform the requested
    chat completion.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    ChatCompletionsFunctionToolCall


    :ivar type: The object type. Required. Default value is None.
    :vartype type: str
    :ivar id: The ID of the tool call. Required.
    :vartype id: str
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The object type. Required. Default value is None."""
    id: str = rest_field()
    """The ID of the tool call. Required."""

    @overload
    def __init__(
        self,
        *,
        type: str,
        id: str,  # pylint: disable=redefined-builtin
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsFunctionToolCall(ChatCompletionsToolCall, discriminator="function"):
    """A tool call to a function tool, issued by the model in evaluation of a configured function
    tool, that represents
    a function invocation needed for a subsequent chat completions request to resolve.


    :ivar id: The ID of the tool call. Required.
    :vartype id: str
    :ivar type: The type of tool call, in this case always 'function'. Required. Default value is
     "function".
    :vartype type: str
    :ivar function: The details of the function invocation requested by the tool call. Required.
    :vartype function: ~azure.openai.models.FunctionCall
    """

    type: Literal["function"] = rest_discriminator(name="type")  # type: ignore
    """The type of tool call, in this case always 'function'. Required. Default value is \"function\"."""
    function: "_models.FunctionCall" = rest_field()
    """The details of the function invocation requested by the tool call. Required."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        function: "_models.FunctionCall",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="function", **kwargs)


class ChatCompletionsToolDefinition(_model_base.Model):
    """An abstract representation of a tool that can be used by the model to improve a chat
    completions response.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    ChatCompletionsFunctionToolDefinition

    All required parameters must be populated in order to send to server.

    :ivar type: The object type. Required. Default value is None.
    :vartype type: str
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The object type. Required. Default value is None."""

    @overload
    def __init__(
        self,
        *,
        type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsFunctionToolDefinition(ChatCompletionsToolDefinition, discriminator="function"):
    """The definition information for a chat completions function tool that can call a function in
    response to a tool call.

    All required parameters must be populated in order to send to server.

    :ivar type: The object name, which is always 'function'. Required. Default value is "function".
    :vartype type: str
    :ivar function: The function definition details for the function tool. Required.
    :vartype function: ~azure.openai.models.ChatCompletionsFunctionToolDefinitionFunction
    """

    type: Literal["function"] = rest_discriminator(name="type")  # type: ignore
    """The object name, which is always 'function'. Required. Default value is \"function\"."""
    function: "_models.ChatCompletionsFunctionToolDefinitionFunction" = rest_field()
    """The function definition details for the function tool. Required."""

    @overload
    def __init__(
        self,
        *,
        function: "_models.ChatCompletionsFunctionToolDefinitionFunction",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="function", **kwargs)


class ChatCompletionsFunctionToolDefinitionFunction(_model_base.Model):  # pylint: disable=name-too-long
    """ChatCompletionsFunctionToolDefinitionFunction.

    All required parameters must be populated in order to send to server.

    :ivar description: A description of what the function does, used by the model to choose when
     and how to call the function.
    :vartype description: str
    :ivar name: The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
     underscores and dashes, with a maximum length of 64. Required.
    :vartype name: str
    :ivar parameters:
    :vartype parameters: any
    :ivar strict: Whether to enable strict schema adherence when generating the function call. If
     set to true, the model will follow the exact schema defined in the ``parameters`` field. Only a
     subset of JSON Schema is supported when ``strict`` is ``true``. Learn more about Structured
     Outputs in the `function calling guide <docs/guides/function-calling>`_.
    :vartype strict: bool
    """

    description: Optional[str] = rest_field()
    """A description of what the function does, used by the model to choose when and how to call the
     function."""
    name: str = rest_field()
    """The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
     dashes, with a maximum length of 64. Required."""
    parameters: Optional[Any] = rest_field()
    strict: Optional[bool] = rest_field()
    """Whether to enable strict schema adherence when generating the function call. If set to true,
     the model will follow the exact schema defined in the ``parameters`` field. Only a subset of
     JSON Schema is supported when ``strict`` is ``true``. Learn more about Structured Outputs in
     the `function calling guide <docs/guides/function-calling>`_."""

    @overload
    def __init__(
        self,
        *,
        name: str,
        description: Optional[str] = None,
        parameters: Optional[Any] = None,
        strict: Optional[bool] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsFunctionToolSelection(_model_base.Model):
    """A tool selection of a specific, named function tool that will limit chat completions to using
    the named function.

    All required parameters must be populated in order to send to server.

    :ivar name: The name of the function that should be called. Required.
    :vartype name: str
    """

    name: str = rest_field()
    """The name of the function that should be called. Required."""

    @overload
    def __init__(
        self,
        *,
        name: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsResponseFormat(_model_base.Model):
    """An abstract representation of a response format configuration usable by Chat Completions. Can
    be used to enable JSON
    mode.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    ChatCompletionsJsonResponseFormat, ChatCompletionsJsonSchemaResponseFormat,
    ChatCompletionsTextResponseFormat

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminated type for the response format. Required. Default value is None.
    :vartype type: str
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The discriminated type for the response format. Required. Default value is None."""

    @overload
    def __init__(
        self,
        *,
        type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsJsonResponseFormat(ChatCompletionsResponseFormat, discriminator="json_object"):
    """A response format for Chat Completions that restricts responses to emitting valid JSON objects.

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminated object type, which is always 'json_object' for this format.
     Required. Default value is "json_object".
    :vartype type: str
    """

    type: Literal["json_object"] = rest_discriminator(name="type")  # type: ignore
    """The discriminated object type, which is always 'json_object' for this format. Required. Default
     value is \"json_object\"."""

    @overload
    def __init__(
        self,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="json_object", **kwargs)


class ChatCompletionsJsonSchemaResponseFormat(ChatCompletionsResponseFormat, discriminator="json_schema"):
    """A response format for Chat Completions that restricts responses to emitting JSON that conforms
    to a provided JSON
    Schema for Structured Outputs.

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminated object type, which is always 'json_schema' for this format.
     Required. Default value is "json_schema".
    :vartype type: str
    :ivar json_schema: Required.
    :vartype json_schema: ~azure.openai.models.ChatCompletionsJsonSchemaResponseFormatJsonSchema
    """

    type: Literal["json_schema"] = rest_discriminator(name="type")  # type: ignore
    """The discriminated object type, which is always 'json_schema' for this format. Required. Default
     value is \"json_schema\"."""
    json_schema: "_models.ChatCompletionsJsonSchemaResponseFormatJsonSchema" = rest_field()
    """Required."""

    @overload
    def __init__(
        self,
        *,
        json_schema: "_models.ChatCompletionsJsonSchemaResponseFormatJsonSchema",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="json_schema", **kwargs)


class ChatCompletionsJsonSchemaResponseFormatJsonSchema(_model_base.Model):  # pylint: disable=name-too-long
    """ChatCompletionsJsonSchemaResponseFormatJsonSchema.

    All required parameters must be populated in order to send to server.

    :ivar description: A description of what the response format is for, used by the model to
     determine how to respond in the format.
    :vartype description: str
    :ivar name: The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and
     dashes, with a maximum length of 64. Required.
    :vartype name: str
    :ivar schema:
    :vartype schema: any
    :ivar strict: Whether to enable strict schema adherence when generating the output. If set to
     true, the model will always follow the exact schema defined in the ``schema`` field. Only a
     subset of JSON Schema is supported when ``strict`` is ``true``. To learn more, read the
     `Structured Outputs guide </docs/guides/structured-outputs>`_.
    :vartype strict: bool
    """

    description: Optional[str] = rest_field()
    """A description of what the response format is for, used by the model to determine how to respond
     in the format."""
    name: str = rest_field()
    """The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with
     a maximum length of 64. Required."""
    schema: Optional[Any] = rest_field()
    strict: Optional[bool] = rest_field()
    """Whether to enable strict schema adherence when generating the output. If set to true, the model
     will always follow the exact schema defined in the ``schema`` field. Only a subset of JSON
     Schema is supported when ``strict`` is ``true``. To learn more, read the `Structured Outputs
     guide </docs/guides/structured-outputs>`_."""

    @overload
    def __init__(
        self,
        *,
        name: str,
        description: Optional[str] = None,
        schema: Optional[Any] = None,
        strict: Optional[bool] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsNamedToolSelection(_model_base.Model):
    """An abstract representation of an explicit, named tool selection to use for a chat completions
    request.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    ChatCompletionsNamedFunctionToolSelection

    All required parameters must be populated in order to send to server.

    :ivar type: The object type. Required. Default value is None.
    :vartype type: str
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The object type. Required. Default value is None."""

    @overload
    def __init__(
        self,
        *,
        type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsNamedFunctionToolSelection(
    ChatCompletionsNamedToolSelection, discriminator="function"
):  # pylint: disable=name-too-long
    """A tool selection of a specific, named function tool that will limit chat completions to using
    the named function.

    All required parameters must be populated in order to send to server.

    :ivar type: The object type, which is always 'function'. Required. Default value is "function".
    :vartype type: str
    :ivar function: The function that should be called. Required.
    :vartype function: ~azure.openai.models.ChatCompletionsFunctionToolSelection
    """

    type: Literal["function"] = rest_discriminator(name="type")  # type: ignore
    """The object type, which is always 'function'. Required. Default value is \"function\"."""
    function: "_models.ChatCompletionsFunctionToolSelection" = rest_field()
    """The function that should be called. Required."""

    @overload
    def __init__(
        self,
        *,
        function: "_models.ChatCompletionsFunctionToolSelection",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="function", **kwargs)


class ChatCompletionsOptions(_model_base.Model):
    """The configuration information for a chat completions request.
    Completions support a wide variety of tasks and generate text that continues from or
    "completes"
    provided prompt data.

    All required parameters must be populated in order to send to server.

    :ivar messages: The collection of context messages associated with this chat completions
     request.
     Typical usage begins with a chat message for the System role that provides instructions for
     the behavior of the assistant, followed by alternating messages between the User and
     Assistant roles. Required.
    :vartype messages: list[~azure.openai.models.ChatRequestMessage]
    :ivar functions: A list of functions the model may generate JSON inputs for.
    :vartype functions: list[~azure.openai.models.FunctionDefinition]
    :ivar function_call: Controls how the model responds to function calls. "none" means the model
     does not call a function,
     and responds to the end-user. "auto" means the model can pick between an end-user or calling a
     function.
      Specifying a particular function via ``{"name": "my_function"}`` forces the model to call
     that function.
      "none" is the default when no functions are present. "auto" is the default if functions are
     present. Is either a Union[str, "_models.FunctionCallPreset"] type or a FunctionName type.
    :vartype function_call: str or ~azure.openai.models.FunctionCallPreset or
     ~azure.openai.models.FunctionName
    :ivar max_tokens: The maximum number of tokens allowed for the generated answer. By default,
     the number of tokens the model can return will be (4096 - prompt tokens).

     This value is now deprecated in favor of ``max_completion_tokens``\\ , and is not compatible
     with o1 series models.
    :vartype max_tokens: int
    :ivar max_completion_tokens: An upper bound for the number of tokens that can be generated for
     a completion, including visible output tokens and reasoning tokens.
    :vartype max_completion_tokens: int
    :ivar temperature: The sampling temperature to use that controls the apparent creativity of
     generated completions.
     Higher values will make output more random while lower values will make results more focused
     and deterministic.
     It is not recommended to modify temperature and top_p for the same completions request as the
     interaction of these two settings is difficult to predict.
    :vartype temperature: float
    :ivar top_p: An alternative to sampling with temperature called nucleus sampling. This value
     causes the
     model to consider the results of tokens with the provided probability mass. As an example, a
     value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
     considered.
     It is not recommended to modify temperature and top_p for the same completions request as the
     interaction of these two settings is difficult to predict.
    :vartype top_p: float
    :ivar logit_bias: A map between GPT token IDs and bias scores that influences the probability
     of specific tokens
     appearing in a completions response. Token IDs are computed via external tokenizer tools,
     while
     bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding
     to
     a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
     score varies by model.
    :vartype logit_bias: dict[str, int]
    :ivar user: An identifier for the caller or end user of the operation. This may be used for
     tracking
     or rate-limiting purposes.
    :vartype user: str
    :ivar n: The number of chat completions choices that should be generated for a chat completions
     response.
     Because this setting can generate many completions, it may quickly consume your token quota.
     Use carefully and ensure reasonable settings for max_tokens and stop.
    :vartype n: int
    :ivar stop: A collection of textual sequences that will end completions generation.
    :vartype stop: list[str]
    :ivar presence_penalty: A value that influences the probability of generated tokens appearing
     based on their existing
     presence in generated text.
     Positive values will make tokens less likely to appear when they already exist and increase
     the
     model's likelihood to output new topics.
    :vartype presence_penalty: float
    :ivar frequency_penalty: A value that influences the probability of generated tokens appearing
     based on their cumulative
     frequency in generated text.
     Positive values will make tokens less likely to appear as their frequency increases and
     decrease the likelihood of the model repeating the same statements verbatim.
    :vartype frequency_penalty: float
    :ivar stream: A value indicating whether chat completions should be streamed for this request.
    :vartype stream: bool
    :ivar stream_options: Options for streaming response. Only set this when you set ``stream:
     true``.
    :vartype stream_options: ~azure.openai.models.ChatCompletionStreamOptions
    :ivar model: The model name to provide as part of this completions request.
     Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     resource URI that's connected to.
    :vartype model: str
    :ivar data_sources: The configuration entries for Azure OpenAI chat extensions that use them.
       This additional specification is only compatible with Azure OpenAI.
    :vartype data_sources: list[~azure.openai.models.AzureChatExtensionConfiguration]
    :ivar enhancements: If provided, the configuration options for available Azure OpenAI chat
     enhancements.
    :vartype enhancements: ~azure.openai.models.AzureChatEnhancementConfiguration
    :ivar seed: If specified, the system will make a best effort to sample deterministically such
     that repeated requests with the
     same seed and parameters should return the same result. Determinism is not guaranteed, and you
     should refer to the
     system_fingerprint response parameter to monitor changes in the backend.".
    :vartype seed: int
    :ivar logprobs: Whether to return log probabilities of the output tokens or not. If true,
     returns the log probabilities of each output token returned in the ``content`` of ``message``.
     This option is currently not available on the ``gpt-4-vision-preview`` model.
    :vartype logprobs: bool
    :ivar top_logprobs: An integer between 0 and 5 specifying the number of most likely tokens to
     return at each token position, each with an associated log probability. ``logprobs`` must be
     set to ``true`` if this parameter is used.
    :vartype top_logprobs: int
    :ivar response_format: An object specifying the format that the model must output. Used to
     enable JSON mode.
    :vartype response_format: ~azure.openai.models.ChatCompletionsResponseFormat
    :ivar tools: The available tool definitions that the chat completions request can use,
     including caller-defined functions.
    :vartype tools: list[~azure.openai.models.ChatCompletionsToolDefinition]
    :ivar tool_choice: If specified, the model will configure which of the provided tools it can
     use for the chat completions response. Is either a Union[str,
     "_models.ChatCompletionsToolSelectionPreset"] type or a ChatCompletionsNamedToolSelection type.
    :vartype tool_choice: str or ~azure.openai.models.ChatCompletionsToolSelectionPreset or
     ~azure.openai.models.ChatCompletionsNamedToolSelection
    :ivar parallel_tool_calls: Whether to enable parallel function calling during tool use.
    :vartype parallel_tool_calls: bool
    """

    messages: List["_models.ChatRequestMessage"] = rest_field()
    """The collection of context messages associated with this chat completions request.
     Typical usage begins with a chat message for the System role that provides instructions for
     the behavior of the assistant, followed by alternating messages between the User and
     Assistant roles. Required."""
    functions: Optional[List["_models.FunctionDefinition"]] = rest_field()
    """A list of functions the model may generate JSON inputs for."""
    function_call: Optional[Union[str, "_models.FunctionCallPreset", "_models.FunctionName"]] = rest_field()
    """Controls how the model responds to function calls. \"none\" means the model does not call a
     function,
     and responds to the end-user. \"auto\" means the model can pick between an end-user or calling
     a function.
      Specifying a particular function via ``{\"name\": \"my_function\"}`` forces the model to call
     that function.
      \"none\" is the default when no functions are present. \"auto\" is the default if functions
     are present. Is either a Union[str, \"_models.FunctionCallPreset\"] type or a FunctionName
     type."""
    max_tokens: Optional[int] = rest_field()
    """The maximum number of tokens allowed for the generated answer. By default, the number of tokens
     the model can return will be (4096 - prompt tokens).
     
     This value is now deprecated in favor of ``max_completion_tokens``\ , and is not compatible
     with o1 series models."""
    max_completion_tokens: Optional[int] = rest_field()
    """An upper bound for the number of tokens that can be generated for a completion, including
     visible output tokens and reasoning tokens."""
    temperature: Optional[float] = rest_field()
    """The sampling temperature to use that controls the apparent creativity of generated completions.
     Higher values will make output more random while lower values will make results more focused
     and deterministic.
     It is not recommended to modify temperature and top_p for the same completions request as the
     interaction of these two settings is difficult to predict."""
    top_p: Optional[float] = rest_field()
    """An alternative to sampling with temperature called nucleus sampling. This value causes the
     model to consider the results of tokens with the provided probability mass. As an example, a
     value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
     considered.
     It is not recommended to modify temperature and top_p for the same completions request as the
     interaction of these two settings is difficult to predict."""
    logit_bias: Optional[Dict[str, int]] = rest_field()
    """A map between GPT token IDs and bias scores that influences the probability of specific tokens
     appearing in a completions response. Token IDs are computed via external tokenizer tools, while
     bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to
     a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
     score varies by model."""
    user: Optional[str] = rest_field()
    """An identifier for the caller or end user of the operation. This may be used for tracking
     or rate-limiting purposes."""
    n: Optional[int] = rest_field()
    """The number of chat completions choices that should be generated for a chat completions
     response.
     Because this setting can generate many completions, it may quickly consume your token quota.
     Use carefully and ensure reasonable settings for max_tokens and stop."""
    stop: Optional[List[str]] = rest_field()
    """A collection of textual sequences that will end completions generation."""
    presence_penalty: Optional[float] = rest_field()
    """A value that influences the probability of generated tokens appearing based on their existing
     presence in generated text.
     Positive values will make tokens less likely to appear when they already exist and increase the
     model's likelihood to output new topics."""
    frequency_penalty: Optional[float] = rest_field()
    """A value that influences the probability of generated tokens appearing based on their cumulative
     frequency in generated text.
     Positive values will make tokens less likely to appear as their frequency increases and
     decrease the likelihood of the model repeating the same statements verbatim."""
    stream: Optional[bool] = rest_field()
    """A value indicating whether chat completions should be streamed for this request."""
    stream_options: Optional["_models.ChatCompletionStreamOptions"] = rest_field()
    """Options for streaming response. Only set this when you set ``stream: true``."""
    model: Optional[str] = rest_field()
    """The model name to provide as part of this completions request.
     Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     resource URI that's connected to."""
    data_sources: Optional[List["_models.AzureChatExtensionConfiguration"]] = rest_field()
    """The configuration entries for Azure OpenAI chat extensions that use them.
       This additional specification is only compatible with Azure OpenAI."""
    enhancements: Optional["_models.AzureChatEnhancementConfiguration"] = rest_field()
    """If provided, the configuration options for available Azure OpenAI chat enhancements."""
    seed: Optional[int] = rest_field()
    """If specified, the system will make a best effort to sample deterministically such that repeated
     requests with the
     same seed and parameters should return the same result. Determinism is not guaranteed, and you
     should refer to the
     system_fingerprint response parameter to monitor changes in the backend.\"."""
    logprobs: Optional[bool] = rest_field()
    """Whether to return log probabilities of the output tokens or not. If true, returns the log
     probabilities of each output token returned in the ``content`` of ``message``. This option is
     currently not available on the ``gpt-4-vision-preview`` model."""
    top_logprobs: Optional[int] = rest_field()
    """An integer between 0 and 5 specifying the number of most likely tokens to return at each token
     position, each with an associated log probability. ``logprobs`` must be set to ``true`` if this
     parameter is used."""
    response_format: Optional["_models.ChatCompletionsResponseFormat"] = rest_field()
    """An object specifying the format that the model must output. Used to enable JSON mode."""
    tools: Optional[List["_models.ChatCompletionsToolDefinition"]] = rest_field()
    """The available tool definitions that the chat completions request can use, including
     caller-defined functions."""
    tool_choice: Optional[
        Union[str, "_models.ChatCompletionsToolSelectionPreset", "_models.ChatCompletionsNamedToolSelection"]
    ] = rest_field()
    """If specified, the model will configure which of the provided tools it can use for the chat
     completions response. Is either a Union[str, \"_models.ChatCompletionsToolSelectionPreset\"]
     type or a ChatCompletionsNamedToolSelection type."""
    parallel_tool_calls: Optional[bool] = rest_field()
    """Whether to enable parallel function calling during tool use."""

    @overload
    def __init__(  # pylint: disable=too-many-locals
        self,
        *,
        messages: List["_models.ChatRequestMessage"],
        functions: Optional[List["_models.FunctionDefinition"]] = None,
        function_call: Optional[Union[str, "_models.FunctionCallPreset", "_models.FunctionName"]] = None,
        max_tokens: Optional[int] = None,
        max_completion_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        logit_bias: Optional[Dict[str, int]] = None,
        user: Optional[str] = None,
        n: Optional[int] = None,
        stop: Optional[List[str]] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        stream: Optional[bool] = None,
        stream_options: Optional["_models.ChatCompletionStreamOptions"] = None,
        model: Optional[str] = None,
        data_sources: Optional[List["_models.AzureChatExtensionConfiguration"]] = None,
        enhancements: Optional["_models.AzureChatEnhancementConfiguration"] = None,
        seed: Optional[int] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        response_format: Optional["_models.ChatCompletionsResponseFormat"] = None,
        tools: Optional[List["_models.ChatCompletionsToolDefinition"]] = None,
        tool_choice: Optional[
            Union[str, "_models.ChatCompletionsToolSelectionPreset", "_models.ChatCompletionsNamedToolSelection"]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatCompletionsTextResponseFormat(ChatCompletionsResponseFormat, discriminator="text"):
    """The standard Chat Completions response format that can freely generate text and is not
    guaranteed to produce response
    content that adheres to a specific schema.

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminated object type, which is always 'text' for this format. Required.
     Default value is "text".
    :vartype type: str
    """

    type: Literal["text"] = rest_discriminator(name="type")  # type: ignore
    """The discriminated object type, which is always 'text' for this format. Required. Default value
     is \"text\"."""

    @overload
    def __init__(
        self,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="text", **kwargs)


class ChatCompletionStreamOptions(_model_base.Model):
    """Options for streaming response. Only set this when you set ``stream: true``.

    :ivar include_usage: If set, an additional chunk will be streamed before the ``data: [DONE]``
     message. The ``usage`` field on this chunk shows the token usage statistics for the entire
     request, and the ``choices`` field will always be an empty array. All other chunks will also
     include a ``usage`` field, but with a null value.
    :vartype include_usage: bool
    """

    include_usage: Optional[bool] = rest_field()
    """If set, an additional chunk will be streamed before the ``data: [DONE]`` message. The ``usage``
     field on this chunk shows the token usage statistics for the entire request, and the
     ``choices`` field will always be an empty array. All other chunks will also include a ``usage``
     field, but with a null value."""

    @overload
    def __init__(
        self,
        *,
        include_usage: Optional[bool] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatMessageContentItem(_model_base.Model):
    """An abstract representation of a structured content item within a chat message.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    ChatMessageImageContentItem, ChatMessageRefusalContentItem, ChatMessageTextContentItem

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminated object type. Required. Default value is None.
    :vartype type: str
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The discriminated object type. Required. Default value is None."""

    @overload
    def __init__(
        self,
        *,
        type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatMessageImageContentItem(ChatMessageContentItem, discriminator="image_url"):
    """A structured chat content item containing an image reference.

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminated object type: always 'image_url' for this type. Required. Default
     value is "image_url".
    :vartype type: str
    :ivar image_url: An internet location, which must be accessible to the model,from which the
     image may be retrieved. Required.
    :vartype image_url: ~azure.openai.models.ChatMessageImageUrl
    """

    type: Literal["image_url"] = rest_discriminator(name="type")  # type: ignore
    """The discriminated object type: always 'image_url' for this type. Required. Default value is
     \"image_url\"."""
    image_url: "_models.ChatMessageImageUrl" = rest_field()
    """An internet location, which must be accessible to the model,from which the image may be
     retrieved. Required."""

    @overload
    def __init__(
        self,
        *,
        image_url: "_models.ChatMessageImageUrl",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="image_url", **kwargs)


class ChatMessageImageUrl(_model_base.Model):
    """An internet location from which the model may retrieve an image.

    All required parameters must be populated in order to send to server.

    :ivar url: The URL of the image. Required.
    :vartype url: str
    :ivar detail: The evaluation quality setting to use, which controls relative prioritization of
     speed, token consumption, and
     accuracy. Known values are: "auto", "low", and "high".
    :vartype detail: str or ~azure.openai.models.ChatMessageImageDetailLevel
    """

    url: str = rest_field()
    """The URL of the image. Required."""
    detail: Optional[Union[str, "_models.ChatMessageImageDetailLevel"]] = rest_field()
    """The evaluation quality setting to use, which controls relative prioritization of speed, token
     consumption, and
     accuracy. Known values are: \"auto\", \"low\", and \"high\"."""

    @overload
    def __init__(
        self,
        *,
        url: str,
        detail: Optional[Union[str, "_models.ChatMessageImageDetailLevel"]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatMessageRefusalContentItem(ChatMessageContentItem, discriminator="refusal"):
    """A structured chat content item containing model refusal information for a structured outputs
    request.

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminated object type: always 'refusal' for this type. Required. Default
     value is "refusal".
    :vartype type: str
    :ivar refusal: The refusal message. Required.
    :vartype refusal: str
    """

    type: Literal["refusal"] = rest_discriminator(name="type")  # type: ignore
    """The discriminated object type: always 'refusal' for this type. Required. Default value is
     \"refusal\"."""
    refusal: str = rest_field()
    """The refusal message. Required."""

    @overload
    def __init__(
        self,
        *,
        refusal: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="refusal", **kwargs)


class ChatMessageTextContentItem(ChatMessageContentItem, discriminator="text"):
    """A structured chat content item containing plain text.

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminated object type: always 'text' for this type. Required. Default value
     is "text".
    :vartype type: str
    :ivar text: The content of the message. Required.
    :vartype text: str
    """

    type: Literal["text"] = rest_discriminator(name="type")  # type: ignore
    """The discriminated object type: always 'text' for this type. Required. Default value is
     \"text\"."""
    text: str = rest_field()
    """The content of the message. Required."""

    @overload
    def __init__(
        self,
        *,
        text: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type="text", **kwargs)


class ChatRequestMessage(_model_base.Model):
    """An abstract representation of a chat message as provided in a request.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    ChatRequestAssistantMessage, ChatRequestFunctionMessage, ChatRequestSystemMessage,
    ChatRequestToolMessage, ChatRequestUserMessage

    All required parameters must be populated in order to send to server.

    :ivar role: The chat role associated with this message. Required. Known values are: "system",
     "assistant", "user", "function", and "tool".
    :vartype role: str or ~azure.openai.models.ChatRole
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    role: str = rest_discriminator(name="role")
    """The chat role associated with this message. Required. Known values are: \"system\",
     \"assistant\", \"user\", \"function\", and \"tool\"."""

    @overload
    def __init__(
        self,
        *,
        role: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatRequestAssistantMessage(ChatRequestMessage, discriminator="assistant"):
    """A request chat message representing response or action from the assistant.

    All required parameters must be populated in order to send to server.

    :ivar role: The chat role associated with this message, which is always 'assistant' for
     assistant messages. Required. The role that provides responses to system-instructed,
     user-prompted input.
    :vartype role: str or ~azure.openai.models.ASSISTANT
    :ivar content: The content of the message. Required. Is one of the following types: str,
     [ChatMessageTextContentItem], [ChatMessageRefusalContentItem]
    :vartype content: str or list[~azure.openai.models.ChatMessageTextContentItem] or
     list[~azure.openai.models.ChatMessageRefusalContentItem]
    :ivar name: An optional name for the participant.
    :vartype name: str
    :ivar tool_calls: The tool calls that must be resolved and have their outputs appended to
     subsequent input messages for the chat
     completions request to resolve as configured.
    :vartype tool_calls: list[~azure.openai.models.ChatCompletionsToolCall]
    :ivar function_call: The function call that must be resolved and have its output appended to
     subsequent input messages for the chat
     completions request to resolve as configured.
    :vartype function_call: ~azure.openai.models.FunctionCall
    :ivar refusal: The refusal message by the assistant.
    :vartype refusal: str
    """

    role: Literal[ChatRole.ASSISTANT] = rest_discriminator(name="role")  # type: ignore
    """The chat role associated with this message, which is always 'assistant' for assistant messages.
     Required. The role that provides responses to system-instructed, user-prompted input."""
    content: Union[str, List["_models.ChatMessageTextContentItem"], List["_models.ChatMessageRefusalContentItem"]] = (
        rest_field()
    )
    """The content of the message. Required. Is one of the following types: str,
     [ChatMessageTextContentItem], [ChatMessageRefusalContentItem]"""
    name: Optional[str] = rest_field()
    """An optional name for the participant."""
    tool_calls: Optional[List["_models.ChatCompletionsToolCall"]] = rest_field()
    """The tool calls that must be resolved and have their outputs appended to subsequent input
     messages for the chat
     completions request to resolve as configured."""
    function_call: Optional["_models.FunctionCall"] = rest_field()
    """The function call that must be resolved and have its output appended to subsequent input
     messages for the chat
     completions request to resolve as configured."""
    refusal: Optional[str] = rest_field()
    """The refusal message by the assistant."""

    @overload
    def __init__(
        self,
        *,
        content: Union[str, List["_models.ChatMessageTextContentItem"], List["_models.ChatMessageRefusalContentItem"]],
        name: Optional[str] = None,
        tool_calls: Optional[List["_models.ChatCompletionsToolCall"]] = None,
        function_call: Optional["_models.FunctionCall"] = None,
        refusal: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, role=ChatRole.ASSISTANT, **kwargs)


class ChatRequestFunctionMessage(ChatRequestMessage, discriminator="function"):
    """A request chat message representing requested output from a configured function.

    All required parameters must be populated in order to send to server.

    :ivar role: The chat role associated with this message, which is always 'function' for function
     messages. Required. The role that provides function results for chat completions.
    :vartype role: str or ~azure.openai.models.FUNCTION
    :ivar name: The name of the function that was called to produce output. Required.
    :vartype name: str
    :ivar content: The output of the function as requested by the function call. Required.
    :vartype content: str
    """

    role: Literal[ChatRole.FUNCTION] = rest_discriminator(name="role")  # type: ignore
    """The chat role associated with this message, which is always 'function' for function messages.
     Required. The role that provides function results for chat completions."""
    name: str = rest_field()
    """The name of the function that was called to produce output. Required."""
    content: str = rest_field()
    """The output of the function as requested by the function call. Required."""

    @overload
    def __init__(
        self,
        *,
        name: str,
        content: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, role=ChatRole.FUNCTION, **kwargs)


class ChatRequestSystemMessage(ChatRequestMessage, discriminator="system"):
    """A request chat message containing system instructions that influence how the model will
    generate a chat completions
    response.

    All required parameters must be populated in order to send to server.

    :ivar role: The chat role associated with this message, which is always 'system' for system
     messages. Required. The role that instructs or sets the behavior of the assistant.
    :vartype role: str or ~azure.openai.models.SYSTEM
    :ivar content: The contents of the system message. Required. Is either a str type or a
     [ChatMessageTextContentItem] type.
    :vartype content: str or list[~azure.openai.models.ChatMessageTextContentItem]
    :ivar name: An optional name for the participant.
    :vartype name: str
    """

    role: Literal[ChatRole.SYSTEM] = rest_discriminator(name="role")  # type: ignore
    """The chat role associated with this message, which is always 'system' for system messages.
     Required. The role that instructs or sets the behavior of the assistant."""
    content: Union[str, List["_models.ChatMessageTextContentItem"]] = rest_field()
    """The contents of the system message. Required. Is either a str type or a
     [ChatMessageTextContentItem] type."""
    name: Optional[str] = rest_field()
    """An optional name for the participant."""

    @overload
    def __init__(
        self,
        *,
        content: Union[str, List["_models.ChatMessageTextContentItem"]],
        name: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, role=ChatRole.SYSTEM, **kwargs)


class ChatRequestToolMessage(ChatRequestMessage, discriminator="tool"):
    """A request chat message representing requested output from a configured tool.

    All required parameters must be populated in order to send to server.

    :ivar role: The chat role associated with this message, which is always 'tool' for tool
     messages. Required. The role that represents extension tool activity within a chat completions
     operation.
    :vartype role: str or ~azure.openai.models.TOOL
    :ivar content: The content of the message. Required. Is either a str type or a
     [ChatMessageTextContentItem] type.
    :vartype content: str or list[~azure.openai.models.ChatMessageTextContentItem]
    :ivar tool_call_id: The ID of the tool call resolved by the provided content. Required.
    :vartype tool_call_id: str
    """

    role: Literal[ChatRole.TOOL] = rest_discriminator(name="role")  # type: ignore
    """The chat role associated with this message, which is always 'tool' for tool messages. Required.
     The role that represents extension tool activity within a chat completions operation."""
    content: Union[str, List["_models.ChatMessageTextContentItem"]] = rest_field()
    """The content of the message. Required. Is either a str type or a [ChatMessageTextContentItem]
     type."""
    tool_call_id: str = rest_field()
    """The ID of the tool call resolved by the provided content. Required."""

    @overload
    def __init__(
        self,
        *,
        content: Union[str, List["_models.ChatMessageTextContentItem"]],
        tool_call_id: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, role=ChatRole.TOOL, **kwargs)


class ChatRequestUserMessage(ChatRequestMessage, discriminator="user"):
    """A request chat message representing user input to the assistant.

    All required parameters must be populated in order to send to server.

    :ivar role: The chat role associated with this message, which is always 'user' for user
     messages. Required. The role that provides input for chat completions.
    :vartype role: str or ~azure.openai.models.USER
    :ivar content: The contents of the user message, with available input types varying by selected
     model. Required. Is one of the following types: str, [ChatMessageTextContentItem],
     [ChatMessageImageContentItem]
    :vartype content: str or list[~azure.openai.models.ChatMessageTextContentItem] or
     list[~azure.openai.models.ChatMessageImageContentItem]
    :ivar name: An optional name for the participant.
    :vartype name: str
    """

    role: Literal[ChatRole.USER] = rest_discriminator(name="role")  # type: ignore
    """The chat role associated with this message, which is always 'user' for user messages. Required.
     The role that provides input for chat completions."""
    content: Union[str, List["_models.ChatMessageTextContentItem"], List["_models.ChatMessageImageContentItem"]] = (
        rest_field()
    )
    """The contents of the user message, with available input types varying by selected model.
     Required. Is one of the following types: str, [ChatMessageTextContentItem],
     [ChatMessageImageContentItem]"""
    name: Optional[str] = rest_field()
    """An optional name for the participant."""

    @overload
    def __init__(
        self,
        *,
        content: Union[str, List["_models.ChatMessageTextContentItem"], List["_models.ChatMessageImageContentItem"]],
        name: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, role=ChatRole.USER, **kwargs)


class ChatResponseMessage(_model_base.Model):
    """A representation of a chat message as received in a response.


    :ivar role: The chat role associated with the message. Required. Known values are: "system",
     "assistant", "user", "function", and "tool".
    :vartype role: str or ~azure.openai.models.ChatRole
    :ivar refusal: The refusal message generated by the model. Required.
    :vartype refusal: str
    :ivar content: The content of the message. Required.
    :vartype content: str
    :ivar tool_calls: The tool calls that must be resolved and have their outputs appended to
     subsequent input messages for the chat
     completions request to resolve as configured.
    :vartype tool_calls: list[~azure.openai.models.ChatCompletionsToolCall]
    :ivar function_call: The function call that must be resolved and have its output appended to
     subsequent input messages for the chat
     completions request to resolve as configured.
    :vartype function_call: ~azure.openai.models.FunctionCall
    :ivar context: If Azure OpenAI chat extensions are configured, this array represents the
     incremental steps performed by those
     extensions while processing the chat completions request.
    :vartype context: ~azure.openai.models.AzureChatExtensionsMessageContext
    """

    role: Union[str, "_models.ChatRole"] = rest_field()
    """The chat role associated with the message. Required. Known values are: \"system\",
     \"assistant\", \"user\", \"function\", and \"tool\"."""
    refusal: str = rest_field()
    """The refusal message generated by the model. Required."""
    content: str = rest_field()
    """The content of the message. Required."""
    tool_calls: Optional[List["_models.ChatCompletionsToolCall"]] = rest_field()
    """The tool calls that must be resolved and have their outputs appended to subsequent input
     messages for the chat
     completions request to resolve as configured."""
    function_call: Optional["_models.FunctionCall"] = rest_field()
    """The function call that must be resolved and have its output appended to subsequent input
     messages for the chat
     completions request to resolve as configured."""
    context: Optional["_models.AzureChatExtensionsMessageContext"] = rest_field()
    """If Azure OpenAI chat extensions are configured, this array represents the incremental steps
     performed by those
     extensions while processing the chat completions request."""

    @overload
    def __init__(
        self,
        *,
        role: Union[str, "_models.ChatRole"],
        refusal: str,
        content: str,
        tool_calls: Optional[List["_models.ChatCompletionsToolCall"]] = None,
        function_call: Optional["_models.FunctionCall"] = None,
        context: Optional["_models.AzureChatExtensionsMessageContext"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatTokenLogProbabilityInfo(_model_base.Model):
    """A representation of the log probability information for a single message content token.


    :ivar token: The message content token. Required.
    :vartype token: str
    :ivar logprob: The log probability of the message content token. Required.
    :vartype logprob: float
    :ivar bytes: A list of integers representing the UTF-8 bytes representation of the token.
     Useful in instances where characters are represented by multiple tokens and their byte
     representations must be combined to generate the correct text representation. Can be null if
     there is no bytes representation for the token. Required.
    :vartype bytes: list[int]
    """

    token: str = rest_field()
    """The message content token. Required."""
    logprob: float = rest_field()
    """The log probability of the message content token. Required."""
    bytes: List[int] = rest_field()
    """A list of integers representing the UTF-8 bytes representation of the token. Useful in
     instances where characters are represented by multiple tokens and their byte representations
     must be combined to generate the correct text representation. Can be null if there is no bytes
     representation for the token. Required."""

    @overload
    def __init__(
        self,
        *,
        token: str,
        logprob: float,
        bytes: List[int],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ChatTokenLogProbabilityResult(_model_base.Model):
    """A representation of the log probability information for a single content token, including a
    list of most likely tokens if 'top_logprobs' were requested.


    :ivar token: The message content token. Required.
    :vartype token: str
    :ivar logprob: The log probability of the message content token. Required.
    :vartype logprob: float
    :ivar bytes: A list of integers representing the UTF-8 bytes representation of the token.
     Useful in instances where characters are represented by multiple tokens and their byte
     representations must be combined to generate the correct text representation. Can be null if
     there is no bytes representation for the token. Required.
    :vartype bytes: list[int]
    :ivar top_logprobs: The list of most likely tokens and their log probability information, as
     requested via 'top_logprobs'. Required.
    :vartype top_logprobs: list[~azure.openai.models.ChatTokenLogProbabilityInfo]
    """

    token: str = rest_field()
    """The message content token. Required."""
    logprob: float = rest_field()
    """The log probability of the message content token. Required."""
    bytes: List[int] = rest_field()
    """A list of integers representing the UTF-8 bytes representation of the token. Useful in
     instances where characters are represented by multiple tokens and their byte representations
     must be combined to generate the correct text representation. Can be null if there is no bytes
     representation for the token. Required."""
    top_logprobs: List["_models.ChatTokenLogProbabilityInfo"] = rest_field()
    """The list of most likely tokens and their log probability information, as requested via
     'top_logprobs'. Required."""

    @overload
    def __init__(
        self,
        *,
        token: str,
        logprob: float,
        bytes: List[int],
        top_logprobs: List["_models.ChatTokenLogProbabilityInfo"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class Choice(_model_base.Model):
    """The representation of a single prompt completion as part of an overall completions request.
    Generally, ``n`` choices are generated per provided prompt with a default value of 1.
    Token limits and other settings may limit the number of choices generated.


    :ivar text: The generated text for a given completions prompt. Required.
    :vartype text: str
    :ivar index: The ordered index associated with this completions choice. Required.
    :vartype index: int
    :ivar content_filter_results: Information about the content filtering category (hate, sexual,
     violence, self_harm), if it
     has been detected, as well as the severity level (very_low, low, medium, high-scale that
     determines the intensity and risk level of harmful content) and if it has been filtered or
     not.
    :vartype content_filter_results: ~azure.openai.models.ContentFilterResultsForChoice
    :ivar logprobs: The log probabilities model for tokens associated with this completions choice.
     Required.
    :vartype logprobs: ~azure.openai.models.CompletionsLogProbabilityModel
    :ivar finish_reason: Reason for finishing. Required. Known values are: "stop", "length",
     "content_filter", "function_call", and "tool_calls".
    :vartype finish_reason: str or ~azure.openai.models.CompletionsFinishReason
    """

    text: str = rest_field()
    """The generated text for a given completions prompt. Required."""
    index: int = rest_field()
    """The ordered index associated with this completions choice. Required."""
    content_filter_results: Optional["_models.ContentFilterResultsForChoice"] = rest_field()
    """Information about the content filtering category (hate, sexual, violence, self_harm), if it
     has been detected, as well as the severity level (very_low, low, medium, high-scale that
     determines the intensity and risk level of harmful content) and if it has been filtered or not."""
    logprobs: "_models.CompletionsLogProbabilityModel" = rest_field()
    """The log probabilities model for tokens associated with this completions choice. Required."""
    finish_reason: Union[str, "_models.CompletionsFinishReason"] = rest_field()
    """Reason for finishing. Required. Known values are: \"stop\", \"length\", \"content_filter\",
     \"function_call\", and \"tool_calls\"."""

    @overload
    def __init__(
        self,
        *,
        text: str,
        index: int,
        logprobs: "_models.CompletionsLogProbabilityModel",
        finish_reason: Union[str, "_models.CompletionsFinishReason"],
        content_filter_results: Optional["_models.ContentFilterResultsForChoice"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class CompleteUploadRequest(_model_base.Model):
    """The request body of an upload completion request.

    All required parameters must be populated in order to send to server.

    :ivar part_ids: The ordered list of Part IDs. Required.
    :vartype part_ids: list[str]
    :ivar md5: The optional md5 checksum for the file contents to verify if the bytes uploaded
     matches what you expect.
    :vartype md5: str
    """

    part_ids: List[str] = rest_field()
    """The ordered list of Part IDs. Required."""
    md5: Optional[str] = rest_field()
    """The optional md5 checksum for the file contents to verify if the bytes uploaded matches what
     you expect."""

    @overload
    def __init__(
        self,
        *,
        part_ids: List[str],
        md5: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class Completions(_model_base.Model):
    """Representation of the response data from a completions request.
    Completions support a wide variety of tasks and generate text that continues from or
    "completes"
    provided prompt data.


    :ivar id: A unique identifier associated with this completions response. Required.
    :vartype id: str
    :ivar created: The first timestamp associated with generation activity for this completions
     response,
     represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
    :vartype created: ~datetime.datetime
    :ivar prompt_filter_results: Content filtering results for zero or more prompts in the request.
     In a streaming request,
     results for different prompts may arrive at different times or in different orders.
    :vartype prompt_filter_results: list[~azure.openai.models.ContentFilterResultsForPrompt]
    :ivar choices: The collection of completions choices associated with this completions response.
     Generally, ``n`` choices are generated per provided prompt with a default value of 1.
     Token limits and other settings may limit the number of choices generated. Required.
    :vartype choices: list[~azure.openai.models.Choice]
    :ivar usage: Usage information for tokens processed and generated as part of this completions
     operation. Required.
    :vartype usage: ~azure.openai.models.CompletionsUsage
    :ivar system_fingerprint: This fingerprint represents the backend configuration that the model
     runs with.

     Can be used in conjunction with the ``seed`` request parameter to understand when backend
     changes have been made that might impact determinism.
    :vartype system_fingerprint: str
    """

    id: str = rest_field()
    """A unique identifier associated with this completions response. Required."""
    created: datetime.datetime = rest_field(format="unix-timestamp")
    """The first timestamp associated with generation activity for this completions response,
     represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required."""
    prompt_filter_results: Optional[List["_models.ContentFilterResultsForPrompt"]] = rest_field()
    """Content filtering results for zero or more prompts in the request. In a streaming request,
     results for different prompts may arrive at different times or in different orders."""
    choices: List["_models.Choice"] = rest_field()
    """The collection of completions choices associated with this completions response.
     Generally, ``n`` choices are generated per provided prompt with a default value of 1.
     Token limits and other settings may limit the number of choices generated. Required."""
    usage: "_models.CompletionsUsage" = rest_field()
    """Usage information for tokens processed and generated as part of this completions operation.
     Required."""
    system_fingerprint: Optional[str] = rest_field()
    """This fingerprint represents the backend configuration that the model runs with.
     
     Can be used in conjunction with the ``seed`` request parameter to understand when backend
     changes have been made that might impact determinism."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        created: datetime.datetime,
        choices: List["_models.Choice"],
        usage: "_models.CompletionsUsage",
        prompt_filter_results: Optional[List["_models.ContentFilterResultsForPrompt"]] = None,
        system_fingerprint: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class CompletionsLogProbabilityModel(_model_base.Model):
    """Representation of a log probabilities model for a completions generation.


    :ivar tokens: The textual forms of tokens evaluated in this probability model. Required.
    :vartype tokens: list[str]
    :ivar token_logprobs: A collection of log probability values for the tokens in this completions
     data. Required.
    :vartype token_logprobs: list[float]
    :ivar top_logprobs: A mapping of tokens to maximum log probability values in this completions
     data. Required.
    :vartype top_logprobs: list[dict[str, float]]
    :ivar text_offset: The text offsets associated with tokens in this completions data. Required.
    :vartype text_offset: list[int]
    """

    tokens: List[str] = rest_field()
    """The textual forms of tokens evaluated in this probability model. Required."""
    token_logprobs: List[float] = rest_field()
    """A collection of log probability values for the tokens in this completions data. Required."""
    top_logprobs: List[Dict[str, float]] = rest_field()
    """A mapping of tokens to maximum log probability values in this completions data. Required."""
    text_offset: List[int] = rest_field()
    """The text offsets associated with tokens in this completions data. Required."""

    @overload
    def __init__(
        self,
        *,
        tokens: List[str],
        token_logprobs: List[float],
        top_logprobs: List[Dict[str, float]],
        text_offset: List[int],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class CompletionsOptions(_model_base.Model):
    """The configuration information for a completions request.
    Completions support a wide variety of tasks and generate text that continues from or
    "completes"
    provided prompt data.

    All required parameters must be populated in order to send to server.

    :ivar prompt: The prompts to generate completions from. Required.
    :vartype prompt: list[str]
    :ivar max_tokens: The maximum number of tokens to generate.
    :vartype max_tokens: int
    :ivar temperature: The sampling temperature to use that controls the apparent creativity of
     generated completions.
     Higher values will make output more random while lower values will make results more focused
     and deterministic.
     It is not recommended to modify temperature and top_p for the same completions request as the
     interaction of these two settings is difficult to predict.
    :vartype temperature: float
    :ivar top_p: An alternative to sampling with temperature called nucleus sampling. This value
     causes the
     model to consider the results of tokens with the provided probability mass. As an example, a
     value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
     considered.
     It is not recommended to modify temperature and top_p for the same completions request as the
     interaction of these two settings is difficult to predict.
    :vartype top_p: float
    :ivar logit_bias: A map between GPT token IDs and bias scores that influences the probability
     of specific tokens
     appearing in a completions response. Token IDs are computed via external tokenizer tools,
     while
     bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding
     to
     a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
     score varies by model.
    :vartype logit_bias: dict[str, int]
    :ivar user: An identifier for the caller or end user of the operation. This may be used for
     tracking
     or rate-limiting purposes.
    :vartype user: str
    :ivar n: The number of completions choices that should be generated per provided prompt as part
     of an
     overall completions response.
     Because this setting can generate many completions, it may quickly consume your token quota.
     Use carefully and ensure reasonable settings for max_tokens and stop.
    :vartype n: int
    :ivar logprobs: A value that controls the emission of log probabilities for the provided number
     of most likely
     tokens within a completions response.
    :vartype logprobs: int
    :ivar suffix: The suffix that comes after a completion of inserted text.
    :vartype suffix: str
    :ivar echo: A value specifying whether completions responses should include input prompts as
     prefixes to
     their generated output.
    :vartype echo: bool
    :ivar stop: A collection of textual sequences that will end completions generation.
    :vartype stop: list[str]
    :ivar presence_penalty: A value that influences the probability of generated tokens appearing
     based on their existing
     presence in generated text.
     Positive values will make tokens less likely to appear when they already exist and increase
     the
     model's likelihood to output new topics.
    :vartype presence_penalty: float
    :ivar frequency_penalty: A value that influences the probability of generated tokens appearing
     based on their cumulative
     frequency in generated text.
     Positive values will make tokens less likely to appear as their frequency increases and
     decrease the likelihood of the model repeating the same statements verbatim.
    :vartype frequency_penalty: float
    :ivar best_of: A value that controls how many completions will be internally generated prior to
     response
     formulation.
     When used together with n, best_of controls the number of candidate completions and must be
     greater than n.
     Because this setting can generate many completions, it may quickly consume your token quota.
     Use carefully and ensure reasonable settings for max_tokens and stop.
    :vartype best_of: int
    :ivar stream: A value indicating whether chat completions should be streamed for this request.
    :vartype stream: bool
    :ivar stream_options: Options for streaming response. Only set this when you set ``stream:
     true``.
    :vartype stream_options: ~azure.openai.models.ChatCompletionStreamOptions
    :ivar model: The model name to provide as part of this completions request.
     Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     resource URI that's connected to.
    :vartype model: str
    :ivar seed: If specified, our system will make a best effort to sample deterministically, such
     that repeated requests with the same ``seed`` and parameters should return the same result.

     Determinism is not guaranteed, and you should refer to the ``system_fingerprint`` response
     parameter to monitor changes in the backend.
    :vartype seed: int
    """

    prompt: List[str] = rest_field()
    """The prompts to generate completions from. Required."""
    max_tokens: Optional[int] = rest_field()
    """The maximum number of tokens to generate."""
    temperature: Optional[float] = rest_field()
    """The sampling temperature to use that controls the apparent creativity of generated completions.
     Higher values will make output more random while lower values will make results more focused
     and deterministic.
     It is not recommended to modify temperature and top_p for the same completions request as the
     interaction of these two settings is difficult to predict."""
    top_p: Optional[float] = rest_field()
    """An alternative to sampling with temperature called nucleus sampling. This value causes the
     model to consider the results of tokens with the provided probability mass. As an example, a
     value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
     considered.
     It is not recommended to modify temperature and top_p for the same completions request as the
     interaction of these two settings is difficult to predict."""
    logit_bias: Optional[Dict[str, int]] = rest_field()
    """A map between GPT token IDs and bias scores that influences the probability of specific tokens
     appearing in a completions response. Token IDs are computed via external tokenizer tools, while
     bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to
     a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
     score varies by model."""
    user: Optional[str] = rest_field()
    """An identifier for the caller or end user of the operation. This may be used for tracking
     or rate-limiting purposes."""
    n: Optional[int] = rest_field()
    """The number of completions choices that should be generated per provided prompt as part of an
     overall completions response.
     Because this setting can generate many completions, it may quickly consume your token quota.
     Use carefully and ensure reasonable settings for max_tokens and stop."""
    logprobs: Optional[int] = rest_field()
    """A value that controls the emission of log probabilities for the provided number of most likely
     tokens within a completions response."""
    suffix: Optional[str] = rest_field()
    """The suffix that comes after a completion of inserted text."""
    echo: Optional[bool] = rest_field()
    """A value specifying whether completions responses should include input prompts as prefixes to
     their generated output."""
    stop: Optional[List[str]] = rest_field()
    """A collection of textual sequences that will end completions generation."""
    presence_penalty: Optional[float] = rest_field()
    """A value that influences the probability of generated tokens appearing based on their existing
     presence in generated text.
     Positive values will make tokens less likely to appear when they already exist and increase the
     model's likelihood to output new topics."""
    frequency_penalty: Optional[float] = rest_field()
    """A value that influences the probability of generated tokens appearing based on their cumulative
     frequency in generated text.
     Positive values will make tokens less likely to appear as their frequency increases and
     decrease the likelihood of the model repeating the same statements verbatim."""
    best_of: Optional[int] = rest_field()
    """A value that controls how many completions will be internally generated prior to response
     formulation.
     When used together with n, best_of controls the number of candidate completions and must be
     greater than n.
     Because this setting can generate many completions, it may quickly consume your token quota.
     Use carefully and ensure reasonable settings for max_tokens and stop."""
    stream: Optional[bool] = rest_field()
    """A value indicating whether chat completions should be streamed for this request."""
    stream_options: Optional["_models.ChatCompletionStreamOptions"] = rest_field()
    """Options for streaming response. Only set this when you set ``stream: true``."""
    model: Optional[str] = rest_field()
    """The model name to provide as part of this completions request.
     Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     resource URI that's connected to."""
    seed: Optional[int] = rest_field()
    """If specified, our system will make a best effort to sample deterministically, such that
     repeated requests with the same ``seed`` and parameters should return the same result.
     
     Determinism is not guaranteed, and you should refer to the ``system_fingerprint`` response
     parameter to monitor changes in the backend."""

    @overload
    def __init__(
        self,
        *,
        prompt: List[str],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        logit_bias: Optional[Dict[str, int]] = None,
        user: Optional[str] = None,
        n: Optional[int] = None,
        logprobs: Optional[int] = None,
        suffix: Optional[str] = None,
        echo: Optional[bool] = None,
        stop: Optional[List[str]] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        best_of: Optional[int] = None,
        stream: Optional[bool] = None,
        stream_options: Optional["_models.ChatCompletionStreamOptions"] = None,
        model: Optional[str] = None,
        seed: Optional[int] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class CompletionsUsage(_model_base.Model):
    """Representation of the token counts processed for a completions request.
    Counts consider all tokens across prompts, choices, choice alternates, best_of generations, and
    other consumers.


    :ivar completion_tokens: The number of tokens generated across all completions emissions.
     Required.
    :vartype completion_tokens: int
    :ivar prompt_tokens: The number of tokens in the provided prompts for the completions request.
     Required.
    :vartype prompt_tokens: int
    :ivar total_tokens: The total number of tokens processed for the completions request and
     response. Required.
    :vartype total_tokens: int
    :ivar prompt_tokens_details: Details of the prompt tokens.
    :vartype prompt_tokens_details: ~azure.openai.models.CompletionsUsagePromptTokensDetails
    :ivar completion_tokens_details: Breakdown of tokens used in a completion.
    :vartype completion_tokens_details:
     ~azure.openai.models.CompletionsUsageCompletionTokensDetails
    """

    completion_tokens: int = rest_field()
    """The number of tokens generated across all completions emissions. Required."""
    prompt_tokens: int = rest_field()
    """The number of tokens in the provided prompts for the completions request. Required."""
    total_tokens: int = rest_field()
    """The total number of tokens processed for the completions request and response. Required."""
    prompt_tokens_details: Optional["_models.CompletionsUsagePromptTokensDetails"] = rest_field()
    """Details of the prompt tokens."""
    completion_tokens_details: Optional["_models.CompletionsUsageCompletionTokensDetails"] = rest_field()
    """Breakdown of tokens used in a completion."""

    @overload
    def __init__(
        self,
        *,
        completion_tokens: int,
        prompt_tokens: int,
        total_tokens: int,
        prompt_tokens_details: Optional["_models.CompletionsUsagePromptTokensDetails"] = None,
        completion_tokens_details: Optional["_models.CompletionsUsageCompletionTokensDetails"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class CompletionsUsageCompletionTokensDetails(_model_base.Model):
    """CompletionsUsageCompletionTokensDetails.

    :ivar reasoning_tokens: Tokens generated by the model for reasoning.
    :vartype reasoning_tokens: int
    """

    reasoning_tokens: Optional[int] = rest_field()
    """Tokens generated by the model for reasoning."""

    @overload
    def __init__(
        self,
        *,
        reasoning_tokens: Optional[int] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class CompletionsUsagePromptTokensDetails(_model_base.Model):
    """CompletionsUsagePromptTokensDetails.

    :ivar cached_tokens: The number of cached prompt tokens.
    :vartype cached_tokens: int
    """

    cached_tokens: Optional[int] = rest_field()
    """The number of cached prompt tokens."""

    @overload
    def __init__(
        self,
        *,
        cached_tokens: Optional[int] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterBlocklistIdResult(_model_base.Model):
    """Represents the outcome of an evaluation against a custom blocklist as performed by content
    filtering.


    :ivar filtered: A value indicating whether or not the content has been filtered. Required.
    :vartype filtered: bool
    :ivar id: The ID of the custom blocklist evaluated. Required.
    :vartype id: str
    """

    filtered: bool = rest_field()
    """A value indicating whether or not the content has been filtered. Required."""
    id: str = rest_field()
    """The ID of the custom blocklist evaluated. Required."""

    @overload
    def __init__(
        self,
        *,
        filtered: bool,
        id: str,  # pylint: disable=redefined-builtin
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterCitedDetectionResult(_model_base.Model):
    """Represents the outcome of a detection operation against protected resources as performed by
    content filtering.


    :ivar filtered: A value indicating whether or not the content has been filtered. Required.
    :vartype filtered: bool
    :ivar detected: A value indicating whether detection occurred, irrespective of severity or
     whether the content was filtered. Required.
    :vartype detected: bool
    :ivar url: The internet location associated with the detection.
    :vartype url: str
    :ivar license: The license description associated with the detection.
    :vartype license: str
    """

    filtered: bool = rest_field()
    """A value indicating whether or not the content has been filtered. Required."""
    detected: bool = rest_field()
    """A value indicating whether detection occurred, irrespective of severity or whether the content
     was filtered. Required."""
    url: Optional[str] = rest_field(name="URL")
    """The internet location associated with the detection."""
    license: Optional[str] = rest_field()
    """The license description associated with the detection."""

    @overload
    def __init__(
        self,
        *,
        filtered: bool,
        detected: bool,
        url: Optional[str] = None,
        license: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterCompletionTextSpan(_model_base.Model):
    """Describes a span within generated completion text. Offset 0 is the first UTF32 code point of
    the completion text.


    :ivar completion_start_offset: Offset of the UTF32 code point which begins the span. Required.
    :vartype completion_start_offset: int
    :ivar completion_end_offset: Offset of the first UTF32 code point which is excluded from the
     span.
     This field is always equal to completion_start_offset for empty spans.
     This field is always larger than completion_start_offset for non-empty spans. Required.
    :vartype completion_end_offset: int
    """

    completion_start_offset: int = rest_field()
    """Offset of the UTF32 code point which begins the span. Required."""
    completion_end_offset: int = rest_field()
    """Offset of the first UTF32 code point which is excluded from the span.
     This field is always equal to completion_start_offset for empty spans.
     This field is always larger than completion_start_offset for non-empty spans. Required."""

    @overload
    def __init__(
        self,
        *,
        completion_start_offset: int,
        completion_end_offset: int,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterCompletionTextSpanResult(_model_base.Model):
    """Describes a span within generated completion text.


    :ivar filtered: A value indicating whether or not the content has been filtered. Required.
    :vartype filtered: bool
    :ivar detected: A value indicating whether detection occurred, irrespective of severity or
     whether the content was filtered. Required.
    :vartype detected: bool
    :ivar details: The collection of completion text spans. Required.
    :vartype details: list[~azure.openai.models.ContentFilterCompletionTextSpan]
    """

    filtered: bool = rest_field()
    """A value indicating whether or not the content has been filtered. Required."""
    detected: bool = rest_field()
    """A value indicating whether detection occurred, irrespective of severity or whether the content
     was filtered. Required."""
    details: List["_models.ContentFilterCompletionTextSpan"] = rest_field()
    """The collection of completion text spans. Required."""

    @overload
    def __init__(
        self,
        *,
        filtered: bool,
        detected: bool,
        details: List["_models.ContentFilterCompletionTextSpan"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterDetailedResults(_model_base.Model):
    """Represents a structured collection of result details for content filtering.


    :ivar filtered: A value indicating whether or not the content has been filtered. Required.
    :vartype filtered: bool
    :ivar details: The collection of detailed blocklist result information. Required.
    :vartype details: list[~azure.openai.models.ContentFilterBlocklistIdResult]
    """

    filtered: bool = rest_field()
    """A value indicating whether or not the content has been filtered. Required."""
    details: List["_models.ContentFilterBlocklistIdResult"] = rest_field()
    """The collection of detailed blocklist result information. Required."""

    @overload
    def __init__(
        self,
        *,
        filtered: bool,
        details: List["_models.ContentFilterBlocklistIdResult"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterDetectionResult(_model_base.Model):
    """Represents the outcome of a detection operation performed by content filtering.


    :ivar filtered: A value indicating whether or not the content has been filtered. Required.
    :vartype filtered: bool
    :ivar detected: A value indicating whether detection occurred, irrespective of severity or
     whether the content was filtered. Required.
    :vartype detected: bool
    """

    filtered: bool = rest_field()
    """A value indicating whether or not the content has been filtered. Required."""
    detected: bool = rest_field()
    """A value indicating whether detection occurred, irrespective of severity or whether the content
     was filtered. Required."""

    @overload
    def __init__(
        self,
        *,
        filtered: bool,
        detected: bool,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterResult(_model_base.Model):
    """Information about filtered content severity level and if it has been filtered or not.


    :ivar filtered: A value indicating whether or not the content has been filtered. Required.
    :vartype filtered: bool
    :ivar severity: Ratings for the intensity and risk level of filtered content. Required. Known
     values are: "safe", "low", "medium", and "high".
    :vartype severity: str or ~azure.openai.models.ContentFilterSeverity
    """

    filtered: bool = rest_field()
    """A value indicating whether or not the content has been filtered. Required."""
    severity: Union[str, "_models.ContentFilterSeverity"] = rest_field()
    """Ratings for the intensity and risk level of filtered content. Required. Known values are:
     \"safe\", \"low\", \"medium\", and \"high\"."""

    @overload
    def __init__(
        self,
        *,
        filtered: bool,
        severity: Union[str, "_models.ContentFilterSeverity"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterResultDetailsForPrompt(_model_base.Model):
    """Information about content filtering evaluated against input data to Azure OpenAI.

    :ivar sexual: Describes language related to anatomical organs and genitals, romantic
     relationships,
      acts portrayed in erotic or affectionate terms, physical sexual acts, including
      those portrayed as an assault or a forced sexual violent act against ones will,
      prostitution, pornography, and abuse.
    :vartype sexual: ~azure.openai.models.ContentFilterResult
    :ivar violence: Describes language related to physical actions intended to hurt, injure,
     damage, or
     kill someone or something; describes weapons, etc.
    :vartype violence: ~azure.openai.models.ContentFilterResult
    :ivar hate: Describes language attacks or uses that include pejorative or discriminatory
     language
     with reference to a person or identity group on the basis of certain differentiating
     attributes of these groups including but not limited to race, ethnicity, nationality,
     gender identity and expression, sexual orientation, religion, immigration status, ability
     status, personal appearance, and body size.
    :vartype hate: ~azure.openai.models.ContentFilterResult
    :ivar self_harm: Describes language related to physical actions intended to purposely hurt,
     injure,
     or damage ones body, or kill oneself.
    :vartype self_harm: ~azure.openai.models.ContentFilterResult
    :ivar profanity: Describes whether profanity was detected.
    :vartype profanity: ~azure.openai.models.ContentFilterDetectionResult
    :ivar custom_blocklists: Describes detection results against configured custom blocklists.
    :vartype custom_blocklists: ~azure.openai.models.ContentFilterDetailedResults
    :ivar error: Describes an error returned if the content filtering system is
     down or otherwise unable to complete the operation in time.
    :vartype error: ~azure.core.ODataV4Format
    :ivar jailbreak: Whether a jailbreak attempt was detected in the prompt.
    :vartype jailbreak: ~azure.openai.models.ContentFilterDetectionResult
    :ivar indirect_attack: Whether an indirect attack was detected in the prompt.
    :vartype indirect_attack: ~azure.openai.models.ContentFilterDetectionResult
    """

    sexual: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to anatomical organs and genitals, romantic relationships,
      acts portrayed in erotic or affectionate terms, physical sexual acts, including
      those portrayed as an assault or a forced sexual violent act against ones will,
      prostitution, pornography, and abuse."""
    violence: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to physical actions intended to hurt, injure, damage, or
     kill someone or something; describes weapons, etc."""
    hate: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language attacks or uses that include pejorative or discriminatory language
     with reference to a person or identity group on the basis of certain differentiating
     attributes of these groups including but not limited to race, ethnicity, nationality,
     gender identity and expression, sexual orientation, religion, immigration status, ability
     status, personal appearance, and body size."""
    self_harm: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to physical actions intended to purposely hurt, injure,
     or damage ones body, or kill oneself."""
    profanity: Optional["_models.ContentFilterDetectionResult"] = rest_field()
    """Describes whether profanity was detected."""
    custom_blocklists: Optional["_models.ContentFilterDetailedResults"] = rest_field()
    """Describes detection results against configured custom blocklists."""
    error: Optional[ODataV4Format] = rest_field()
    """Describes an error returned if the content filtering system is
     down or otherwise unable to complete the operation in time."""
    jailbreak: Optional["_models.ContentFilterDetectionResult"] = rest_field()
    """Whether a jailbreak attempt was detected in the prompt."""
    indirect_attack: Optional["_models.ContentFilterDetectionResult"] = rest_field()
    """Whether an indirect attack was detected in the prompt."""

    @overload
    def __init__(
        self,
        *,
        sexual: Optional["_models.ContentFilterResult"] = None,
        violence: Optional["_models.ContentFilterResult"] = None,
        hate: Optional["_models.ContentFilterResult"] = None,
        self_harm: Optional["_models.ContentFilterResult"] = None,
        profanity: Optional["_models.ContentFilterDetectionResult"] = None,
        custom_blocklists: Optional["_models.ContentFilterDetailedResults"] = None,
        error: Optional[ODataV4Format] = None,
        jailbreak: Optional["_models.ContentFilterDetectionResult"] = None,
        indirect_attack: Optional["_models.ContentFilterDetectionResult"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterResultsForChoice(_model_base.Model):
    """Information about content filtering evaluated against generated model output.

    :ivar sexual: Describes language related to anatomical organs and genitals, romantic
     relationships,
      acts portrayed in erotic or affectionate terms, physical sexual acts, including
      those portrayed as an assault or a forced sexual violent act against ones will,
      prostitution, pornography, and abuse.
    :vartype sexual: ~azure.openai.models.ContentFilterResult
    :ivar violence: Describes language related to physical actions intended to hurt, injure,
     damage, or
     kill someone or something; describes weapons, etc.
    :vartype violence: ~azure.openai.models.ContentFilterResult
    :ivar hate: Describes language attacks or uses that include pejorative or discriminatory
     language
     with reference to a person or identity group on the basis of certain differentiating
     attributes of these groups including but not limited to race, ethnicity, nationality,
     gender identity and expression, sexual orientation, religion, immigration status, ability
     status, personal appearance, and body size.
    :vartype hate: ~azure.openai.models.ContentFilterResult
    :ivar self_harm: Describes language related to physical actions intended to purposely hurt,
     injure,
     or damage ones body, or kill oneself.
    :vartype self_harm: ~azure.openai.models.ContentFilterResult
    :ivar profanity: Describes whether profanity was detected.
    :vartype profanity: ~azure.openai.models.ContentFilterDetectionResult
    :ivar custom_blocklists: Describes detection results against configured custom blocklists.
    :vartype custom_blocklists: ~azure.openai.models.ContentFilterDetailedResults
    :ivar error: Describes an error returned if the content filtering system is
     down or otherwise unable to complete the operation in time.
    :vartype error: ~azure.core.ODataV4Format
    :ivar protected_material_text: Information about detection of protected text material.
    :vartype protected_material_text: ~azure.openai.models.ContentFilterDetectionResult
    :ivar protected_material_code: Information about detection of protected code material.
    :vartype protected_material_code: ~azure.openai.models.ContentFilterCitedDetectionResult
    :ivar ungrounded_material: Information about detection of ungrounded material.
    :vartype ungrounded_material: ~azure.openai.models.ContentFilterCompletionTextSpanResult
    """

    sexual: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to anatomical organs and genitals, romantic relationships,
      acts portrayed in erotic or affectionate terms, physical sexual acts, including
      those portrayed as an assault or a forced sexual violent act against ones will,
      prostitution, pornography, and abuse."""
    violence: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to physical actions intended to hurt, injure, damage, or
     kill someone or something; describes weapons, etc."""
    hate: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language attacks or uses that include pejorative or discriminatory language
     with reference to a person or identity group on the basis of certain differentiating
     attributes of these groups including but not limited to race, ethnicity, nationality,
     gender identity and expression, sexual orientation, religion, immigration status, ability
     status, personal appearance, and body size."""
    self_harm: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to physical actions intended to purposely hurt, injure,
     or damage ones body, or kill oneself."""
    profanity: Optional["_models.ContentFilterDetectionResult"] = rest_field()
    """Describes whether profanity was detected."""
    custom_blocklists: Optional["_models.ContentFilterDetailedResults"] = rest_field()
    """Describes detection results against configured custom blocklists."""
    error: Optional[ODataV4Format] = rest_field()
    """Describes an error returned if the content filtering system is
     down or otherwise unable to complete the operation in time."""
    protected_material_text: Optional["_models.ContentFilterDetectionResult"] = rest_field()
    """Information about detection of protected text material."""
    protected_material_code: Optional["_models.ContentFilterCitedDetectionResult"] = rest_field()
    """Information about detection of protected code material."""
    ungrounded_material: Optional["_models.ContentFilterCompletionTextSpanResult"] = rest_field()
    """Information about detection of ungrounded material."""

    @overload
    def __init__(
        self,
        *,
        sexual: Optional["_models.ContentFilterResult"] = None,
        violence: Optional["_models.ContentFilterResult"] = None,
        hate: Optional["_models.ContentFilterResult"] = None,
        self_harm: Optional["_models.ContentFilterResult"] = None,
        profanity: Optional["_models.ContentFilterDetectionResult"] = None,
        custom_blocklists: Optional["_models.ContentFilterDetailedResults"] = None,
        error: Optional[ODataV4Format] = None,
        protected_material_text: Optional["_models.ContentFilterDetectionResult"] = None,
        protected_material_code: Optional["_models.ContentFilterCitedDetectionResult"] = None,
        ungrounded_material: Optional["_models.ContentFilterCompletionTextSpanResult"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ContentFilterResultsForPrompt(_model_base.Model):
    """Content filtering results for a single prompt in the request.


    :ivar prompt_index: The index of this prompt in the set of prompt results. Required.
    :vartype prompt_index: int
    :ivar content_filter_results: Content filtering results for this prompt. Required.
    :vartype content_filter_results: ~azure.openai.models.ContentFilterResultDetailsForPrompt
    """

    prompt_index: int = rest_field()
    """The index of this prompt in the set of prompt results. Required."""
    content_filter_results: "_models.ContentFilterResultDetailsForPrompt" = rest_field()
    """Content filtering results for this prompt. Required."""

    @overload
    def __init__(
        self,
        *,
        prompt_index: int,
        content_filter_results: "_models.ContentFilterResultDetailsForPrompt",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class CreateUploadRequest(_model_base.Model):
    """The request body of an upload creation operation.

    All required parameters must be populated in order to send to server.

    :ivar filename: The name of the file to upload. Required.
    :vartype filename: str
    :ivar purpose: The intended purpose of the uploaded file.

     Use 'assistants' for Assistants and Message files, 'vision' for Assistants image file inputs,
     'batch' for Batch API, and 'fine-tune' for Fine-tuning. Required. Is one of the following
     types: Literal["assistants"], Literal["batch"], Literal["fine-tune"], Literal["vision"], str
    :vartype purpose: str or str or str or str or str
    :ivar bytes: The number of bytes in the file you are uploading. Required.
    :vartype bytes: int
    :ivar mime_type: The MIME type of the file.

     This must fall within the supported MIME types for your file purpose. See the supported MIME
     types for assistants and vision. Required.
    :vartype mime_type: str
    """

    filename: str = rest_field()
    """The name of the file to upload. Required."""
    purpose: Union[Literal["assistants"], Literal["batch"], Literal["fine-tune"], Literal["vision"], str] = rest_field()
    """The intended purpose of the uploaded file.
     
     Use 'assistants' for Assistants and Message files, 'vision' for Assistants image file inputs,
     'batch' for Batch API, and 'fine-tune' for Fine-tuning. Required. Is one of the following
     types: Literal[\"assistants\"], Literal[\"batch\"], Literal[\"fine-tune\"],
     Literal[\"vision\"], str"""
    bytes: int = rest_field()
    """The number of bytes in the file you are uploading. Required."""
    mime_type: str = rest_field()
    """The MIME type of the file.
     
     This must fall within the supported MIME types for your file purpose. See the supported MIME
     types for assistants and vision. Required."""

    @overload
    def __init__(
        self,
        *,
        filename: str,
        purpose: Union[Literal["assistants"], Literal["batch"], Literal["fine-tune"], Literal["vision"], str],
        bytes: int,
        mime_type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ElasticsearchChatExtensionConfiguration(AzureChatExtensionConfiguration, discriminator="elasticsearch"):
    """A specific representation of configurable options for Elasticsearch when using it as an Azure
    OpenAI chat
    extension.

    All required parameters must be populated in order to send to server.

    :ivar type: The type label to use when configuring Azure OpenAI chat extensions. This should
     typically not be changed from its
     default value for Elasticsearch. Required. Represents the use of Elasticsearch index as an
     Azure OpenAI chat extension.
    :vartype type: str or ~azure.openai.models.ELASTICSEARCH
    :ivar parameters: The parameters to use when configuring Elasticsearch. Required.
    :vartype parameters: ~azure.openai.models.ElasticsearchChatExtensionParameters
    """

    type: Literal[AzureChatExtensionType.ELASTICSEARCH] = rest_discriminator(name="type")  # type: ignore
    """The type label to use when configuring Azure OpenAI chat extensions. This should typically not
     be changed from its
     default value for Elasticsearch. Required. Represents the use of Elasticsearch index as an
     Azure OpenAI chat extension."""
    parameters: "_models.ElasticsearchChatExtensionParameters" = rest_field()
    """The parameters to use when configuring Elasticsearch. Required."""

    @overload
    def __init__(
        self,
        *,
        parameters: "_models.ElasticsearchChatExtensionParameters",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=AzureChatExtensionType.ELASTICSEARCH, **kwargs)


class ElasticsearchChatExtensionParameters(_model_base.Model):
    """Parameters to use when configuring Elasticsearch as an Azure OpenAI chat extension. The
    supported authentication types are KeyAndKeyId and EncodedAPIKey.

    All required parameters must be populated in order to send to server.

    :ivar top_n_documents: The configured top number of documents to feature for the configured
     query.
    :vartype top_n_documents: int
    :ivar in_scope: Whether queries should be restricted to use of indexed data.
    :vartype in_scope: bool
    :ivar strictness: The configured strictness of the search relevance filtering. The higher of
     strictness, the higher of the precision but lower recall of the answer.
    :vartype strictness: int
    :ivar max_search_queries: The max number of rewritten queries should be send to search provider
     for one user message. If not specified,
     the system will decide the number of queries to send.
    :vartype max_search_queries: int
    :ivar allow_partial_result: If specified as true, the system will allow partial search results
     to be used and the request fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails.
    :vartype allow_partial_result: bool
    :ivar include_contexts: The included properties of the output context. If not specified, the
     default value is ``citations`` and ``intent``.
    :vartype include_contexts: list[str or ~azure.openai.models.OnYourDataContextProperty]
    :ivar authentication: The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication.
    :vartype authentication: ~azure.openai.models.OnYourDataAuthenticationOptions
    :ivar endpoint: The endpoint of Elasticsearch. Required.
    :vartype endpoint: str
    :ivar index_name: The index name of Elasticsearch. Required.
    :vartype index_name: str
    :ivar fields_mapping: The index field mapping options of Elasticsearch.
    :vartype fields_mapping: ~azure.openai.models.ElasticsearchIndexFieldMappingOptions
    :ivar query_type: The query type of Elasticsearch. Known values are: "simple" and "vector".
    :vartype query_type: str or ~azure.openai.models.ElasticsearchQueryType
    :ivar embedding_dependency: The embedding dependency for vector search.
    :vartype embedding_dependency: ~azure.openai.models.OnYourDataVectorizationSource
    """

    top_n_documents: Optional[int] = rest_field()
    """The configured top number of documents to feature for the configured query."""
    in_scope: Optional[bool] = rest_field()
    """Whether queries should be restricted to use of indexed data."""
    strictness: Optional[int] = rest_field()
    """The configured strictness of the search relevance filtering. The higher of strictness, the
     higher of the precision but lower recall of the answer."""
    max_search_queries: Optional[int] = rest_field()
    """The max number of rewritten queries should be send to search provider for one user message. If
     not specified,
     the system will decide the number of queries to send."""
    allow_partial_result: Optional[bool] = rest_field()
    """If specified as true, the system will allow partial search results to be used and the request
     fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails."""
    include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = rest_field()
    """The included properties of the output context. If not specified, the default value is
     ``citations`` and ``intent``."""
    authentication: Optional["_models.OnYourDataAuthenticationOptions"] = rest_field()
    """The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication."""
    endpoint: str = rest_field()
    """The endpoint of Elasticsearch. Required."""
    index_name: str = rest_field()
    """The index name of Elasticsearch. Required."""
    fields_mapping: Optional["_models.ElasticsearchIndexFieldMappingOptions"] = rest_field()
    """The index field mapping options of Elasticsearch."""
    query_type: Optional[Union[str, "_models.ElasticsearchQueryType"]] = rest_field()
    """The query type of Elasticsearch. Known values are: \"simple\" and \"vector\"."""
    embedding_dependency: Optional["_models.OnYourDataVectorizationSource"] = rest_field()
    """The embedding dependency for vector search."""

    @overload
    def __init__(
        self,
        *,
        endpoint: str,
        index_name: str,
        top_n_documents: Optional[int] = None,
        in_scope: Optional[bool] = None,
        strictness: Optional[int] = None,
        max_search_queries: Optional[int] = None,
        allow_partial_result: Optional[bool] = None,
        include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = None,
        authentication: Optional["_models.OnYourDataAuthenticationOptions"] = None,
        fields_mapping: Optional["_models.ElasticsearchIndexFieldMappingOptions"] = None,
        query_type: Optional[Union[str, "_models.ElasticsearchQueryType"]] = None,
        embedding_dependency: Optional["_models.OnYourDataVectorizationSource"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ElasticsearchIndexFieldMappingOptions(_model_base.Model):
    """Optional settings to control how fields are processed when using a configured Elasticsearch
    resource.

    :ivar title_field: The name of the index field to use as a title.
    :vartype title_field: str
    :ivar url_field: The name of the index field to use as a URL.
    :vartype url_field: str
    :ivar filepath_field: The name of the index field to use as a filepath.
    :vartype filepath_field: str
    :ivar content_fields: The names of index fields that should be treated as content.
    :vartype content_fields: list[str]
    :ivar content_fields_separator: The separator pattern that content fields should use.
    :vartype content_fields_separator: str
    :ivar vector_fields: The names of fields that represent vector data.
    :vartype vector_fields: list[str]
    """

    title_field: Optional[str] = rest_field()
    """The name of the index field to use as a title."""
    url_field: Optional[str] = rest_field()
    """The name of the index field to use as a URL."""
    filepath_field: Optional[str] = rest_field()
    """The name of the index field to use as a filepath."""
    content_fields: Optional[List[str]] = rest_field()
    """The names of index fields that should be treated as content."""
    content_fields_separator: Optional[str] = rest_field()
    """The separator pattern that content fields should use."""
    vector_fields: Optional[List[str]] = rest_field()
    """The names of fields that represent vector data."""

    @overload
    def __init__(
        self,
        *,
        title_field: Optional[str] = None,
        url_field: Optional[str] = None,
        filepath_field: Optional[str] = None,
        content_fields: Optional[List[str]] = None,
        content_fields_separator: Optional[str] = None,
        vector_fields: Optional[List[str]] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class EmbeddingItem(_model_base.Model):
    """Representation of a single embeddings relatedness comparison.

    Readonly variables are only populated by the server, and will be ignored when sending a request.


    :ivar embedding: List of embeddings value for the input prompt. These represent a measurement
     of the
     vector-based relatedness of the provided input. Required.
    :vartype embedding: list[float]
    :ivar index: Index of the prompt to which the EmbeddingItem corresponds. Required.
    :vartype index: int
    :ivar object: The object type which is always 'embedding'. Required. Default value is
     "embedding".
    :vartype object: str
    """

    embedding: List[float] = rest_field()
    """List of embeddings value for the input prompt. These represent a measurement of the
     vector-based relatedness of the provided input. Required."""
    index: int = rest_field()
    """Index of the prompt to which the EmbeddingItem corresponds. Required."""
    object: Literal["embedding"] = rest_field()
    """The object type which is always 'embedding'. Required. Default value is \"embedding\"."""

    @overload
    def __init__(
        self,
        *,
        embedding: List[float],
        index: int,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.object: Literal["embedding"] = "embedding"


class Embeddings(_model_base.Model):
    """Representation of the response data from an embeddings request.
    Embeddings measure the relatedness of text strings and are commonly used for search,
    clustering,
    recommendations, and other similar scenarios.


    :ivar data: Embedding values for the prompts submitted in the request. Required.
    :vartype data: list[~azure.openai.models.EmbeddingItem]
    :ivar usage: Usage counts for tokens input using the embeddings API. Required.
    :vartype usage: ~azure.openai.models.EmbeddingsUsage
    """

    data: List["_models.EmbeddingItem"] = rest_field()
    """Embedding values for the prompts submitted in the request. Required."""
    usage: "_models.EmbeddingsUsage" = rest_field()
    """Usage counts for tokens input using the embeddings API. Required."""

    @overload
    def __init__(
        self,
        *,
        data: List["_models.EmbeddingItem"],
        usage: "_models.EmbeddingsUsage",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class EmbeddingsOptions(_model_base.Model):
    """The configuration information for an embeddings request.
    Embeddings measure the relatedness of text strings and are commonly used for search,
    clustering,
    recommendations, and other similar scenarios.

    All required parameters must be populated in order to send to server.

    :ivar user: An identifier for the caller or end user of the operation. This may be used for
     tracking
     or rate-limiting purposes.
    :vartype user: str
    :ivar model: The model name to provide as part of this embeddings request.
     Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     resource URI that's connected to.
    :vartype model: str
    :ivar input: Input texts to get embeddings for, encoded as a an array of strings.
     Each input must not exceed 2048 tokens in length.

     Unless you are embedding code, we suggest replacing newlines (\\n) in your input with a single
     space,
     as we have observed inferior results when newlines are present. Required.
    :vartype input: list[str]
    :ivar encoding_format: The response encoding format to use for embedding data. Known values
     are: "float" and "base64".
    :vartype encoding_format: str or ~azure.openai.models.EmbeddingEncodingFormat
    :ivar dimensions: The number of dimensions the resulting output embeddings should have. Only
     supported in ``text-embedding-3`` and later models.
    :vartype dimensions: int
    :ivar input_type: When using Azure OpenAI, specifies the input type to use for embedding
     search.
    :vartype input_type: str
    """

    user: Optional[str] = rest_field()
    """An identifier for the caller or end user of the operation. This may be used for tracking
     or rate-limiting purposes."""
    model: Optional[str] = rest_field()
    """The model name to provide as part of this embeddings request.
     Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     resource URI that's connected to."""
    input: List[str] = rest_field()
    """Input texts to get embeddings for, encoded as a an array of strings.
     Each input must not exceed 2048 tokens in length.
     
     Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single
     space,
     as we have observed inferior results when newlines are present. Required."""
    encoding_format: Optional[Union[str, "_models.EmbeddingEncodingFormat"]] = rest_field()
    """The response encoding format to use for embedding data. Known values are: \"float\" and
     \"base64\"."""
    dimensions: Optional[int] = rest_field()
    """The number of dimensions the resulting output embeddings should have. Only supported in
     ``text-embedding-3`` and later models."""
    input_type: Optional[str] = rest_field()
    """When using Azure OpenAI, specifies the input type to use for embedding search."""

    @overload
    def __init__(
        self,
        *,
        input: List[str],
        user: Optional[str] = None,
        model: Optional[str] = None,
        encoding_format: Optional[Union[str, "_models.EmbeddingEncodingFormat"]] = None,
        dimensions: Optional[int] = None,
        input_type: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class EmbeddingsUsage(_model_base.Model):
    """Measurement of the amount of tokens used in this request and response.


    :ivar prompt_tokens: Number of tokens sent in the original request. Required.
    :vartype prompt_tokens: int
    :ivar total_tokens: Total number of tokens transacted in this request/response. Required.
    :vartype total_tokens: int
    """

    prompt_tokens: int = rest_field()
    """Number of tokens sent in the original request. Required."""
    total_tokens: int = rest_field()
    """Total number of tokens transacted in this request/response. Required."""

    @overload
    def __init__(
        self,
        *,
        prompt_tokens: int,
        total_tokens: int,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class FileDeletionStatus(_model_base.Model):
    """A status response from a file deletion operation.

    Readonly variables are only populated by the server, and will be ignored when sending a request.


    :ivar id: The ID of the resource specified for deletion. Required.
    :vartype id: str
    :ivar deleted: A value indicating whether deletion was successful. Required.
    :vartype deleted: bool
    :ivar object: The object type, which is always 'file'. Required. Default value is "file".
    :vartype object: str
    """

    id: str = rest_field()
    """The ID of the resource specified for deletion. Required."""
    deleted: bool = rest_field()
    """A value indicating whether deletion was successful. Required."""
    object: Literal["file"] = rest_field()
    """The object type, which is always 'file'. Required. Default value is \"file\"."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        deleted: bool,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.object: Literal["file"] = "file"


class FileListResponse(_model_base.Model):
    """The response data from a file list operation.

    Readonly variables are only populated by the server, and will be ignored when sending a request.


    :ivar object: The object type, which is always 'list'. Required. Default value is "list".
    :vartype object: str
    :ivar data: The files returned for the request. Required.
    :vartype data: list[~azure.openai.models.OpenAIFile]
    """

    object: Literal["list"] = rest_field()
    """The object type, which is always 'list'. Required. Default value is \"list\"."""
    data: List["_models.OpenAIFile"] = rest_field()
    """The files returned for the request. Required."""

    @overload
    def __init__(
        self,
        *,
        data: List["_models.OpenAIFile"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.object: Literal["list"] = "list"


class FunctionCall(_model_base.Model):
    """The name and arguments of a function that should be called, as generated by the model.


    :ivar name: The name of the function to call. Required.
    :vartype name: str
    :ivar arguments: The arguments to call the function with, as generated by the model in JSON
     format.
     Note that the model does not always generate valid JSON, and may hallucinate parameters
     not defined by your function schema. Validate the arguments in your code before calling
     your function. Required.
    :vartype arguments: str
    """

    name: str = rest_field()
    """The name of the function to call. Required."""
    arguments: str = rest_field()
    """The arguments to call the function with, as generated by the model in JSON format.
     Note that the model does not always generate valid JSON, and may hallucinate parameters
     not defined by your function schema. Validate the arguments in your code before calling
     your function. Required."""

    @overload
    def __init__(
        self,
        *,
        name: str,
        arguments: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class FunctionDefinition(_model_base.Model):
    """The definition of a caller-specified function that chat completions may invoke in response to
    matching user input.

    All required parameters must be populated in order to send to server.

    :ivar name: The name of the function to be called. Required.
    :vartype name: str
    :ivar description: A description of what the function does. The model will use this description
     when selecting the function and
     interpreting its parameters.
    :vartype description: str
    :ivar parameters: The parameters the function accepts, described as a JSON Schema object.
    :vartype parameters: any
    """

    name: str = rest_field()
    """The name of the function to be called. Required."""
    description: Optional[str] = rest_field()
    """A description of what the function does. The model will use this description when selecting the
     function and
     interpreting its parameters."""
    parameters: Optional[Any] = rest_field()
    """The parameters the function accepts, described as a JSON Schema object."""

    @overload
    def __init__(
        self,
        *,
        name: str,
        description: Optional[str] = None,
        parameters: Optional[Any] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class FunctionName(_model_base.Model):
    """A structure that specifies the exact name of a specific, request-provided function to use when
    processing a chat
    completions operation.

    All required parameters must be populated in order to send to server.

    :ivar name: The name of the function to call. Required.
    :vartype name: str
    """

    name: str = rest_field()
    """The name of the function to call. Required."""

    @overload
    def __init__(
        self,
        *,
        name: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ImageGenerationContentFilterResults(_model_base.Model):
    """Describes the content filtering result for the image generation request.

    :ivar sexual: Describes language related to anatomical organs and genitals, romantic
     relationships,
      acts portrayed in erotic or affectionate terms, physical sexual acts, including
      those portrayed as an assault or a forced sexual violent act against ones will,
      prostitution, pornography, and abuse.
    :vartype sexual: ~azure.openai.models.ContentFilterResult
    :ivar violence: Describes language related to physical actions intended to hurt, injure,
     damage, or
     kill someone or something; describes weapons, etc.
    :vartype violence: ~azure.openai.models.ContentFilterResult
    :ivar hate: Describes language attacks or uses that include pejorative or discriminatory
     language
     with reference to a person or identity group on the basis of certain differentiating
     attributes of these groups including but not limited to race, ethnicity, nationality,
     gender identity and expression, sexual orientation, religion, immigration status, ability
     status, personal appearance, and body size.
    :vartype hate: ~azure.openai.models.ContentFilterResult
    :ivar self_harm: Describes language related to physical actions intended to purposely hurt,
     injure,
     or damage ones body, or kill oneself.
    :vartype self_harm: ~azure.openai.models.ContentFilterResult
    """

    sexual: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to anatomical organs and genitals, romantic relationships,
      acts portrayed in erotic or affectionate terms, physical sexual acts, including
      those portrayed as an assault or a forced sexual violent act against ones will,
      prostitution, pornography, and abuse."""
    violence: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to physical actions intended to hurt, injure, damage, or
     kill someone or something; describes weapons, etc."""
    hate: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language attacks or uses that include pejorative or discriminatory language
     with reference to a person or identity group on the basis of certain differentiating
     attributes of these groups including but not limited to race, ethnicity, nationality,
     gender identity and expression, sexual orientation, religion, immigration status, ability
     status, personal appearance, and body size."""
    self_harm: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to physical actions intended to purposely hurt, injure,
     or damage ones body, or kill oneself."""

    @overload
    def __init__(
        self,
        *,
        sexual: Optional["_models.ContentFilterResult"] = None,
        violence: Optional["_models.ContentFilterResult"] = None,
        hate: Optional["_models.ContentFilterResult"] = None,
        self_harm: Optional["_models.ContentFilterResult"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ImageGenerationData(_model_base.Model):
    """A representation of a single generated image, provided as either base64-encoded data or as a
    URL from which the image
    may be retrieved.

    :ivar url: The URL that provides temporary access to download the generated image.
    :vartype url: str
    :ivar base64_data: The complete data for an image, represented as a base64-encoded string.
    :vartype base64_data: str
    :ivar content_filter_results: Information about the content filtering results.
    :vartype content_filter_results: ~azure.openai.models.ImageGenerationContentFilterResults
    :ivar revised_prompt: The final prompt used by the model to generate the image.
     Only provided with dall-3-models and only when revisions were made to the prompt.
    :vartype revised_prompt: str
    :ivar prompt_filter_results: Information about the content filtering category (hate, sexual,
     violence, self_harm), if
     it has been detected, as well as the severity level (very_low, low, medium, high-scale
     that determines the intensity and risk level of harmful content) and if it has been
     filtered or not. Information about jailbreak content and profanity, if it has been detected,
     and if it has been filtered or not. And information about customer block list, if it has
     been filtered and its id.
    :vartype prompt_filter_results: ~azure.openai.models.ImageGenerationPromptFilterResults
    """

    url: Optional[str] = rest_field()
    """The URL that provides temporary access to download the generated image."""
    base64_data: Optional[str] = rest_field(name="b64_json")
    """The complete data for an image, represented as a base64-encoded string."""
    content_filter_results: Optional["_models.ImageGenerationContentFilterResults"] = rest_field()
    """Information about the content filtering results."""
    revised_prompt: Optional[str] = rest_field()
    """The final prompt used by the model to generate the image.
     Only provided with dall-3-models and only when revisions were made to the prompt."""
    prompt_filter_results: Optional["_models.ImageGenerationPromptFilterResults"] = rest_field()
    """Information about the content filtering category (hate, sexual, violence, self_harm), if
     it has been detected, as well as the severity level (very_low, low, medium, high-scale
     that determines the intensity and risk level of harmful content) and if it has been
     filtered or not. Information about jailbreak content and profanity, if it has been detected,
     and if it has been filtered or not. And information about customer block list, if it has
     been filtered and its id."""

    @overload
    def __init__(
        self,
        *,
        url: Optional[str] = None,
        base64_data: Optional[str] = None,
        content_filter_results: Optional["_models.ImageGenerationContentFilterResults"] = None,
        revised_prompt: Optional[str] = None,
        prompt_filter_results: Optional["_models.ImageGenerationPromptFilterResults"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ImageGenerationOptions(_model_base.Model):
    """Represents the request data used to generate images.

    All required parameters must be populated in order to send to server.

    :ivar model: The model name or Azure OpenAI model deployment name to use for image generation.
     If not specified, dall-e-2 will be
     inferred as a default.
    :vartype model: str
    :ivar prompt: A description of the desired images. Required.
    :vartype prompt: str
    :ivar n: The number of images to generate.
     Dall-e-2 models support values between 1 and 10.
     Dall-e-3 models only support a value of 1.
    :vartype n: int
    :ivar size: The desired dimensions for generated images.
     Dall-e-2 models support 256x256, 512x512, or 1024x1024.
     Dall-e-3 models support 1024x1024, 1792x1024, or 1024x1792. Known values are: "256x256",
     "512x512", "1024x1024", "1792x1024", and "1024x1792".
    :vartype size: str or ~azure.openai.models.ImageSize
    :ivar response_format: The format in which image generation response items should be presented.
     Known values are: "url" and "b64_json".
    :vartype response_format: str or ~azure.openai.models.ImageGenerationResponseFormat
    :ivar quality: The desired image generation quality level to use.
     Only configurable with dall-e-3 models. Known values are: "standard" and "hd".
    :vartype quality: str or ~azure.openai.models.ImageGenerationQuality
    :ivar style: The desired image generation style to use.
     Only configurable with dall-e-3 models. Known values are: "natural" and "vivid".
    :vartype style: str or ~azure.openai.models.ImageGenerationStyle
    :ivar user: A unique identifier representing your end-user, which can help to monitor and
     detect abuse.
    :vartype user: str
    """

    model: Optional[str] = rest_field()
    """The model name or Azure OpenAI model deployment name to use for image generation. If not
     specified, dall-e-2 will be
     inferred as a default."""
    prompt: str = rest_field()
    """A description of the desired images. Required."""
    n: Optional[int] = rest_field()
    """The number of images to generate.
     Dall-e-2 models support values between 1 and 10.
     Dall-e-3 models only support a value of 1."""
    size: Optional[Union[str, "_models.ImageSize"]] = rest_field()
    """The desired dimensions for generated images.
     Dall-e-2 models support 256x256, 512x512, or 1024x1024.
     Dall-e-3 models support 1024x1024, 1792x1024, or 1024x1792. Known values are: \"256x256\",
     \"512x512\", \"1024x1024\", \"1792x1024\", and \"1024x1792\"."""
    response_format: Optional[Union[str, "_models.ImageGenerationResponseFormat"]] = rest_field()
    """The format in which image generation response items should be presented. Known values are:
     \"url\" and \"b64_json\"."""
    quality: Optional[Union[str, "_models.ImageGenerationQuality"]] = rest_field()
    """The desired image generation quality level to use.
     Only configurable with dall-e-3 models. Known values are: \"standard\" and \"hd\"."""
    style: Optional[Union[str, "_models.ImageGenerationStyle"]] = rest_field()
    """The desired image generation style to use.
     Only configurable with dall-e-3 models. Known values are: \"natural\" and \"vivid\"."""
    user: Optional[str] = rest_field()
    """A unique identifier representing your end-user, which can help to monitor and detect abuse."""

    @overload
    def __init__(
        self,
        *,
        prompt: str,
        model: Optional[str] = None,
        n: Optional[int] = None,
        size: Optional[Union[str, "_models.ImageSize"]] = None,
        response_format: Optional[Union[str, "_models.ImageGenerationResponseFormat"]] = None,
        quality: Optional[Union[str, "_models.ImageGenerationQuality"]] = None,
        style: Optional[Union[str, "_models.ImageGenerationStyle"]] = None,
        user: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ImageGenerationPromptFilterResults(_model_base.Model):
    """Describes the content filtering results for the prompt of a image generation request.

    :ivar sexual: Describes language related to anatomical organs and genitals, romantic
     relationships,
      acts portrayed in erotic or affectionate terms, physical sexual acts, including
      those portrayed as an assault or a forced sexual violent act against ones will,
      prostitution, pornography, and abuse.
    :vartype sexual: ~azure.openai.models.ContentFilterResult
    :ivar violence: Describes language related to physical actions intended to hurt, injure,
     damage, or
     kill someone or something; describes weapons, etc.
    :vartype violence: ~azure.openai.models.ContentFilterResult
    :ivar hate: Describes language attacks or uses that include pejorative or discriminatory
     language
     with reference to a person or identity group on the basis of certain differentiating
     attributes of these groups including but not limited to race, ethnicity, nationality,
     gender identity and expression, sexual orientation, religion, immigration status, ability
     status, personal appearance, and body size.
    :vartype hate: ~azure.openai.models.ContentFilterResult
    :ivar self_harm: Describes language related to physical actions intended to purposely hurt,
     injure,
     or damage ones body, or kill oneself.
    :vartype self_harm: ~azure.openai.models.ContentFilterResult
    :ivar profanity: Describes whether profanity was detected.
    :vartype profanity: ~azure.openai.models.ContentFilterDetectionResult
    :ivar jailbreak: Whether a jailbreak attempt was detected in the prompt.
    :vartype jailbreak: ~azure.openai.models.ContentFilterDetectionResult
    :ivar custom_blocklists: Information about customer block lists and if something was detected
     the associated list ID.
    :vartype custom_blocklists: ~azure.openai.models.ContentFilterDetailedResults
    """

    sexual: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to anatomical organs and genitals, romantic relationships,
      acts portrayed in erotic or affectionate terms, physical sexual acts, including
      those portrayed as an assault or a forced sexual violent act against ones will,
      prostitution, pornography, and abuse."""
    violence: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to physical actions intended to hurt, injure, damage, or
     kill someone or something; describes weapons, etc."""
    hate: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language attacks or uses that include pejorative or discriminatory language
     with reference to a person or identity group on the basis of certain differentiating
     attributes of these groups including but not limited to race, ethnicity, nationality,
     gender identity and expression, sexual orientation, religion, immigration status, ability
     status, personal appearance, and body size."""
    self_harm: Optional["_models.ContentFilterResult"] = rest_field()
    """Describes language related to physical actions intended to purposely hurt, injure,
     or damage ones body, or kill oneself."""
    profanity: Optional["_models.ContentFilterDetectionResult"] = rest_field()
    """Describes whether profanity was detected."""
    jailbreak: Optional["_models.ContentFilterDetectionResult"] = rest_field()
    """Whether a jailbreak attempt was detected in the prompt."""
    custom_blocklists: Optional["_models.ContentFilterDetailedResults"] = rest_field()
    """Information about customer block lists and if something was detected the associated list ID."""

    @overload
    def __init__(
        self,
        *,
        sexual: Optional["_models.ContentFilterResult"] = None,
        violence: Optional["_models.ContentFilterResult"] = None,
        hate: Optional["_models.ContentFilterResult"] = None,
        self_harm: Optional["_models.ContentFilterResult"] = None,
        profanity: Optional["_models.ContentFilterDetectionResult"] = None,
        jailbreak: Optional["_models.ContentFilterDetectionResult"] = None,
        custom_blocklists: Optional["_models.ContentFilterDetailedResults"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class ImageGenerations(_model_base.Model):
    """The result of a successful image generation operation.


    :ivar created: A timestamp representing when this operation was started.
     Expressed in seconds since the Unix epoch of 1970-01-01T00:00:00+0000. Required.
    :vartype created: ~datetime.datetime
    :ivar data: The images generated by the operation. Required.
    :vartype data: list[~azure.openai.models.ImageGenerationData]
    """

    created: datetime.datetime = rest_field(format="unix-timestamp")
    """A timestamp representing when this operation was started.
     Expressed in seconds since the Unix epoch of 1970-01-01T00:00:00+0000. Required."""
    data: List["_models.ImageGenerationData"] = rest_field()
    """The images generated by the operation. Required."""

    @overload
    def __init__(
        self,
        *,
        created: datetime.datetime,
        data: List["_models.ImageGenerationData"],
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class MongoDBChatExtensionConfiguration(AzureChatExtensionConfiguration, discriminator="mongo_db"):
    """A specific representation of configurable options for a MongoDB chat extension configuration.

    All required parameters must be populated in order to send to server.

    :ivar type: The type label for MongoDB. Required. Represents the use of a MongoDB chat
     extension.
    :vartype type: str or ~azure.openai.models.MONGO_DB
    :ivar parameters: The parameters for the MongoDB chat extension. Required.
    :vartype parameters: ~azure.openai.models.MongoDBChatExtensionParameters
    """

    type: Literal[AzureChatExtensionType.MONGO_DB] = rest_discriminator(name="type")  # type: ignore
    """The type label for MongoDB. Required. Represents the use of a MongoDB chat extension."""
    parameters: "_models.MongoDBChatExtensionParameters" = rest_field()
    """The parameters for the MongoDB chat extension. Required."""

    @overload
    def __init__(
        self,
        *,
        parameters: "_models.MongoDBChatExtensionParameters",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=AzureChatExtensionType.MONGO_DB, **kwargs)


class MongoDBChatExtensionParameters(_model_base.Model):
    """Parameters for the MongoDB chat extension. The supported authentication types are AccessToken,
    SystemAssignedManagedIdentity and UserAssignedManagedIdentity.

    All required parameters must be populated in order to send to server.

    :ivar top_n_documents: The configured top number of documents to feature for the configured
     query.
    :vartype top_n_documents: int
    :ivar in_scope: Whether queries should be restricted to use of indexed data.
    :vartype in_scope: bool
    :ivar strictness: The configured strictness of the search relevance filtering. The higher of
     strictness, the higher of the precision but lower recall of the answer.
    :vartype strictness: int
    :ivar max_search_queries: The max number of rewritten queries should be send to search provider
     for one user message. If not specified,
     the system will decide the number of queries to send.
    :vartype max_search_queries: int
    :ivar allow_partial_result: If specified as true, the system will allow partial search results
     to be used and the request fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails.
    :vartype allow_partial_result: bool
    :ivar include_contexts: The included properties of the output context. If not specified, the
     default value is ``citations`` and ``intent``.
    :vartype include_contexts: list[str or ~azure.openai.models.OnYourDataContextProperty]
    :ivar authentication: The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication.
    :vartype authentication:
     ~azure.openai.models.OnYourDataUsernameAndPasswordAuthenticationOptions
    :ivar endpoint: The endpoint name for MongoDB. Required.
    :vartype endpoint: str
    :ivar collection_name: The collection name for MongoDB. Required.
    :vartype collection_name: str
    :ivar database_name: The database name for MongoDB. Required.
    :vartype database_name: str
    :ivar app_name: The app name for MongoDB. Required.
    :vartype app_name: str
    :ivar index_name: The name of the MongoDB index. Required.
    :vartype index_name: str
    :ivar fields_mapping: Field mappings to apply to data used by the MongoDB data source.
     Note that content and vector field mappings are required for MongoDB. Required.
    :vartype fields_mapping: ~azure.openai.models.MongoDBChatExtensionParametersFieldsMapping
    :ivar embedding_dependency: The vectorization source to use with the MongoDB chat extension.
     Required. Is either a OnYourDataEndpointVectorizationSource type or a
     OnYourDataDeploymentNameVectorizationSource type.
    :vartype embedding_dependency: ~azure.openai.models.OnYourDataEndpointVectorizationSource or
     ~azure.openai.models.OnYourDataDeploymentNameVectorizationSource
    """

    top_n_documents: Optional[int] = rest_field()
    """The configured top number of documents to feature for the configured query."""
    in_scope: Optional[bool] = rest_field()
    """Whether queries should be restricted to use of indexed data."""
    strictness: Optional[int] = rest_field()
    """The configured strictness of the search relevance filtering. The higher of strictness, the
     higher of the precision but lower recall of the answer."""
    max_search_queries: Optional[int] = rest_field()
    """The max number of rewritten queries should be send to search provider for one user message. If
     not specified,
     the system will decide the number of queries to send."""
    allow_partial_result: Optional[bool] = rest_field()
    """If specified as true, the system will allow partial search results to be used and the request
     fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails."""
    include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = rest_field()
    """The included properties of the output context. If not specified, the default value is
     ``citations`` and ``intent``."""
    authentication: Optional["_models.OnYourDataUsernameAndPasswordAuthenticationOptions"] = rest_field()
    """The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication."""
    endpoint: str = rest_field()
    """The endpoint name for MongoDB. Required."""
    collection_name: str = rest_field()
    """The collection name for MongoDB. Required."""
    database_name: str = rest_field()
    """The database name for MongoDB. Required."""
    app_name: str = rest_field()
    """The app name for MongoDB. Required."""
    index_name: str = rest_field()
    """The name of the MongoDB index. Required."""
    fields_mapping: "_models.MongoDBChatExtensionParametersFieldsMapping" = rest_field()
    """Field mappings to apply to data used by the MongoDB data source.
     Note that content and vector field mappings are required for MongoDB. Required."""
    embedding_dependency: Union[
        "_models.OnYourDataEndpointVectorizationSource", "_models.OnYourDataDeploymentNameVectorizationSource"
    ] = rest_field()
    """The vectorization source to use with the MongoDB chat extension. Required. Is either a
     OnYourDataEndpointVectorizationSource type or a OnYourDataDeploymentNameVectorizationSource
     type."""

    @overload
    def __init__(
        self,
        *,
        endpoint: str,
        collection_name: str,
        database_name: str,
        app_name: str,
        index_name: str,
        fields_mapping: "_models.MongoDBChatExtensionParametersFieldsMapping",
        embedding_dependency: Union[
            "_models.OnYourDataEndpointVectorizationSource", "_models.OnYourDataDeploymentNameVectorizationSource"
        ],
        top_n_documents: Optional[int] = None,
        in_scope: Optional[bool] = None,
        strictness: Optional[int] = None,
        max_search_queries: Optional[int] = None,
        allow_partial_result: Optional[bool] = None,
        include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = None,
        authentication: Optional["_models.OnYourDataUsernameAndPasswordAuthenticationOptions"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class MongoDBChatExtensionParametersFieldsMapping(_model_base.Model):  # pylint: disable=name-too-long
    """MongoDBChatExtensionParametersFieldsMapping.

    All required parameters must be populated in order to send to server.

    :ivar content_fields: Required.
    :vartype content_fields: list[str]
    :ivar vector_fields: Required.
    :vartype vector_fields: list[str]
    :ivar title_field:
    :vartype title_field: str
    :ivar url_field:
    :vartype url_field: str
    :ivar filepath_field:
    :vartype filepath_field: str
    :ivar content_fields_separator:
    :vartype content_fields_separator: str
    """

    content_fields: List[str] = rest_field()
    """Required."""
    vector_fields: List[str] = rest_field()
    """Required."""
    title_field: Optional[str] = rest_field()
    url_field: Optional[str] = rest_field()
    filepath_field: Optional[str] = rest_field()
    content_fields_separator: Optional[str] = rest_field()

    @overload
    def __init__(
        self,
        *,
        content_fields: List[str],
        vector_fields: List[str],
        title_field: Optional[str] = None,
        url_field: Optional[str] = None,
        filepath_field: Optional[str] = None,
        content_fields_separator: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class OnYourDataAuthenticationOptions(_model_base.Model):
    """The authentication options for Azure OpenAI On Your Data.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    OnYourDataAccessTokenAuthenticationOptions, OnYourDataApiKeyAuthenticationOptions,
    OnYourDataConnectionStringAuthenticationOptions, OnYourDataEncodedApiKeyAuthenticationOptions,
    OnYourDataKeyAndKeyIdAuthenticationOptions,
    OnYourDataSystemAssignedManagedIdentityAuthenticationOptions,
    OnYourDataUserAssignedManagedIdentityAuthenticationOptions,
    OnYourDataUsernameAndPasswordAuthenticationOptions

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type. Required. Known values are: "api_key",
     "connection_string", "key_and_key_id", "encoded_api_key", "access_token",
     "system_assigned_managed_identity", "user_assigned_managed_identity", and
     "username_and_password".
    :vartype type: str or ~azure.openai.models.OnYourDataAuthenticationType
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The authentication type. Required. Known values are: \"api_key\", \"connection_string\",
     \"key_and_key_id\", \"encoded_api_key\", \"access_token\",
     \"system_assigned_managed_identity\", \"user_assigned_managed_identity\", and
     \"username_and_password\"."""

    @overload
    def __init__(
        self,
        *,
        type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class OnYourDataAccessTokenAuthenticationOptions(
    OnYourDataAuthenticationOptions, discriminator="access_token"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data when using access token.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of access token. Required. Authentication via access token.
    :vartype type: str or ~azure.openai.models.ACCESS_TOKEN
    :ivar access_token: The access token to use for authentication. Required.
    :vartype access_token: str
    """

    type: Literal[OnYourDataAuthenticationType.ACCESS_TOKEN] = rest_discriminator(name="type")  # type: ignore
    """The authentication type of access token. Required. Authentication via access token."""
    access_token: str = rest_field()
    """The access token to use for authentication. Required."""

    @overload
    def __init__(
        self,
        *,
        access_token: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataAuthenticationType.ACCESS_TOKEN, **kwargs)


class OnYourDataApiKeyAuthenticationOptions(OnYourDataAuthenticationOptions, discriminator="api_key"):
    """The authentication options for Azure OpenAI On Your Data when using an API key.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of API key. Required. Authentication via API key.
    :vartype type: str or ~azure.openai.models.API_KEY
    :ivar key: The API key to use for authentication. Required.
    :vartype key: str
    """

    type: Literal[OnYourDataAuthenticationType.API_KEY] = rest_discriminator(name="type")  # type: ignore
    """The authentication type of API key. Required. Authentication via API key."""
    key: str = rest_field()
    """The API key to use for authentication. Required."""

    @overload
    def __init__(
        self,
        *,
        key: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataAuthenticationType.API_KEY, **kwargs)


class OnYourDataConnectionStringAuthenticationOptions(
    OnYourDataAuthenticationOptions, discriminator="connection_string"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data when using a connection string.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of connection string. Required. Authentication via
     connection string.
    :vartype type: str or ~azure.openai.models.CONNECTION_STRING
    :ivar connection_string: The connection string to use for authentication. Required.
    :vartype connection_string: str
    """

    type: Literal[OnYourDataAuthenticationType.CONNECTION_STRING] = rest_discriminator(name="type")  # type: ignore
    """The authentication type of connection string. Required. Authentication via connection string."""
    connection_string: str = rest_field()
    """The connection string to use for authentication. Required."""

    @overload
    def __init__(
        self,
        *,
        connection_string: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataAuthenticationType.CONNECTION_STRING, **kwargs)


class OnYourDataVectorizationSource(_model_base.Model):
    """An abstract representation of a vectorization source for Azure OpenAI On Your Data with vector
    search.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    OnYourDataDeploymentNameVectorizationSource, OnYourDataEndpointVectorizationSource,
    OnYourDataIntegratedVectorizationSource, OnYourDataModelIdVectorizationSource

    All required parameters must be populated in order to send to server.

    :ivar type: The type of vectorization source to use. Required. Known values are: "endpoint",
     "deployment_name", "model_id", and "integrated".
    :vartype type: str or ~azure.openai.models.OnYourDataVectorizationSourceType
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The type of vectorization source to use. Required. Known values are: \"endpoint\",
     \"deployment_name\", \"model_id\", and \"integrated\"."""

    @overload
    def __init__(
        self,
        *,
        type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class OnYourDataDeploymentNameVectorizationSource(
    OnYourDataVectorizationSource, discriminator="deployment_name"
):  # pylint: disable=name-too-long
    """The details of a a vectorization source, used by Azure OpenAI On Your Data when applying vector
    search, that is based
    on an internal embeddings model deployment name in the same Azure OpenAI resource.

    All required parameters must be populated in order to send to server.

    :ivar type: The type of vectorization source to use. Always 'DeploymentName' for this type.
     Required. Represents an Ada model deployment name to use. This model deployment must be in the
     same Azure OpenAI resource, but
     On Your Data will use this model deployment via an internal call rather than a public one,
     which enables vector
     search even in private networks.
    :vartype type: str or ~azure.openai.models.DEPLOYMENT_NAME
    :ivar deployment_name: The embedding model deployment name within the same Azure OpenAI
     resource. This enables you to use vector search without Azure OpenAI api-key and without Azure
     OpenAI public network access. Required.
    :vartype deployment_name: str
    :ivar dimensions: The number of dimensions the embeddings should have. Only supported in
     ``text-embedding-3`` and later models.
    :vartype dimensions: int
    """

    type: Literal[OnYourDataVectorizationSourceType.DEPLOYMENT_NAME] = rest_discriminator(name="type")  # type: ignore
    """The type of vectorization source to use. Always 'DeploymentName' for this type. Required.
     Represents an Ada model deployment name to use. This model deployment must be in the same Azure
     OpenAI resource, but
     On Your Data will use this model deployment via an internal call rather than a public one,
     which enables vector
     search even in private networks."""
    deployment_name: str = rest_field()
    """The embedding model deployment name within the same Azure OpenAI resource. This enables you to
     use vector search without Azure OpenAI api-key and without Azure OpenAI public network access.
     Required."""
    dimensions: Optional[int] = rest_field()
    """The number of dimensions the embeddings should have. Only supported in ``text-embedding-3`` and
     later models."""

    @overload
    def __init__(
        self,
        *,
        deployment_name: str,
        dimensions: Optional[int] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataVectorizationSourceType.DEPLOYMENT_NAME, **kwargs)


class OnYourDataEncodedApiKeyAuthenticationOptions(
    OnYourDataAuthenticationOptions, discriminator="encoded_api_key"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data when using an Elasticsearch encoded
    API key.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of Elasticsearch encoded API Key. Required. Authentication
     via encoded API key.
    :vartype type: str or ~azure.openai.models.ENCODED_API_KEY
    :ivar encoded_api_key: The encoded API key to use for authentication. Required.
    :vartype encoded_api_key: str
    """

    type: Literal[OnYourDataAuthenticationType.ENCODED_API_KEY] = rest_discriminator(name="type")  # type: ignore
    """The authentication type of Elasticsearch encoded API Key. Required. Authentication via encoded
     API key."""
    encoded_api_key: str = rest_field()
    """The encoded API key to use for authentication. Required."""

    @overload
    def __init__(
        self,
        *,
        encoded_api_key: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataAuthenticationType.ENCODED_API_KEY, **kwargs)


class OnYourDataEndpointVectorizationSource(OnYourDataVectorizationSource, discriminator="endpoint"):
    """The details of a a vectorization source, used by Azure OpenAI On Your Data when applying vector
    search, that is based
    on a public Azure OpenAI endpoint call for embeddings.

    All required parameters must be populated in order to send to server.

    :ivar type: The type of vectorization source to use. Always 'Endpoint' for this type. Required.
     Represents vectorization performed by public service calls to an Azure OpenAI embedding model.
    :vartype type: str or ~azure.openai.models.ENDPOINT
    :ivar endpoint: Specifies the resource endpoint URL from which embeddings should be retrieved.
     It should be in the format of
     https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/embeddings.
     The api-version query parameter is not allowed. Required.
    :vartype endpoint: str
    :ivar authentication: Specifies the authentication options to use when retrieving embeddings
     from the specified endpoint. Required.
    :vartype authentication: ~azure.openai.models.OnYourDataVectorSearchAuthenticationOptions
    """

    type: Literal[OnYourDataVectorizationSourceType.ENDPOINT] = rest_discriminator(name="type")  # type: ignore
    """The type of vectorization source to use. Always 'Endpoint' for this type. Required. Represents
     vectorization performed by public service calls to an Azure OpenAI embedding model."""
    endpoint: str = rest_field()
    """Specifies the resource endpoint URL from which embeddings should be retrieved. It should be in
     the format of
     https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/embeddings.
     The api-version query parameter is not allowed. Required."""
    authentication: "_models.OnYourDataVectorSearchAuthenticationOptions" = rest_field()
    """Specifies the authentication options to use when retrieving embeddings from the specified
     endpoint. Required."""

    @overload
    def __init__(
        self,
        *,
        endpoint: str,
        authentication: "_models.OnYourDataVectorSearchAuthenticationOptions",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataVectorizationSourceType.ENDPOINT, **kwargs)


class OnYourDataIntegratedVectorizationSource(OnYourDataVectorizationSource, discriminator="integrated"):
    """Represents the integrated vectorizer defined within the search resource.

    All required parameters must be populated in order to send to server.

    :ivar type: The type discriminator. ALways 'integrated'. Required. Represents the integrated
     vectorizer defined within the search resource.
    :vartype type: str or ~azure.openai.models.INTEGRATED
    """

    type: Literal[OnYourDataVectorizationSourceType.INTEGRATED] = rest_discriminator(name="type")  # type: ignore
    """The type discriminator. ALways 'integrated'. Required. Represents the integrated vectorizer
     defined within the search resource."""

    @overload
    def __init__(
        self,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataVectorizationSourceType.INTEGRATED, **kwargs)


class OnYourDataKeyAndKeyIdAuthenticationOptions(
    OnYourDataAuthenticationOptions, discriminator="key_and_key_id"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data when using an Elasticsearch key and
    key ID pair.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of Elasticsearch key and key ID pair. Required.
     Authentication via key and key ID pair.
    :vartype type: str or ~azure.openai.models.KEY_AND_KEY_ID
    :ivar key: The key to use for authentication. Required.
    :vartype key: str
    :ivar key_id: The key ID to use for authentication. Required.
    :vartype key_id: str
    """

    type: Literal[OnYourDataAuthenticationType.KEY_AND_KEY_ID] = rest_discriminator(name="type")  # type: ignore
    """The authentication type of Elasticsearch key and key ID pair. Required. Authentication via key
     and key ID pair."""
    key: str = rest_field()
    """The key to use for authentication. Required."""
    key_id: str = rest_field()
    """The key ID to use for authentication. Required."""

    @overload
    def __init__(
        self,
        *,
        key: str,
        key_id: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataAuthenticationType.KEY_AND_KEY_ID, **kwargs)


class OnYourDataModelIdVectorizationSource(OnYourDataVectorizationSource, discriminator="model_id"):
    """The details of a a vectorization source, used by Azure OpenAI On Your Data when applying vector
    search, that is based
    on a search service model ID. Currently only supported by Elasticsearch.

    All required parameters must be populated in order to send to server.

    :ivar type: The type of vectorization source to use. Always 'ModelId' for this type. Required.
     Represents a specific embedding model ID as defined in the search service.
     Currently only supported by Elasticsearch.
    :vartype type: str or ~azure.openai.models.MODEL_ID
    :ivar model_id: The embedding model ID build inside the search service. Currently only
     supported by Elasticsearch. Required.
    :vartype model_id: str
    """

    type: Literal[OnYourDataVectorizationSourceType.MODEL_ID] = rest_discriminator(name="type")  # type: ignore
    """The type of vectorization source to use. Always 'ModelId' for this type. Required. Represents a
     specific embedding model ID as defined in the search service.
     Currently only supported by Elasticsearch."""
    model_id: str = rest_field()
    """The embedding model ID build inside the search service. Currently only supported by
     Elasticsearch. Required."""

    @overload
    def __init__(
        self,
        *,
        model_id: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataVectorizationSourceType.MODEL_ID, **kwargs)


class OnYourDataSystemAssignedManagedIdentityAuthenticationOptions(
    OnYourDataAuthenticationOptions, discriminator="system_assigned_managed_identity"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data when using a system-assigned managed
    identity.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of system-assigned managed identity. Required.
     Authentication via system-assigned managed identity.
    :vartype type: str or ~azure.openai.models.SYSTEM_ASSIGNED_MANAGED_IDENTITY
    """

    type: Literal[OnYourDataAuthenticationType.SYSTEM_ASSIGNED_MANAGED_IDENTITY] = rest_discriminator(name="type")  # type: ignore # pylint: disable=line-too-long
    """The authentication type of system-assigned managed identity. Required. Authentication via
     system-assigned managed identity."""

    @overload
    def __init__(
        self,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataAuthenticationType.SYSTEM_ASSIGNED_MANAGED_IDENTITY, **kwargs)


class OnYourDataUserAssignedManagedIdentityAuthenticationOptions(
    OnYourDataAuthenticationOptions, discriminator="user_assigned_managed_identity"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data when using a user-assigned managed
    identity.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of user-assigned managed identity. Required. Authentication
     via user-assigned managed identity.
    :vartype type: str or ~azure.openai.models.USER_ASSIGNED_MANAGED_IDENTITY
    :ivar managed_identity_resource_id: The resource ID of the user-assigned managed identity to
     use for authentication. Required.
    :vartype managed_identity_resource_id: str
    """

    type: Literal[OnYourDataAuthenticationType.USER_ASSIGNED_MANAGED_IDENTITY] = rest_discriminator(name="type")  # type: ignore # pylint: disable=line-too-long
    """The authentication type of user-assigned managed identity. Required. Authentication via
     user-assigned managed identity."""
    managed_identity_resource_id: str = rest_field()
    """The resource ID of the user-assigned managed identity to use for authentication. Required."""

    @overload
    def __init__(
        self,
        *,
        managed_identity_resource_id: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataAuthenticationType.USER_ASSIGNED_MANAGED_IDENTITY, **kwargs)


class OnYourDataUsernameAndPasswordAuthenticationOptions(
    OnYourDataAuthenticationOptions, discriminator="username_and_password"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data when using a username and password.

    All required parameters must be populated in order to send to server.

    :ivar type: The discriminator type for username and password. Required. Authentication via
     username and password.
    :vartype type: str or ~azure.openai.models.USERNAME_AND_PASSWORD
    :ivar username: The username. Required.
    :vartype username: str
    :ivar password: The password. Required.
    :vartype password: str
    """

    type: Literal[OnYourDataAuthenticationType.USERNAME_AND_PASSWORD] = rest_discriminator(name="type")  # type: ignore
    """The discriminator type for username and password. Required. Authentication via username and
     password."""
    username: str = rest_field()
    """The username. Required."""
    password: str = rest_field()
    """The password. Required."""

    @overload
    def __init__(
        self,
        *,
        username: str,
        password: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataAuthenticationType.USERNAME_AND_PASSWORD, **kwargs)


class OnYourDataVectorSearchAuthenticationOptions(_model_base.Model):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data vector search.

    You probably want to use the sub-classes and not this class directly. Known sub-classes are:
    OnYourDataVectorSearchAccessTokenAuthenticationOptions,
    OnYourDataVectorSearchApiKeyAuthenticationOptions

    All required parameters must be populated in order to send to server.

    :ivar type: The type of authentication to use. Required. Known values are: "api_key" and
     "access_token".
    :vartype type: str or ~azure.openai.models.OnYourDataVectorSearchAuthenticationType
    """

    __mapping__: Dict[str, _model_base.Model] = {}
    type: str = rest_discriminator(name="type")
    """The type of authentication to use. Required. Known values are: \"api_key\" and
     \"access_token\"."""

    @overload
    def __init__(
        self,
        *,
        type: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class OnYourDataVectorSearchAccessTokenAuthenticationOptions(
    OnYourDataVectorSearchAuthenticationOptions, discriminator="access_token"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data vector search when using access token.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of access token. Required. Authentication via access token.
    :vartype type: str or ~azure.openai.models.ACCESS_TOKEN
    :ivar access_token: The access token to use for authentication. Required.
    :vartype access_token: str
    """

    type: Literal[OnYourDataVectorSearchAuthenticationType.ACCESS_TOKEN] = rest_discriminator(name="type")  # type: ignore # pylint: disable=line-too-long
    """The authentication type of access token. Required. Authentication via access token."""
    access_token: str = rest_field()
    """The access token to use for authentication. Required."""

    @overload
    def __init__(
        self,
        *,
        access_token: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataVectorSearchAuthenticationType.ACCESS_TOKEN, **kwargs)


class OnYourDataVectorSearchApiKeyAuthenticationOptions(
    OnYourDataVectorSearchAuthenticationOptions, discriminator="api_key"
):  # pylint: disable=name-too-long
    """The authentication options for Azure OpenAI On Your Data when using an API key.

    All required parameters must be populated in order to send to server.

    :ivar type: The authentication type of API key. Required. Authentication via API key.
    :vartype type: str or ~azure.openai.models.API_KEY
    :ivar key: The API key to use for authentication. Required.
    :vartype key: str
    """

    type: Literal[OnYourDataVectorSearchAuthenticationType.API_KEY] = rest_discriminator(name="type")  # type: ignore
    """The authentication type of API key. Required. Authentication via API key."""
    key: str = rest_field()
    """The API key to use for authentication. Required."""

    @overload
    def __init__(
        self,
        *,
        key: str,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=OnYourDataVectorSearchAuthenticationType.API_KEY, **kwargs)


class OpenAIFile(_model_base.Model):
    """Represents an assistant that can call the model and use tools.

    Readonly variables are only populated by the server, and will be ignored when sending a request.


    :ivar object: The object type, which is always 'file'. Required. Default value is "file".
    :vartype object: str
    :ivar id: The identifier, which can be referenced in API endpoints. Required.
    :vartype id: str
    :ivar bytes: The size of the file, in bytes. Required.
    :vartype bytes: int
    :ivar filename: The name of the file. Required.
    :vartype filename: str
    :ivar created_at: The Unix timestamp, in seconds, representing when this object was created.
     Required.
    :vartype created_at: ~datetime.datetime
    :ivar purpose: The intended purpose of a file. Required. Known values are: "fine-tune",
     "fine-tune-results", "assistants", "assistants_output", "batch", "batch_output", and "vision".
    :vartype purpose: str or ~azure.openai.models.FilePurpose
    :ivar status: The state of the file. This field is available in Azure OpenAI only. Known values
     are: "uploaded", "pending", "running", "processed", "error", "deleting", and "deleted".
    :vartype status: str or ~azure.openai.models.FileState
    :ivar status_details: The error message with details in case processing of this file failed.
     This field is available in Azure OpenAI only.
    :vartype status_details: str
    """

    object: Literal["file"] = rest_field()
    """The object type, which is always 'file'. Required. Default value is \"file\"."""
    id: str = rest_field()
    """The identifier, which can be referenced in API endpoints. Required."""
    bytes: int = rest_field()
    """The size of the file, in bytes. Required."""
    filename: str = rest_field()
    """The name of the file. Required."""
    created_at: datetime.datetime = rest_field(format="unix-timestamp")
    """The Unix timestamp, in seconds, representing when this object was created. Required."""
    purpose: Union[str, "_models.FilePurpose"] = rest_field()
    """The intended purpose of a file. Required. Known values are: \"fine-tune\",
     \"fine-tune-results\", \"assistants\", \"assistants_output\", \"batch\", \"batch_output\", and
     \"vision\"."""
    status: Optional[Union[str, "_models.FileState"]] = rest_field()
    """The state of the file. This field is available in Azure OpenAI only. Known values are:
     \"uploaded\", \"pending\", \"running\", \"processed\", \"error\", \"deleting\", and
     \"deleted\"."""
    status_details: Optional[str] = rest_field()
    """The error message with details in case processing of this file failed. This field is available
     in Azure OpenAI only."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        bytes: int,
        filename: str,
        created_at: datetime.datetime,
        purpose: Union[str, "_models.FilePurpose"],
        status: Optional[Union[str, "_models.FileState"]] = None,
        status_details: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.object: Literal["file"] = "file"


class OpenAIPageableListOfBatch(_model_base.Model):
    """The response data for a requested list of items.

    Readonly variables are only populated by the server, and will be ignored when sending a request.


    :ivar object: The object type, which is always list. Required. Default value is "list".
    :vartype object: str
    :ivar data: The requested list of items.
    :vartype data: list[~azure.openai.models.Batch]
    :ivar first_id: The first ID represented in this list.
    :vartype first_id: str
    :ivar last_id: The last ID represented in this list.
    :vartype last_id: str
    :ivar has_more: A value indicating whether there are additional values available not captured
     in this list.
    :vartype has_more: bool
    """

    object: Literal["list"] = rest_field()
    """The object type, which is always list. Required. Default value is \"list\"."""
    data: Optional[List["_models.Batch"]] = rest_field()
    """The requested list of items."""
    first_id: Optional[str] = rest_field()
    """The first ID represented in this list."""
    last_id: Optional[str] = rest_field()
    """The last ID represented in this list."""
    has_more: Optional[bool] = rest_field()
    """A value indicating whether there are additional values available not captured in this list."""

    @overload
    def __init__(
        self,
        *,
        data: Optional[List["_models.Batch"]] = None,
        first_id: Optional[str] = None,
        last_id: Optional[str] = None,
        has_more: Optional[bool] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.object: Literal["list"] = "list"


class PineconeChatExtensionConfiguration(AzureChatExtensionConfiguration, discriminator="pinecone"):
    """A specific representation of configurable options for Pinecone when using it as an Azure OpenAI
    chat
    extension.

    All required parameters must be populated in order to send to server.

    :ivar type: The type label to use when configuring Azure OpenAI chat extensions. This should
     typically not be changed from its
     default value for Pinecone. Required. Represents the use of Pinecone index as an Azure OpenAI
     chat extension.
    :vartype type: str or ~azure.openai.models.PINECONE
    :ivar parameters: The parameters to use when configuring Azure OpenAI chat extensions.
     Required.
    :vartype parameters: ~azure.openai.models.PineconeChatExtensionParameters
    """

    type: Literal[AzureChatExtensionType.PINECONE] = rest_discriminator(name="type")  # type: ignore
    """The type label to use when configuring Azure OpenAI chat extensions. This should typically not
     be changed from its
     default value for Pinecone. Required. Represents the use of Pinecone index as an Azure OpenAI
     chat extension."""
    parameters: "_models.PineconeChatExtensionParameters" = rest_field()
    """The parameters to use when configuring Azure OpenAI chat extensions. Required."""

    @overload
    def __init__(
        self,
        *,
        parameters: "_models.PineconeChatExtensionParameters",
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, type=AzureChatExtensionType.PINECONE, **kwargs)


class PineconeChatExtensionParameters(_model_base.Model):
    """Parameters for configuring Azure OpenAI Pinecone chat extensions. The supported authentication
    type is APIKey.

    All required parameters must be populated in order to send to server.

    :ivar top_n_documents: The configured top number of documents to feature for the configured
     query.
    :vartype top_n_documents: int
    :ivar in_scope: Whether queries should be restricted to use of indexed data.
    :vartype in_scope: bool
    :ivar strictness: The configured strictness of the search relevance filtering. The higher of
     strictness, the higher of the precision but lower recall of the answer.
    :vartype strictness: int
    :ivar max_search_queries: The max number of rewritten queries should be send to search provider
     for one user message. If not specified,
     the system will decide the number of queries to send.
    :vartype max_search_queries: int
    :ivar allow_partial_result: If specified as true, the system will allow partial search results
     to be used and the request fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails.
    :vartype allow_partial_result: bool
    :ivar include_contexts: The included properties of the output context. If not specified, the
     default value is ``citations`` and ``intent``.
    :vartype include_contexts: list[str or ~azure.openai.models.OnYourDataContextProperty]
    :ivar authentication: The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication.
    :vartype authentication: ~azure.openai.models.OnYourDataAuthenticationOptions
    :ivar environment: The environment name of Pinecone. Required.
    :vartype environment: str
    :ivar index_name: The name of the Pinecone database index. Required.
    :vartype index_name: str
    :ivar fields_mapping: Customized field mapping behavior to use when interacting with the search
     index. Required.
    :vartype fields_mapping: ~azure.openai.models.PineconeFieldMappingOptions
    :ivar embedding_dependency: The embedding dependency for vector search. Required.
    :vartype embedding_dependency: ~azure.openai.models.OnYourDataVectorizationSource
    """

    top_n_documents: Optional[int] = rest_field()
    """The configured top number of documents to feature for the configured query."""
    in_scope: Optional[bool] = rest_field()
    """Whether queries should be restricted to use of indexed data."""
    strictness: Optional[int] = rest_field()
    """The configured strictness of the search relevance filtering. The higher of strictness, the
     higher of the precision but lower recall of the answer."""
    max_search_queries: Optional[int] = rest_field()
    """The max number of rewritten queries should be send to search provider for one user message. If
     not specified,
     the system will decide the number of queries to send."""
    allow_partial_result: Optional[bool] = rest_field()
    """If specified as true, the system will allow partial search results to be used and the request
     fails if all the queries fail.
     If not specified, or specified as false, the request will fail if any search query fails."""
    include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = rest_field()
    """The included properties of the output context. If not specified, the default value is
     ``citations`` and ``intent``."""
    authentication: Optional["_models.OnYourDataAuthenticationOptions"] = rest_field()
    """The authentication method to use when accessing the defined data source.
     Each data source type supports a specific set of available authentication methods; please see
     the documentation of
     the data source for supported mechanisms.
     If not otherwise provided, On Your Data will attempt to use System Managed Identity (default
     credential)
     authentication."""
    environment: str = rest_field()
    """The environment name of Pinecone. Required."""
    index_name: str = rest_field()
    """The name of the Pinecone database index. Required."""
    fields_mapping: "_models.PineconeFieldMappingOptions" = rest_field()
    """Customized field mapping behavior to use when interacting with the search index. Required."""
    embedding_dependency: "_models.OnYourDataVectorizationSource" = rest_field()
    """The embedding dependency for vector search. Required."""

    @overload
    def __init__(
        self,
        *,
        environment: str,
        index_name: str,
        fields_mapping: "_models.PineconeFieldMappingOptions",
        embedding_dependency: "_models.OnYourDataVectorizationSource",
        top_n_documents: Optional[int] = None,
        in_scope: Optional[bool] = None,
        strictness: Optional[int] = None,
        max_search_queries: Optional[int] = None,
        allow_partial_result: Optional[bool] = None,
        include_contexts: Optional[List[Union[str, "_models.OnYourDataContextProperty"]]] = None,
        authentication: Optional["_models.OnYourDataAuthenticationOptions"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class PineconeFieldMappingOptions(_model_base.Model):
    """Optional settings to control how fields are processed when using a configured Pinecone
    resource.

    All required parameters must be populated in order to send to server.

    :ivar title_field: The name of the index field to use as a title.
    :vartype title_field: str
    :ivar url_field: The name of the index field to use as a URL.
    :vartype url_field: str
    :ivar filepath_field: The name of the index field to use as a filepath.
    :vartype filepath_field: str
    :ivar content_fields: The names of index fields that should be treated as content. Required.
    :vartype content_fields: list[str]
    :ivar content_fields_separator: The separator pattern that content fields should use.
    :vartype content_fields_separator: str
    """

    title_field: Optional[str] = rest_field()
    """The name of the index field to use as a title."""
    url_field: Optional[str] = rest_field()
    """The name of the index field to use as a URL."""
    filepath_field: Optional[str] = rest_field()
    """The name of the index field to use as a filepath."""
    content_fields: List[str] = rest_field()
    """The names of index fields that should be treated as content. Required."""
    content_fields_separator: Optional[str] = rest_field()
    """The separator pattern that content fields should use."""

    @overload
    def __init__(
        self,
        *,
        content_fields: List[str],
        title_field: Optional[str] = None,
        url_field: Optional[str] = None,
        filepath_field: Optional[str] = None,
        content_fields_separator: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class SpeechGenerationOptions(_model_base.Model):
    """A representation of the request options that control the behavior of a text-to-speech
    operation.

    All required parameters must be populated in order to send to server.

    :ivar input: The text to generate audio for. The maximum length is 4096 characters. Required.
    :vartype input: str
    :ivar voice: The voice to use for text-to-speech. Required. Known values are: "alloy", "echo",
     "fable", "onyx", "nova", and "shimmer".
    :vartype voice: str or ~azure.openai.models.SpeechVoice
    :ivar response_format: The audio output format for the spoken text. By default, the MP3 format
     will be used. Known values are: "mp3", "opus", "aac", "flac", "wav", and "pcm".
    :vartype response_format: str or ~azure.openai.models.SpeechGenerationResponseFormat
    :ivar speed: The speed of speech for generated audio. Values are valid in the range from 0.25
     to 4.0, with 1.0 the default and higher values corresponding to faster speech.
    :vartype speed: float
    :ivar model: The model to use for this text-to-speech request.
    :vartype model: str
    """

    input: str = rest_field()
    """The text to generate audio for. The maximum length is 4096 characters. Required."""
    voice: Union[str, "_models.SpeechVoice"] = rest_field()
    """The voice to use for text-to-speech. Required. Known values are: \"alloy\", \"echo\",
     \"fable\", \"onyx\", \"nova\", and \"shimmer\"."""
    response_format: Optional[Union[str, "_models.SpeechGenerationResponseFormat"]] = rest_field()
    """The audio output format for the spoken text. By default, the MP3 format will be used. Known
     values are: \"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", and \"pcm\"."""
    speed: Optional[float] = rest_field()
    """The speed of speech for generated audio. Values are valid in the range from 0.25 to 4.0, with
     1.0 the default and higher values corresponding to faster speech."""
    model: Optional[str] = rest_field()
    """The model to use for this text-to-speech request."""

    @overload
    def __init__(
        self,
        *,
        input: str,
        voice: Union[str, "_models.SpeechVoice"],
        response_format: Optional[Union[str, "_models.SpeechGenerationResponseFormat"]] = None,
        speed: Optional[float] = None,
        model: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class Upload(_model_base.Model):
    """The Upload object can accept byte chunks in the form of Parts.


    :ivar id: The Upload unique identifier, which can be referenced in API endpoints. Required.
    :vartype id: str
    :ivar created_at: The Unix timestamp (in seconds) for when the Upload was created. Required.
    :vartype created_at: ~datetime.datetime
    :ivar filename: The name of the file to be uploaded. Required.
    :vartype filename: str
    :ivar bytes: The intended number of bytes to be uploaded. Required.
    :vartype bytes: int
    :ivar purpose: The intended purpose of the file. Required. Is one of the following types:
     Literal["batch"], Literal["batch_output"], Literal["fine-tune"], Literal["fine-tune-results"],
     Literal["assistants"], Literal["assistants_output"], Literal["vision"], str
    :vartype purpose: str or str or str or str or str or str or str or str
    :ivar status: The status of the Upload. Required. Is one of the following types:
     Literal["pending"], Literal["completed"], Literal["cancelled"], Literal["expired"], str
    :vartype status: str or str or str or str or str
    :ivar expires_at: The Unix timestamp (in seconds) for when the Upload was created. Required.
    :vartype expires_at: ~datetime.datetime
    :ivar object: The object type, which is always "upload". Default value is "upload".
    :vartype object: str
    :ivar file: The ready File object after the Upload is completed.
    :vartype file: ~azure.openai.models.OpenAIFile
    """

    id: str = rest_field()
    """The Upload unique identifier, which can be referenced in API endpoints. Required."""
    created_at: datetime.datetime = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the Upload was created. Required."""
    filename: str = rest_field()
    """The name of the file to be uploaded. Required."""
    bytes: int = rest_field()
    """The intended number of bytes to be uploaded. Required."""
    purpose: Union[
        Literal["batch"],
        Literal["batch_output"],
        Literal["fine-tune"],
        Literal["fine-tune-results"],
        Literal["assistants"],
        Literal["assistants_output"],
        Literal["vision"],
        str,
    ] = rest_field()
    """The intended purpose of the file. Required. Is one of the following types: Literal[\"batch\"],
     Literal[\"batch_output\"], Literal[\"fine-tune\"], Literal[\"fine-tune-results\"],
     Literal[\"assistants\"], Literal[\"assistants_output\"], Literal[\"vision\"], str"""
    status: Union[Literal["pending"], Literal["completed"], Literal["cancelled"], Literal["expired"], str] = (
        rest_field()
    )
    """The status of the Upload. Required. Is one of the following types: Literal[\"pending\"],
     Literal[\"completed\"], Literal[\"cancelled\"], Literal[\"expired\"], str"""
    expires_at: datetime.datetime = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the Upload was created. Required."""
    object: Optional[Literal["upload"]] = rest_field()
    """The object type, which is always \"upload\". Default value is \"upload\"."""
    file: Optional["_models.OpenAIFile"] = rest_field()
    """The ready File object after the Upload is completed."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        created_at: datetime.datetime,
        filename: str,
        bytes: int,
        purpose: Union[
            Literal["batch"],
            Literal["batch_output"],
            Literal["fine-tune"],
            Literal["fine-tune-results"],
            Literal["assistants"],
            Literal["assistants_output"],
            Literal["vision"],
            str,
        ],
        status: Union[Literal["pending"], Literal["completed"], Literal["cancelled"], Literal["expired"], str],
        expires_at: datetime.datetime,
        object: Optional[Literal["upload"]] = None,
        file: Optional["_models.OpenAIFile"] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)


class UploadPart(_model_base.Model):
    """The upload Part represents a chunk of bytes we can add to an Upload object.

    Readonly variables are only populated by the server, and will be ignored when sending a request.


    :ivar id: The upload Part unique identifier, which can be referenced in API endpoints.
     Required.
    :vartype id: str
    :ivar created_at: The Unix timestamp (in seconds) for when the Part was created. Required.
    :vartype created_at: ~datetime.datetime
    :ivar upload_id: The ID of the Upload object that this Part was added to. Required.
    :vartype upload_id: str
    :ivar object: The object type, which is always ``upload.part``. Required. Default value is
     "upload.part".
    :vartype object: str
    :ivar azure_block_id: Azure only field.
    :vartype azure_block_id: str
    """

    id: str = rest_field()
    """The upload Part unique identifier, which can be referenced in API endpoints. Required."""
    created_at: datetime.datetime = rest_field(format="unix-timestamp")
    """The Unix timestamp (in seconds) for when the Part was created. Required."""
    upload_id: str = rest_field()
    """The ID of the Upload object that this Part was added to. Required."""
    object: Literal["upload.part"] = rest_field()
    """The object type, which is always ``upload.part``. Required. Default value is \"upload.part\"."""
    azure_block_id: Optional[str] = rest_field()
    """Azure only field."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        created_at: datetime.datetime,
        upload_id: str,
        azure_block_id: Optional[str] = None,
    ) -> None: ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]) -> None:
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.object: Literal["upload.part"] = "upload.part"
