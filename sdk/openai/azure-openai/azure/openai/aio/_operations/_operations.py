# pylint: disable=too-many-lines
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import json
import sys
from typing import Any, AsyncIterator, Callable, Dict, IO, List, Optional, TypeVar, Union, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    StreamClosedError,
    StreamConsumedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.rest import AsyncHttpResponse, HttpRequest
from azure.core.tracing.decorator_async import distributed_trace_async
from azure.core.utils import case_insensitive_dict

from ... import _model_base, models as _models
from ..._model_base import SdkJSONEncoder, _deserialize
from ..._operations._operations import (
    build_open_ai_add_upload_part_request,
    build_open_ai_cancel_batch_request,
    build_open_ai_cancel_upload_request,
    build_open_ai_complete_upload_request,
    build_open_ai_create_batch_request,
    build_open_ai_create_upload_request,
    build_open_ai_delete_file_request,
    build_open_ai_generate_speech_from_text_request,
    build_open_ai_get_audio_transcription_as_plain_text_request,
    build_open_ai_get_audio_transcription_as_response_object_request,
    build_open_ai_get_audio_translation_as_plain_text_request,
    build_open_ai_get_audio_translation_as_response_object_request,
    build_open_ai_get_batch_request,
    build_open_ai_get_chat_completions_request,
    build_open_ai_get_completions_request,
    build_open_ai_get_embeddings_request,
    build_open_ai_get_file_content_request,
    build_open_ai_get_file_request,
    build_open_ai_get_image_generations_request,
    build_open_ai_list_batches_request,
    build_open_ai_list_files_request,
    build_open_ai_upload_file_request,
)
from ..._validation import api_version_validation
from ..._vendor import FileType, prepare_multipart_form_data
from .._vendor import OpenAIClientMixinABC

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore
JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
_Unset: Any = object()
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]


class OpenAIClientOperationsMixin(OpenAIClientMixinABC):  # pylint: disable=too-many-public-methods

    @overload
    async def generate_speech_from_text(
        self,
        deployment_id: str,
        body: _models.SpeechGenerationOptions,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> AsyncIterator[bytes]:
        """Generates text-to-speech audio from the input text.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: A representation of the request options that control the behavior of a
         text-to-speech operation. Required.
        :type body: ~azure.openai.models.SpeechGenerationOptions
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AsyncIterator[bytes]
        :rtype: AsyncIterator[bytes]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def generate_speech_from_text(
        self, deployment_id: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> AsyncIterator[bytes]:
        """Generates text-to-speech audio from the input text.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: A representation of the request options that control the behavior of a
         text-to-speech operation. Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AsyncIterator[bytes]
        :rtype: AsyncIterator[bytes]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def generate_speech_from_text(
        self, deployment_id: str, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> AsyncIterator[bytes]:
        """Generates text-to-speech audio from the input text.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: A representation of the request options that control the behavior of a
         text-to-speech operation. Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AsyncIterator[bytes]
        :rtype: AsyncIterator[bytes]
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-02-15-preview",
        params_added_on={"2024-02-15-preview": ["api_version", "deployment_id", "content_type", "accept"]},
    )
    async def generate_speech_from_text(
        self, deployment_id: str, body: Union[_models.SpeechGenerationOptions, JSON, IO[bytes]], **kwargs: Any
    ) -> AsyncIterator[bytes]:
        """Generates text-to-speech audio from the input text.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: A representation of the request options that control the behavior of a
         text-to-speech operation. Is one of the following types: SpeechGenerationOptions, JSON,
         IO[bytes] Required.
        :type body: ~azure.openai.models.SpeechGenerationOptions or JSON or IO[bytes]
        :return: AsyncIterator[bytes]
        :rtype: AsyncIterator[bytes]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[AsyncIterator[bytes]] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_open_ai_generate_speech_from_text_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", True)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["content-type"] = self._deserialize("str", response.headers.get("content-type"))

        deserialized = response.iter_bytes()

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def get_audio_transcription_as_plain_text(
        self, deployment_id: str, body: _models.AudioTranscriptionOptions, **kwargs: Any
    ) -> str:
        """Gets transcribed text and associated metadata from provided spoken audio data. Audio will be
        transcribed in the
        written language corresponding to the language it was spoken in.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio transcription request. Required.
        :type body: ~azure.openai.models.AudioTranscriptionOptions
        :return: str
        :rtype: str
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_audio_transcription_as_plain_text(self, deployment_id: str, body: JSON, **kwargs: Any) -> str:
        """Gets transcribed text and associated metadata from provided spoken audio data. Audio will be
        transcribed in the
        written language corresponding to the language it was spoken in.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio transcription request. Required.
        :type body: JSON
        :return: str
        :rtype: str
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-02-15-preview",
        params_added_on={"2024-02-15-preview": ["api_version", "deployment_id", "content_type", "accept"]},
    )
    async def get_audio_transcription_as_plain_text(
        self, deployment_id: str, body: Union[_models.AudioTranscriptionOptions, JSON], **kwargs: Any
    ) -> str:
        """Gets transcribed text and associated metadata from provided spoken audio data. Audio will be
        transcribed in the
        written language corresponding to the language it was spoken in.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio transcription request. Is either a
         AudioTranscriptionOptions type or a JSON type. Required.
        :type body: ~azure.openai.models.AudioTranscriptionOptions or JSON
        :return: str
        :rtype: str
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[str] = kwargs.pop("cls", None)

        _body = body.as_dict() if isinstance(body, _model_base.Model) else body
        _file_fields: List[str] = ["file"]
        _data_fields: List[str] = [
            "filename",
            "response_format",
            "language",
            "prompt",
            "temperature",
            "timestamp_granularities",
            "model",
        ]
        _files, _data = prepare_multipart_form_data(_body, _file_fields, _data_fields)

        _request = build_open_ai_get_audio_transcription_as_plain_text_request(
            deployment_id=deployment_id,
            api_version=self._config.api_version,
            files=_files,
            data=_data,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["content-type"] = self._deserialize("str", response.headers.get("content-type"))

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(str, response.text())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def get_audio_transcription_as_response_object(  # pylint: disable=name-too-long
        self, deployment_id: str, body: _models.AudioTranscriptionOptions, **kwargs: Any
    ) -> _models.AudioTranscription:
        """Gets transcribed text and associated metadata from provided spoken audio data. Audio will be
        transcribed in the
        written language corresponding to the language it was spoken in.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio transcription request. Required.
        :type body: ~azure.openai.models.AudioTranscriptionOptions
        :return: AudioTranscription. The AudioTranscription is compatible with MutableMapping
        :rtype: ~azure.openai.models.AudioTranscription
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_audio_transcription_as_response_object(  # pylint: disable=name-too-long
        self, deployment_id: str, body: JSON, **kwargs: Any
    ) -> _models.AudioTranscription:
        """Gets transcribed text and associated metadata from provided spoken audio data. Audio will be
        transcribed in the
        written language corresponding to the language it was spoken in.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio transcription request. Required.
        :type body: JSON
        :return: AudioTranscription. The AudioTranscription is compatible with MutableMapping
        :rtype: ~azure.openai.models.AudioTranscription
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-02-15-preview",
        params_added_on={"2024-02-15-preview": ["api_version", "deployment_id", "content_type", "accept"]},
    )
    async def get_audio_transcription_as_response_object(  # pylint: disable=name-too-long
        self, deployment_id: str, body: Union[_models.AudioTranscriptionOptions, JSON], **kwargs: Any
    ) -> _models.AudioTranscription:
        """Gets transcribed text and associated metadata from provided spoken audio data. Audio will be
        transcribed in the
        written language corresponding to the language it was spoken in.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio transcription request. Is either a
         AudioTranscriptionOptions type or a JSON type. Required.
        :type body: ~azure.openai.models.AudioTranscriptionOptions or JSON
        :return: AudioTranscription. The AudioTranscription is compatible with MutableMapping
        :rtype: ~azure.openai.models.AudioTranscription
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.AudioTranscription] = kwargs.pop("cls", None)

        _body = body.as_dict() if isinstance(body, _model_base.Model) else body
        _file_fields: List[str] = ["file"]
        _data_fields: List[str] = [
            "filename",
            "response_format",
            "language",
            "prompt",
            "temperature",
            "timestamp_granularities",
            "model",
        ]
        _files, _data = prepare_multipart_form_data(_body, _file_fields, _data_fields)

        _request = build_open_ai_get_audio_transcription_as_response_object_request(
            deployment_id=deployment_id,
            api_version=self._config.api_version,
            files=_files,
            data=_data,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.AudioTranscription, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def get_audio_translation_as_plain_text(
        self, deployment_id: str, body: _models.AudioTranslationOptions, **kwargs: Any
    ) -> str:
        """Gets English language transcribed text and associated metadata from provided spoken audio data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio translation request. Required.
        :type body: ~azure.openai.models.AudioTranslationOptions
        :return: str
        :rtype: str
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_audio_translation_as_plain_text(self, deployment_id: str, body: JSON, **kwargs: Any) -> str:
        """Gets English language transcribed text and associated metadata from provided spoken audio data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio translation request. Required.
        :type body: JSON
        :return: str
        :rtype: str
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-02-15-preview",
        params_added_on={"2024-02-15-preview": ["api_version", "deployment_id", "content_type", "accept"]},
    )
    async def get_audio_translation_as_plain_text(
        self, deployment_id: str, body: Union[_models.AudioTranslationOptions, JSON], **kwargs: Any
    ) -> str:
        """Gets English language transcribed text and associated metadata from provided spoken audio data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio translation request. Is either a
         AudioTranslationOptions type or a JSON type. Required.
        :type body: ~azure.openai.models.AudioTranslationOptions or JSON
        :return: str
        :rtype: str
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[str] = kwargs.pop("cls", None)

        _body = body.as_dict() if isinstance(body, _model_base.Model) else body
        _file_fields: List[str] = ["file"]
        _data_fields: List[str] = ["filename", "response_format", "prompt", "temperature", "model"]
        _files, _data = prepare_multipart_form_data(_body, _file_fields, _data_fields)

        _request = build_open_ai_get_audio_translation_as_plain_text_request(
            deployment_id=deployment_id,
            api_version=self._config.api_version,
            files=_files,
            data=_data,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["content-type"] = self._deserialize("str", response.headers.get("content-type"))

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(str, response.text())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def get_audio_translation_as_response_object(
        self, deployment_id: str, body: _models.AudioTranslationOptions, **kwargs: Any
    ) -> _models.AudioTranslation:
        """Gets English language transcribed text and associated metadata from provided spoken audio data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio translation request. Required.
        :type body: ~azure.openai.models.AudioTranslationOptions
        :return: AudioTranslation. The AudioTranslation is compatible with MutableMapping
        :rtype: ~azure.openai.models.AudioTranslation
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_audio_translation_as_response_object(
        self, deployment_id: str, body: JSON, **kwargs: Any
    ) -> _models.AudioTranslation:
        """Gets English language transcribed text and associated metadata from provided spoken audio data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio translation request. Required.
        :type body: JSON
        :return: AudioTranslation. The AudioTranslation is compatible with MutableMapping
        :rtype: ~azure.openai.models.AudioTranslation
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-02-15-preview",
        params_added_on={"2024-02-15-preview": ["api_version", "deployment_id", "content_type", "accept"]},
    )
    async def get_audio_translation_as_response_object(
        self, deployment_id: str, body: Union[_models.AudioTranslationOptions, JSON], **kwargs: Any
    ) -> _models.AudioTranslation:
        """Gets English language transcribed text and associated metadata from provided spoken audio data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an audio translation request. Is either a
         AudioTranslationOptions type or a JSON type. Required.
        :type body: ~azure.openai.models.AudioTranslationOptions or JSON
        :return: AudioTranslation. The AudioTranslation is compatible with MutableMapping
        :rtype: ~azure.openai.models.AudioTranslation
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.AudioTranslation] = kwargs.pop("cls", None)

        _body = body.as_dict() if isinstance(body, _model_base.Model) else body
        _file_fields: List[str] = ["file"]
        _data_fields: List[str] = ["filename", "response_format", "prompt", "temperature", "model"]
        _files, _data = prepare_multipart_form_data(_body, _file_fields, _data_fields)

        _request = build_open_ai_get_audio_translation_as_response_object_request(
            deployment_id=deployment_id,
            api_version=self._config.api_version,
            files=_files,
            data=_data,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.AudioTranslation, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["after", "limit", "accept"]},
    )
    async def list_batches(
        self, *, after: Optional[str] = None, limit: Optional[int] = None, **kwargs: Any
    ) -> _models.OpenAIPageableListOfBatch:
        """Gets a list of all batches owned by the Azure OpenAI resource.

        :keyword after: Identifier for the last event from the previous pagination request. Default
         value is None.
        :paramtype after: str
        :keyword limit: Number of batches to retrieve. Defaults to 20. Default value is None.
        :paramtype limit: int
        :return: OpenAIPageableListOfBatch. The OpenAIPageableListOfBatch is compatible with
         MutableMapping
        :rtype: ~azure.openai.models.OpenAIPageableListOfBatch
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.OpenAIPageableListOfBatch] = kwargs.pop("cls", None)

        _request = build_open_ai_list_batches_request(
            after=after,
            limit=limit,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.OpenAIPageableListOfBatch, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def create_batch(
        self, create_batch_request: _models.BatchCreateRequest, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Batch:
        """Creates and executes a batch from an uploaded file of requests.
        Response includes details of the enqueued job including job status.
        The ID of the result file is added to the response once complete.

        :param create_batch_request: The specification of the batch to create and execute. Required.
        :type create_batch_request: ~azure.openai.models.BatchCreateRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Batch. The Batch is compatible with MutableMapping
        :rtype: ~azure.openai.models.Batch
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_batch(
        self, create_batch_request: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Batch:
        """Creates and executes a batch from an uploaded file of requests.
        Response includes details of the enqueued job including job status.
        The ID of the result file is added to the response once complete.

        :param create_batch_request: The specification of the batch to create and execute. Required.
        :type create_batch_request: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Batch. The Batch is compatible with MutableMapping
        :rtype: ~azure.openai.models.Batch
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_batch(
        self, create_batch_request: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Batch:
        """Creates and executes a batch from an uploaded file of requests.
        Response includes details of the enqueued job including job status.
        The ID of the result file is added to the response once complete.

        :param create_batch_request: The specification of the batch to create and execute. Required.
        :type create_batch_request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Batch. The Batch is compatible with MutableMapping
        :rtype: ~azure.openai.models.Batch
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["content_type", "accept"]},
    )
    async def create_batch(
        self, create_batch_request: Union[_models.BatchCreateRequest, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.Batch:
        """Creates and executes a batch from an uploaded file of requests.
        Response includes details of the enqueued job including job status.
        The ID of the result file is added to the response once complete.

        :param create_batch_request: The specification of the batch to create and execute. Is one of
         the following types: BatchCreateRequest, JSON, IO[bytes] Required.
        :type create_batch_request: ~azure.openai.models.BatchCreateRequest or JSON or IO[bytes]
        :return: Batch. The Batch is compatible with MutableMapping
        :rtype: ~azure.openai.models.Batch
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Batch] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(create_batch_request, (IOBase, bytes)):
            _content = create_batch_request
        else:
            _content = json.dumps(create_batch_request, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_open_ai_create_batch_request(
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Batch, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["batch_id", "accept"]},
    )
    async def get_batch(self, batch_id: str, **kwargs: Any) -> _models.Batch:
        """Gets details for a single batch specified by the given batchID.

        :param batch_id: The identifier of the batch. Required.
        :type batch_id: str
        :return: Batch. The Batch is compatible with MutableMapping
        :rtype: ~azure.openai.models.Batch
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Batch] = kwargs.pop("cls", None)

        _request = build_open_ai_get_batch_request(
            batch_id=batch_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Batch, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["batch_id", "accept"]},
    )
    async def cancel_batch(self, batch_id: str, **kwargs: Any) -> _models.Batch:
        """Gets details for a single batch specified by the given batchID.

        :param batch_id: The identifier of the batch. Required.
        :type batch_id: str
        :return: Batch. The Batch is compatible with MutableMapping
        :rtype: ~azure.openai.models.Batch
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Batch] = kwargs.pop("cls", None)

        _request = build_open_ai_cancel_batch_request(
            batch_id=batch_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Batch, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def get_completions(
        self,
        deployment_id: str,
        body: _models.CompletionsOptions,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.Completions:
        """Gets completions for the provided input prompts.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for a completions request.
         Completions support a wide variety of tasks and generate text that continues from or
         "completes"
         provided prompt data. Required.
        :type body: ~azure.openai.models.CompletionsOptions
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Completions. The Completions is compatible with MutableMapping
        :rtype: ~azure.openai.models.Completions
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_completions(
        self, deployment_id: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Completions:
        """Gets completions for the provided input prompts.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for a completions request.
         Completions support a wide variety of tasks and generate text that continues from or
         "completes"
         provided prompt data. Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Completions. The Completions is compatible with MutableMapping
        :rtype: ~azure.openai.models.Completions
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_completions(
        self, deployment_id: str, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Completions:
        """Gets completions for the provided input prompts.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for a completions request.
         Completions support a wide variety of tasks and generate text that continues from or
         "completes"
         provided prompt data. Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Completions. The Completions is compatible with MutableMapping
        :rtype: ~azure.openai.models.Completions
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def get_completions(
        self, deployment_id: str, body: Union[_models.CompletionsOptions, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.Completions:
        """Gets completions for the provided input prompts.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for a completions request.
         Completions support a wide variety of tasks and generate text that continues from or
         "completes"
         provided prompt data. Is one of the following types: CompletionsOptions, JSON, IO[bytes]
         Required.
        :type body: ~azure.openai.models.CompletionsOptions or JSON or IO[bytes]
        :return: Completions. The Completions is compatible with MutableMapping
        :rtype: ~azure.openai.models.Completions
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Completions] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_open_ai_get_completions_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Completions, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def get_chat_completions(
        self,
        deployment_id: str,
        body: _models.ChatCompletionsOptions,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ChatCompletions:
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for a chat completions request.
         Completions support a wide variety of tasks and generate text that continues from or
         "completes"
         provided prompt data. Required.
        :type body: ~azure.openai.models.ChatCompletionsOptions
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.openai.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_chat_completions(
        self, deployment_id: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.ChatCompletions:
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for a chat completions request.
         Completions support a wide variety of tasks and generate text that continues from or
         "completes"
         provided prompt data. Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.openai.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_chat_completions(
        self, deployment_id: str, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.ChatCompletions:
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for a chat completions request.
         Completions support a wide variety of tasks and generate text that continues from or
         "completes"
         provided prompt data. Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.openai.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2023-05-15",
        params_added_on={"2023-05-15": ["api_version", "deployment_id", "content_type", "accept"]},
    )
    async def get_chat_completions(
        self, deployment_id: str, body: Union[_models.ChatCompletionsOptions, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.ChatCompletions:
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for a chat completions request.
         Completions support a wide variety of tasks and generate text that continues from or
         "completes"
         provided prompt data. Is one of the following types: ChatCompletionsOptions, JSON, IO[bytes]
         Required.
        :type body: ~azure.openai.models.ChatCompletionsOptions or JSON or IO[bytes]
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.openai.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.ChatCompletions] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_open_ai_get_chat_completions_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.ChatCompletions, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def get_image_generations(
        self,
        deployment_id: str,
        body: _models.ImageGenerationOptions,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ImageGenerations:
        """Creates an image given a prompt.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: Represents the request data used to generate images. Required.
        :type body: ~azure.openai.models.ImageGenerationOptions
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ImageGenerations. The ImageGenerations is compatible with MutableMapping
        :rtype: ~azure.openai.models.ImageGenerations
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_image_generations(
        self, deployment_id: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.ImageGenerations:
        """Creates an image given a prompt.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: Represents the request data used to generate images. Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ImageGenerations. The ImageGenerations is compatible with MutableMapping
        :rtype: ~azure.openai.models.ImageGenerations
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_image_generations(
        self, deployment_id: str, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.ImageGenerations:
        """Creates an image given a prompt.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: Represents the request data used to generate images. Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ImageGenerations. The ImageGenerations is compatible with MutableMapping
        :rtype: ~azure.openai.models.ImageGenerations
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-02-15-preview",
        params_added_on={"2024-02-15-preview": ["api_version", "deployment_id", "content_type", "accept"]},
    )
    async def get_image_generations(
        self, deployment_id: str, body: Union[_models.ImageGenerationOptions, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.ImageGenerations:
        """Creates an image given a prompt.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: Represents the request data used to generate images. Is one of the following
         types: ImageGenerationOptions, JSON, IO[bytes] Required.
        :type body: ~azure.openai.models.ImageGenerationOptions or JSON or IO[bytes]
        :return: ImageGenerations. The ImageGenerations is compatible with MutableMapping
        :rtype: ~azure.openai.models.ImageGenerations
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.ImageGenerations] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_open_ai_get_image_generations_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.ImageGenerations, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def get_embeddings(
        self,
        deployment_id: str,
        body: _models.EmbeddingsOptions,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.Embeddings:
        """Return the embeddings for a given prompt.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an embeddings request.
         Embeddings measure the relatedness of text strings and are commonly used for search,
         clustering,
         recommendations, and other similar scenarios. Required.
        :type body: ~azure.openai.models.EmbeddingsOptions
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Embeddings. The Embeddings is compatible with MutableMapping
        :rtype: ~azure.openai.models.Embeddings
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_embeddings(
        self, deployment_id: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Embeddings:
        """Return the embeddings for a given prompt.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an embeddings request.
         Embeddings measure the relatedness of text strings and are commonly used for search,
         clustering,
         recommendations, and other similar scenarios. Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Embeddings. The Embeddings is compatible with MutableMapping
        :rtype: ~azure.openai.models.Embeddings
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def get_embeddings(
        self, deployment_id: str, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Embeddings:
        """Return the embeddings for a given prompt.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an embeddings request.
         Embeddings measure the relatedness of text strings and are commonly used for search,
         clustering,
         recommendations, and other similar scenarios. Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Embeddings. The Embeddings is compatible with MutableMapping
        :rtype: ~azure.openai.models.Embeddings
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def get_embeddings(
        self, deployment_id: str, body: Union[_models.EmbeddingsOptions, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.Embeddings:
        """Return the embeddings for a given prompt.

        :param deployment_id: Specifies either the model deployment name (when using Azure OpenAI) or
         model name (when using non-Azure OpenAI) to use for this request. Required.
        :type deployment_id: str
        :param body: The configuration information for an embeddings request.
         Embeddings measure the relatedness of text strings and are commonly used for search,
         clustering,
         recommendations, and other similar scenarios. Is one of the following types:
         EmbeddingsOptions, JSON, IO[bytes] Required.
        :type body: ~azure.openai.models.EmbeddingsOptions or JSON or IO[bytes]
        :return: Embeddings. The Embeddings is compatible with MutableMapping
        :rtype: ~azure.openai.models.Embeddings
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Embeddings] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_open_ai_get_embeddings_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Embeddings, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["purpose", "accept"]},
    )
    async def list_files(
        self, *, purpose: Optional[Union[str, _models.FilePurpose]] = None, **kwargs: Any
    ) -> _models.FileListResponse:
        """Gets a list of previously uploaded files.

        :keyword purpose: A value that, when provided, limits list results to files matching the
         corresponding purpose. Known values are: "fine-tune", "fine-tune-results", "assistants",
         "assistants_output", "batch", "batch_output", and "vision". Default value is None.
        :paramtype purpose: str or ~azure.openai.models.FilePurpose
        :return: FileListResponse. The FileListResponse is compatible with MutableMapping
        :rtype: ~azure.openai.models.FileListResponse
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FileListResponse] = kwargs.pop("cls", None)

        _request = build_open_ai_list_files_request(
            purpose=purpose,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.FileListResponse, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def upload_file(
        self, *, file: FileType, purpose: Union[str, _models.FilePurpose], filename: Optional[str] = None, **kwargs: Any
    ) -> _models.OpenAIFile:
        """Uploads a file for use by other operations.

        :keyword file: The file data (not filename) to upload. Required.
        :paramtype file: ~azure.openai._vendor.FileType
        :keyword purpose: The intended purpose of the file. Known values are: "fine-tune",
         "fine-tune-results", "assistants", "assistants_output", "batch", "batch_output", and "vision".
         Required.
        :paramtype purpose: str or ~azure.openai.models.FilePurpose
        :keyword filename: A filename to associate with the uploaded data. Default value is None.
        :paramtype filename: str
        :return: OpenAIFile. The OpenAIFile is compatible with MutableMapping
        :rtype: ~azure.openai.models.OpenAIFile
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def upload_file(self, body: JSON, **kwargs: Any) -> _models.OpenAIFile:
        """Uploads a file for use by other operations.

        :param body: Required.
        :type body: JSON
        :return: OpenAIFile. The OpenAIFile is compatible with MutableMapping
        :rtype: ~azure.openai.models.OpenAIFile
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["content_type", "accept"]},
    )
    async def upload_file(
        self,
        body: JSON = _Unset,
        *,
        file: FileType = _Unset,
        purpose: Union[str, _models.FilePurpose] = _Unset,
        filename: Optional[str] = None,
        **kwargs: Any
    ) -> _models.OpenAIFile:
        """Uploads a file for use by other operations.

        :param body: Is one of the following types: JSON Required.
        :type body: JSON
        :keyword file: The file data (not filename) to upload. Required.
        :paramtype file: ~azure.openai._vendor.FileType
        :keyword purpose: The intended purpose of the file. Known values are: "fine-tune",
         "fine-tune-results", "assistants", "assistants_output", "batch", "batch_output", and "vision".
         Required.
        :paramtype purpose: str or ~azure.openai.models.FilePurpose
        :keyword filename: A filename to associate with the uploaded data. Default value is None.
        :paramtype filename: str
        :return: OpenAIFile. The OpenAIFile is compatible with MutableMapping
        :rtype: ~azure.openai.models.OpenAIFile
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.OpenAIFile] = kwargs.pop("cls", None)

        if body is _Unset:
            if file is _Unset:
                raise TypeError("missing required argument: file")
            if purpose is _Unset:
                raise TypeError("missing required argument: purpose")
            body = {"file": file, "filename": filename, "purpose": purpose}
            body = {k: v for k, v in body.items() if v is not None}
        _body = body.as_dict() if isinstance(body, _model_base.Model) else body
        _file_fields: List[str] = ["file"]
        _data_fields: List[str] = ["purpose", "filename"]
        _files, _data = prepare_multipart_form_data(_body, _file_fields, _data_fields)

        _request = build_open_ai_upload_file_request(
            files=_files,
            data=_data,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.OpenAIFile, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["file_id", "accept"]},
    )
    async def delete_file(self, file_id: str, **kwargs: Any) -> _models.FileDeletionStatus:
        """Delete a previously uploaded file.

        :param file_id: The ID of the file to delete. Required.
        :type file_id: str
        :return: FileDeletionStatus. The FileDeletionStatus is compatible with MutableMapping
        :rtype: ~azure.openai.models.FileDeletionStatus
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FileDeletionStatus] = kwargs.pop("cls", None)

        _request = build_open_ai_delete_file_request(
            file_id=file_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.FileDeletionStatus, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["file_id", "accept"]},
    )
    async def get_file(self, file_id: str, **kwargs: Any) -> _models.OpenAIFile:
        """Returns information about a specific file. Does not retrieve file content.

        :param file_id: The ID of the file to retrieve. Required.
        :type file_id: str
        :return: OpenAIFile. The OpenAIFile is compatible with MutableMapping
        :rtype: ~azure.openai.models.OpenAIFile
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.OpenAIFile] = kwargs.pop("cls", None)

        _request = build_open_ai_get_file_request(
            file_id=file_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.OpenAIFile, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-07-01-preview",
        params_added_on={"2024-07-01-preview": ["file_id", "accept"]},
    )
    async def get_file_content(self, file_id: str, **kwargs: Any) -> bytes:
        """Returns information about a specific file. Does not retrieve file content.

        :param file_id: The ID of the file to retrieve. Required.
        :type file_id: str
        :return: bytes
        :rtype: bytes
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[bytes] = kwargs.pop("cls", None)

        _request = build_open_ai_get_file_content_request(
            file_id=file_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(bytes, response.json(), format="base64")

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def create_upload(
        self, request_body: _models.CreateUploadRequest, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Upload:
        """Creates an intermediate Upload object that you can add Parts to. Currently, an Upload can
        accept at most 8 GB in total and expires after an hour after you create it.

        Once you complete the Upload, we will create a File object that contains all the parts you
        uploaded. This File is usable in the rest of our platform as a regular File object.

        For certain purposes, the correct mime_type must be specified. Please refer to documentation
        for the supported MIME types for your use case.

        For guidance on the proper filename extensions for each purpose, please follow the
        documentation on creating a File.

        :param request_body: The request body for the operation options. Required.
        :type request_body: ~azure.openai.models.CreateUploadRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_upload(
        self, request_body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Upload:
        """Creates an intermediate Upload object that you can add Parts to. Currently, an Upload can
        accept at most 8 GB in total and expires after an hour after you create it.

        Once you complete the Upload, we will create a File object that contains all the parts you
        uploaded. This File is usable in the rest of our platform as a regular File object.

        For certain purposes, the correct mime_type must be specified. Please refer to documentation
        for the supported MIME types for your use case.

        For guidance on the proper filename extensions for each purpose, please follow the
        documentation on creating a File.

        :param request_body: The request body for the operation options. Required.
        :type request_body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_upload(
        self, request_body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Upload:
        """Creates an intermediate Upload object that you can add Parts to. Currently, an Upload can
        accept at most 8 GB in total and expires after an hour after you create it.

        Once you complete the Upload, we will create a File object that contains all the parts you
        uploaded. This File is usable in the rest of our platform as a regular File object.

        For certain purposes, the correct mime_type must be specified. Please refer to documentation
        for the supported MIME types for your use case.

        For guidance on the proper filename extensions for each purpose, please follow the
        documentation on creating a File.

        :param request_body: The request body for the operation options. Required.
        :type request_body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-08-01-preview",
        params_added_on={"2024-08-01-preview": ["content_type", "accept"]},
    )
    async def create_upload(
        self, request_body: Union[_models.CreateUploadRequest, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.Upload:
        """Creates an intermediate Upload object that you can add Parts to. Currently, an Upload can
        accept at most 8 GB in total and expires after an hour after you create it.

        Once you complete the Upload, we will create a File object that contains all the parts you
        uploaded. This File is usable in the rest of our platform as a regular File object.

        For certain purposes, the correct mime_type must be specified. Please refer to documentation
        for the supported MIME types for your use case.

        For guidance on the proper filename extensions for each purpose, please follow the
        documentation on creating a File.

        :param request_body: The request body for the operation options. Is one of the following types:
         CreateUploadRequest, JSON, IO[bytes] Required.
        :type request_body: ~azure.openai.models.CreateUploadRequest or JSON or IO[bytes]
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Upload] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(request_body, (IOBase, bytes)):
            _content = request_body
        else:
            _content = json.dumps(request_body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_open_ai_create_upload_request(
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Upload, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def add_upload_part(
        self, upload_id: str, request_body: _models.AddUploadPartRequest, **kwargs: Any
    ) -> _models.UploadPart:
        """Adds a Part to an Upload object. A Part represents a chunk of bytes from the file you are
        trying to upload.

        Each Part can be at most 64 MB, and you can add Parts until you hit the Upload maximum of 8 GB.

        It is possible to add multiple Parts in parallel. You can decide the intended order of the
        Parts when you complete the Upload.

        :param upload_id: The ID of the upload associated with this operation. Required.
        :type upload_id: str
        :param request_body: The request body data payload for the operation. Required.
        :type request_body: ~azure.openai.models.AddUploadPartRequest
        :return: UploadPart. The UploadPart is compatible with MutableMapping
        :rtype: ~azure.openai.models.UploadPart
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def add_upload_part(self, upload_id: str, request_body: JSON, **kwargs: Any) -> _models.UploadPart:
        """Adds a Part to an Upload object. A Part represents a chunk of bytes from the file you are
        trying to upload.

        Each Part can be at most 64 MB, and you can add Parts until you hit the Upload maximum of 8 GB.

        It is possible to add multiple Parts in parallel. You can decide the intended order of the
        Parts when you complete the Upload.

        :param upload_id: The ID of the upload associated with this operation. Required.
        :type upload_id: str
        :param request_body: The request body data payload for the operation. Required.
        :type request_body: JSON
        :return: UploadPart. The UploadPart is compatible with MutableMapping
        :rtype: ~azure.openai.models.UploadPart
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-08-01-preview",
        params_added_on={"2024-08-01-preview": ["content_type", "upload_id", "accept"]},
    )
    async def add_upload_part(
        self, upload_id: str, request_body: Union[_models.AddUploadPartRequest, JSON], **kwargs: Any
    ) -> _models.UploadPart:
        """Adds a Part to an Upload object. A Part represents a chunk of bytes from the file you are
        trying to upload.

        Each Part can be at most 64 MB, and you can add Parts until you hit the Upload maximum of 8 GB.

        It is possible to add multiple Parts in parallel. You can decide the intended order of the
        Parts when you complete the Upload.

        :param upload_id: The ID of the upload associated with this operation. Required.
        :type upload_id: str
        :param request_body: The request body data payload for the operation. Is either a
         AddUploadPartRequest type or a JSON type. Required.
        :type request_body: ~azure.openai.models.AddUploadPartRequest or JSON
        :return: UploadPart. The UploadPart is compatible with MutableMapping
        :rtype: ~azure.openai.models.UploadPart
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.UploadPart] = kwargs.pop("cls", None)

        _body = request_body.as_dict() if isinstance(request_body, _model_base.Model) else request_body
        _file_fields: List[str] = ["data"]
        _data_fields: List[str] = []
        _files, _data = prepare_multipart_form_data(_body, _file_fields, _data_fields)

        _request = build_open_ai_add_upload_part_request(
            upload_id=upload_id,
            files=_files,
            data=_data,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.UploadPart, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def complete_upload(
        self,
        upload_id: str,
        request_body: _models.CompleteUploadRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.Upload:
        """Completes the Upload.

        Within the returned Upload object, there is a nested File object that is ready to use in the
        rest of the platform.

        You can specify the order of the Parts by passing in an ordered list of the Part IDs.

        The number of bytes uploaded upon completion must match the number of bytes initially specified
        when creating the Upload object. No Parts may be added after an Upload is completed.

        :param upload_id: The ID of the upload associated with this operation. Required.
        :type upload_id: str
        :param request_body: The request body for the completion operation. Required.
        :type request_body: ~azure.openai.models.CompleteUploadRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def complete_upload(
        self, upload_id: str, request_body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Upload:
        """Completes the Upload.

        Within the returned Upload object, there is a nested File object that is ready to use in the
        rest of the platform.

        You can specify the order of the Parts by passing in an ordered list of the Part IDs.

        The number of bytes uploaded upon completion must match the number of bytes initially specified
        when creating the Upload object. No Parts may be added after an Upload is completed.

        :param upload_id: The ID of the upload associated with this operation. Required.
        :type upload_id: str
        :param request_body: The request body for the completion operation. Required.
        :type request_body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def complete_upload(
        self, upload_id: str, request_body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Upload:
        """Completes the Upload.

        Within the returned Upload object, there is a nested File object that is ready to use in the
        rest of the platform.

        You can specify the order of the Parts by passing in an ordered list of the Part IDs.

        The number of bytes uploaded upon completion must match the number of bytes initially specified
        when creating the Upload object. No Parts may be added after an Upload is completed.

        :param upload_id: The ID of the upload associated with this operation. Required.
        :type upload_id: str
        :param request_body: The request body for the completion operation. Required.
        :type request_body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-08-01-preview",
        params_added_on={"2024-08-01-preview": ["upload_id", "content_type", "accept"]},
    )
    async def complete_upload(
        self, upload_id: str, request_body: Union[_models.CompleteUploadRequest, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.Upload:
        """Completes the Upload.

        Within the returned Upload object, there is a nested File object that is ready to use in the
        rest of the platform.

        You can specify the order of the Parts by passing in an ordered list of the Part IDs.

        The number of bytes uploaded upon completion must match the number of bytes initially specified
        when creating the Upload object. No Parts may be added after an Upload is completed.

        :param upload_id: The ID of the upload associated with this operation. Required.
        :type upload_id: str
        :param request_body: The request body for the completion operation. Is one of the following
         types: CompleteUploadRequest, JSON, IO[bytes] Required.
        :type request_body: ~azure.openai.models.CompleteUploadRequest or JSON or IO[bytes]
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Upload] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(request_body, (IOBase, bytes)):
            _content = request_body
        else:
            _content = json.dumps(request_body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_open_ai_complete_upload_request(
            upload_id=upload_id,
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Upload, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    @api_version_validation(
        method_added_on="2024-08-01-preview",
        params_added_on={"2024-08-01-preview": ["upload_id", "accept"]},
    )
    async def cancel_upload(self, upload_id: str, **kwargs: Any) -> _models.Upload:
        """Cancels the Upload. No Parts may be added after an Upload is cancelled.

        :param upload_id: The ID of the upload associated with this operation. Required.
        :type upload_id: str
        :return: Upload. The Upload is compatible with MutableMapping
        :rtype: ~azure.openai.models.Upload
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Upload] = kwargs.pop("cls", None)

        _request = build_open_ai_cancel_upload_request(
            upload_id=upload_id,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str"),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Upload, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore
